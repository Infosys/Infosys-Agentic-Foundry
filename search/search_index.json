{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to Agentic Foundry","text":"<p>Agentic Foundry is an open-source framework designed to empower developers to create, configure, and deploy customizable AI agents. By using predefined templates, this platform ensures seamless tool integration and streamlines agentic workflow development.</p>"},{"location":"#what-is-agentic-foundry","title":"What is Agentic Foundry","text":"<p>The <code>Agentic Foundry</code> enables the efficient creation and deployment of AI agents by offering customizable templates and a set of powerful tools. These agents can be tailored to specific roles and personas, and developers can integrate their own custom tools to suit various needs.</p> <p>Key Components:</p> <ul> <li> <p><code>Agent Configuration:</code> A reusable agent-building template that helps in creating agents with specific roles or persona definitions.</p> </li> <li> <p><code>Tool Configuration:</code> Manage all the supported tools available for developers. Provides a framework for integrating and onboarding custom tools.</p> </li> </ul>"},{"location":"#how-agentic-ai-differs-from-traditional-software-and-genai","title":"How Agentic AI Differs from Traditional Software and GenAI","text":"<p>Traditional Systems:</p> <p>Traditional softwares including process automation, supervised machine learning (ML), and even generative AI lacks true autonomy. These systems perform only the tasks they are explicitly programmed or trained to do. They follow predefined instructions or generate outputs within defined parameters.</p> <p>What Makes Agentic AI Different:</p> <p>Agentic AI is designed to operate with autonomy. Rather than executing predefined tasks, it receives high-level goals or events as input and independently determines how to act in order to achieve the desired outcomes.</p> <p>Core Mechanisms of Agentic AI:</p> <p>Agentic AI integrates key capabilities to enable autonomous behavior:</p> <ul> <li>Plan: Develops a strategy to achieve the given goal or respond to an event.</li> <li>Act: Executes tasks and may interact with external systems or tools to carry out its strategy.</li> <li>Reflect: Evaluates the outcomes of its actions, learns from them, and adjusts its behavior accordingly.</li> <li>Respond: Delivers a final decision or result based on its planning and learning process.</li> </ul> <p>Key Characteristics:</p> <p>The output of an Agentic AI system is:</p> <ul> <li>Goal-directed </li> <li>Interactive </li> <li>Context-aware</li> </ul>"},{"location":"#why-to-use-agentic-foundry","title":"Why to Use Agentic Foundry","text":"<p>The <code>Agentic Foundry</code>  offers a comprehensive solution with continually evolving new features:</p> <p>1. Agent-as-a-Service:      Open-source framework-based solution that allows users to quickly configure agents and tools. End-users can interact with agents via a conversational interface.</p> <p>2. Vertical Agents: A suite of agents tailored for various industry personas. These agents represent specific workflows for different domains, including:</p> <ul> <li> <p><code>Finance:</code></p> <ul> <li>Financial statement analysis</li> <li>Risk assessment and management</li> <li>Fraud prevention</li> </ul> </li> <li> <p><code>Healthcare:</code></p> <ul> <li>Medical record analysis</li> <li>Diagnostic assistance</li> <li>Treatment plan recommendation</li> </ul> </li> <li> <p><code>Insurance:</code></p> <ul> <li>Policy underwriting assistance</li> <li>Claim fraud detection</li> <li>Risk assessment for policy pricing</li> </ul> </li> <li> <p><code>Retail:</code></p> <ul> <li>Customer segmentation</li> <li>Personalized product recommendations</li> <li>Price optimization</li> </ul> </li> <li> <p><code>Communication:</code></p> <ul> <li>Customer churn prediction</li> <li>Fraud detection in calls and data usage</li> <li>Customer service automation</li> </ul> </li> <li> <p><code>Manufacturing:</code></p> <ul> <li>Quality control inspection</li> <li>Predictive maintenance</li> <li>Product lifecycle management</li> </ul> </li> </ul> <p>The following are sample use cases categorized by domain:</p> FINANCE <ul> <li>Customer Onboarding</li> <li>Credit Worthiness Assessment</li> <li>Personal Finance Advisor</li> </ul> HEALTHCARE <ul> <li>Personalized care plan</li> <li>E-Triage</li> <li>Medical translator</li> </ul> INSURANCE <ul> <li>Personalized insurance quotes &amp; underwriting</li> <li>Improve conversion ratio by AI assisted AI</li> <li>Reduce CSR call efforts</li> </ul> RETAIL <ul> <li>Customer segmentation</li> <li>Supplier recommendation</li> <li>Competitor Product Analysis</li> </ul> TRAVEL <ul> <li>Travel Operations</li> <li>Intelligent accommodation systems</li> <li>Travel Inspirational Video</li> </ul> MANUFACTURING <ul> <li>Demand Forecasting</li> <li>Production Planning and Scheduling</li> <li>Energy Utilization Analytics</li> </ul> <p>3. Horizontal Agents</p> <ul> <li>These agents offer common functionality across industries, such as:<ul> <li>Email sending</li> <li>File search</li> <li>Agentic RAG (Retrieval-Augmented Generation). </li> <li>SDLC</li> </ul> </li> </ul>"},{"location":"#how-to-use-agentic-foundry","title":"How to Use Agentic Foundry","text":"<p>Follow this simple flow to start using the Agentic Foundry:</p> <ul> <li> <p><code>Upload the Required Files:</code>  Begin by uploading all necessary files required for the agent to function. If the agent depends on a database, ensure you upload the relevant database files in the <code>File</code> section. Supported database formats include SQLite files (.db, .sqlite) and PostgreSQL dump files (.sql). Additionally, upload any PDFs or other supporting documents that the agent may need to process or reference.</p> </li> <li> <p><code>Create and Onboard Tools:</code>  Develop the required tools and onboard them into the system.</p> </li> <li> <p><code>Create the Agent:</code> Build your AI agent by adding the necessary tools and configuring it according to the desired role.</p> </li> <li> <p><code>Start Using the Agent:</code>  Once your agent is configured, head to the Inference section and select the agent to start using it for real-time tasks.</p> </li> </ul> <p>     With Agentic Foundry, developers have the power to create highly customizable AI agents for various applications \u2014 empowering businesses and individuals to automate workflows and achieve efficiency with ease. </p>"},{"location":"Admin_Screen/","title":"Admin Screen","text":""},{"location":"Admin_Screen/#overview","title":"Overview","text":"<p>The <code>Admin Screen</code> serves as the central hub for managing and monitoring key components of the system. </p> <ul> <li>It is organized into multiple sections including <code>Tools</code>, <code>Agents</code>, <code>Chat</code>, <code>Vault</code>, <code>Data Connectors</code>, <code>Files</code>, <code>Evaluation</code> and an additional tab labeled <code>Admin</code>. </li> <li>The <code>Admin</code> tab is specifically designed to provide administrators with advanced management capabilities.</li> <li>Within the <code>Admin</code> tab, you will find four main functional areas that enable administrators to effectively manage users, oversee agent feedback, and evaluate system performance: <code>Register</code>, <code>Learning - Feedback Approval</code>, <code>RecycleBin</code> and <code>Unused Tools &amp; Agents</code>.</li> </ul>"},{"location":"Admin_Screen/#1-register","title":"1. Register","text":"<p>This tab is dedicated to user registration and role management. It enables administrators to create new user accounts by providing essential credentials and assigning appropriate roles.</p> <p>User Input Fields:</p> <ul> <li>Email: Unique identifier for the user.</li> <li>Password: Secure password for login.</li> <li>Role: Dropdown selection among <code>Admin</code>, <code>Developer</code>, and <code>User</code>.</li> </ul>"},{"location":"Admin_Screen/#user-roles-and-permissions","title":"User Roles and Permissions","text":"<p>The platform defines three distinct user roles <code>Admin</code>, <code>Developer</code>, and <code>User</code> each with specific access levels and capabilities.</p> <p>1. Admin Role</p> <p>Admins have full administrative privileges with unrestricted access to all system features. This role is typically responsible for platform-wide configuration and oversight.</p> <p>Admin Capabilities:</p> <ul> <li>Onboard new tools and agents.</li> <li>Update existing tools and agents.</li> <li>Delete tools and agents.</li> <li>Upload files via the <code>Files</code> section.</li> <li>Interact with agents in the <code>Chat Inference</code> section.</li> <li>Manage the feedback learning, including review, approval, and learning updates.</li> <li>Perform model evaluations and view detailed performance metrics.</li> </ul> <p>2. Developer Role</p> <p>Developers are responsible for the setup, configuration, and maintenance of tools and agents. While they have extensive technical access, their permissions are slightly restricted compared to Admins.</p> <p>Developer Capabilities:</p> <ul> <li>Onboard new tools and agents.</li> <li>Update tools and agents.</li> <li>Delete tools and agents.</li> <li>Upload files relevant to tool and agent functionality in the <code>Chat Inference</code> section.</li> <li>Interact with agents in the <code>Chat Inference</code> section.</li> <li>No access to feedback approval or evaluation metrics.</li> </ul> <p>3. User Role</p> <p>Users typically interact with tools and agents for practical use cases, testing, or demonstrations. Their permissions mirror those of Developers, focusing on hands-on interaction.</p> <p>User Capabilities:</p> <ul> <li>End-user role with capabilities equivalent to the Developer.</li> <li>Can onboard, update, and delete tools and agents, upload files, and interact with agents.</li> <li>Typically engaged in the practical use and testing of tools rather than administrative oversight.</li> </ul> <p>Note</p> <p>Only the creator (owner) of a tool or agent has permission to update or delete it. This applies to all roles, ensuring secure and accountable resource management.</p>"},{"location":"Admin_Screen/#2-learning-feedback-approval","title":"2. Learning - Feedback Approval","text":"<p>This tab centralizes feedback management for all agents in the system, enabling admins to review and validate user feedback to improve agent responses.</p> <p>Learning - Feedback Approval is a feature designed to help administrators manage, evaluate, and improve the performance of agents through structured feedback. It acts as a centralized hub where all feedback related to the agents' responses is collected and analyzed. The primary goal is to enhance the quality and accuracy of the agents\u2019 replies by reviewing and applying feedback provided by users or evaluators.</p> <p>In this system, admins can view all the agents deployed in the platform. By selecting a specific agent, they can access all the feedback associated with that agent. This feedback can include evaluations of how well the agent answered specific queries, suggestions for improvement, or issues with the agent's responses.</p> <p>The system offers a comprehensive view of each feedback item. For each feedback, admins can see:</p> <ul> <li> <p>The original query from the user, which provides context for understanding the issue or request the agent was addressing.</p> </li> <li> <p>The original response from the agent, showing how the agent initially handled the query.</p> </li> <li> <p>Feedback and updates, which include comments or suggestions from users or evaluators regarding the agent\u2019s response, helping admins understand what needs to be improved.</p> </li> <li> <p>A new final response, which is the updated version of the agent\u2019s answer after incorporating feedback. This step ensures that the agent's future responses are more accurate and relevant.</p> </li> <li> <p>A detailed record of the conversation steps, including the interaction history, tools used, and the context of the conversation, providing deeper insight into the agent's decision-making process.</p> </li> <li> <p><code>Lesson</code>: The lesson summarizes the LLM's understanding and learning based on the provided feedback. Admins can view, edit, and update the lesson as needed. By enabling the \"Included in learning\" toggle, the admin approves the lesson. If the lesson is unsatisfactory, the admin can revise it before approval.</p> </li> </ul> <p>Info</p> <p>The Learning - Feedback Approval process is essential for continuously refining the agent's performance. By regularly reviewing and applying feedback, admins can ensure that the agents become more capable, accurate, and responsive over time, leading to improved user satisfaction and efficiency.</p> <p>Administrative Capabilities:</p> <ul> <li>Review the feedback in detail to assess its validity and impact on agent performance.</li> <li>Approve feedback to validate its effectiveness and relevance.</li> <li>Commit changes, which updates the agent's knowledge base or response logic accordingly.</li> <li>This process supports continuous learning and improvement of agents by integrating verified feedback.</li> </ul>"},{"location":"Admin_Screen/#3-recyclebin","title":"3. RecycleBin","text":"<p>The RecycleBin tab lets administrators manage deleted tools, agents and MCP Servers. All removed items are listed here, providing a way to review them before permanent deletion.</p> <p>Key Features</p> <ul> <li>View Deleted Items: See a list of all deleted tools, agents and MCP Servers.</li> <li>Restore Functionality: Restore mistakenly deleted items with a single click.</li> <li>Permanent Deletion: Permanently remove items that are no longer needed. This action cannot be undone.</li> </ul> <p>Note: Only users with administrative privileges can access the RecycleBin tab and perform restore or permanent delete actions.</p>"},{"location":"Admin_Screen/#4-unused-tools-agents-and-mcp-servers","title":"4. Unused Tools, Agents and MCP Servers","text":"<p>The Unused Tools, Agents and MCP Servers tab displays all tools, agents and MCP Servers that have not been used or updated in the last 15 days. <code>Unused</code> means there has been no interaction with the agent or modification to the tool/agent during this period.</p> <p>Administrative Capabilities:</p> <ul> <li>View a comprehensive list of unused tools, agents and MCP Servers.</li> <li>Delete unused tools, agents or MCP Servers directly from this tab.</li> <li>Deleted items are automatically moved to the RecycleBin for potential recovery or permanent deletion.</li> </ul>"},{"location":"Admin_Screen/#5-installation","title":"5. Installation","text":"<p>The <code>Installation</code> tab provides administrators with comprehensive visibility into the Python package dependencies required by the tools in the system. This section helps ensure that all necessary packages are properly installed and tracks pending installation requests from users.</p>"},{"location":"Admin_Screen/#missing-dependencies","title":"Missing Dependencies","text":"<p>This tab identifies Python packages that are required by existing tools but are not currently installed in the environment.</p> <p>How It Works:</p> <ul> <li>The system extracts all tool code snippets from the database for existing tools.</li> <li>It parses the import statements from each tool's code to identify all referenced modules.</li> <li>User-defined modules and Python standard library (stdlib) packages are filtered out.</li> <li>The remaining packages are identified as real PyPI packages.</li> <li>These identified packages are compared against: <code>requirements.txt</code> file and currently installed packages in the environment.</li> <li>Any packages that are not installed are listed under <code>Missing Dependencies</code>.</li> </ul> <p>Admin Action</p> <ul> <li>Review the list of missing packages.</li> <li>Install the required packages to ensure all tools function correctly.</li> </ul>"},{"location":"Admin_Screen/#installed-dependencies","title":"Installed Dependencies","text":"<p>This tab displays all Python packages currently installed in the environment.</p> <p>How It Works:</p> <ul> <li>The system runs a subprocess command to list all installed packages.</li> <li>The complete list of installed packages with their versions is displayed to the administrator.</li> </ul> <p>Use Case</p> <ul> <li>Verify that required packages are installed.</li> <li>Check package versions for compatibility.</li> <li>Audit the environment for security or compliance purposes.</li> </ul>"},{"location":"Admin_Screen/#pending-dependencies","title":"Pending Dependencies","text":"<p>This tab tracks tool onboarding failures caused by missing module dependencies and displays them for admin review.</p> <p>How It Works:</p> <ul> <li>During tool onboarding, the system validates that all imported modules in the tool code are available.</li> <li>If a module is not found, the tool onboarding is blocked with a \"module not found\" validation error.</li> <li>The system records the following details in the database:</li> <li><code>Module Name:</code> The missing package/module that caused the failure.</li> <li><code>Tool Code:</code> The tool code snippet that contains the import.</li> <li><code>User:</code> The user who attempted to onboard the tool.</li> <li>These pending dependency requests are displayed to the administrator under this tab.</li> </ul> <p>Admin Action</p> <ul> <li>Review pending dependency requests from users.</li> <li>Install the requested packages after validation.</li> <li>Notify users once the package is installed so they can retry tool onboarding.</li> </ul>"},{"location":"Agent_Pipelines/","title":"Agent Pipelines","text":"<p>The Agent Pipelines feature provides a visual pipeline builder that allows users to create, configure, and manage multi-agent workflows using an intuitive drag-and-drop canvas interface. This enables the orchestration of multiple agents with conditional logic, input handling, and output management.</p>"},{"location":"Agent_Pipelines/#overview","title":"Overview","text":"<p>Agent Pipelines allow you to visually design complex workflows by connecting multiple agents together. Instead of manually coordinating agent calls, you can create a pipeline that automatically routes data between agents based on your defined flow.</p> <p>Key Capabilities:</p> <ul> <li><code>Visual Workflow Design</code>: Drag-and-drop interface for building agent workflows</li> <li><code>Multi-Agent Orchestration</code>: Connect multiple agents in sequence or parallel</li> <li><code>Conditional Branching</code>: Route data based on conditions and agent outputs</li> <li><code>Input/Output Management</code>: Define pipeline inputs and structure final outputs</li> <li><code>Reusable Pipelines</code>: Save and reuse pipeline configurations</li> </ul>"},{"location":"Agent_Pipelines/#pipeline-components","title":"Pipeline Components","text":""},{"location":"Agent_Pipelines/#canvas","title":"Canvas","text":"<p>The canvas is the main workspace where you design your pipeline. It provides:</p> <ul> <li><code>Drag-and-Drop</code>: Place nodes by dragging them from the sidebar onto the canvas</li> <li><code>Pan &amp; Zoom</code>: Navigate large pipelines with zoom controls and canvas panning</li> <li><code>Grid Alignment</code>: Nodes snap to a grid for clean visual organization</li> <li><code>Canvas Controls</code>: Zoom in/out, fit view, and clear canvas buttons</li> </ul>"},{"location":"Agent_Pipelines/#node-types","title":"Node Types","text":"<p>Pipelines are built using four types of nodes:</p> <p>1. Input Node</p> <p>The Input Node defines the entry point of your pipeline and specifies what data the pipeline expects to receive.</p> <ul> <li>Defines a single input key: <code>query</code></li> <li>Supports different data types: string, integer, JSON</li> <li>Only one Input node is allowed per pipeline</li> <li>Connects to Agent or Condition nodes</li> </ul> <p>Configuration:</p> Property Description Node Name Custom name for the input node Input Keys List of expected input parameters with their types <p>2. Agent Node</p> <p>The Agent Node represents an agent that will be executed as part of the pipeline workflow.</p> <ul> <li>Select from available agents in IAF</li> <li>Configure which inputs the agent can access</li> <li>Multiple Agent nodes can be added to a pipeline</li> <li>Receives data from Input, other Agents</li> </ul> <p>Configuration:</p> Property Description Node Name Custom name for the agent node Agent Select an agent from the dropdown Accessible Inputs Choose which pipeline inputs and agent outputs this agent can access <p>Accessible Inputs Options:</p> <ul> <li><code>All Inputs</code>: Agent has access to all pipeline inputs and outputs from other agents</li> <li><code>Specific Inputs</code>: Select individual inputs (from Input node) and agent outputs to make available</li> </ul> <p>3. Condition Node</p> <p>The Condition Node enables conditional branching in your pipeline based on expressions.</p> <ul> <li>Define conditions to evaluate agent outputs</li> <li>Route data to different paths based on results</li> <li>Supports multiple output connections for branching logic</li> <li>Useful for error handling, validation, or decision trees</li> </ul> <p>Configuration:</p> Property Description Node Name Custom name for the condition node Condition Condition to evaluate, written in plain English (e.g., \"If the status of result is equal to success, then...\") <p>4. Output Node</p> <p>The Output Node defines the final output of your pipeline.</p> <ul> <li>Specifies the output schema for the pipeline</li> <li>Multiple Output nodes are allowed for different output paths</li> <li>Receives data from Agent or Condition nodes</li> </ul> <p>Configuration:</p> Property Description Node Name Custom name for the output node Output Schema JSON schema defining the structure of the output (optional)"},{"location":"Agent_Pipelines/#connections","title":"Connections","text":"<p>Connections (edges) define the flow of data between nodes in your pipeline.</p> <p>Creating Connections</p> <ol> <li>Click on the output point of a source node</li> <li>Drag to the input point of a target node</li> <li>Release to create the connection</li> </ol> <p>Connection Rules</p> From \u2193 / To \u2192 Input Agent Condition Output Input \u274c \u2705 \u2705 \u2705 Agent \u274c \u2705 \u2705 \u2705 Condition \u274c \u2705 \u274c \u2705 Output \u274c \u274c \u274c \u274c <ul> <li><code>Input nodes</code> can connect to Agent, Condition, or Output nodes</li> <li><code>Agent nodes</code> can connect to other Agents, Conditions, or Output nodes</li> <li><code>Condition nodes</code> can connect to Agent or Output nodes (not to other Conditions)</li> <li><code>Output nodes</code> cannot have outgoing connections</li> </ul>"},{"location":"Agent_Pipelines/#properties-panel","title":"Properties Panel","text":"<p>When you click on a node, the Properties Panel opens on the side, allowing you to configure the selected node.</p> <p>The panel displays:</p> <ul> <li><code>Node Name</code>: Editable name for the node</li> <li><code>Node Type</code>: The type of node (Input, Agent, Condition, Output)</li> <li><code>Node ID</code>: Auto-generated unique identifier</li> <li><code>Type-specific settings</code>: Configuration options based on node type</li> </ul>"},{"location":"Agent_Pipelines/#saving-a-pipeline","title":"Saving a Pipeline","text":"<p>To save your pipeline:</p> <ol> <li>Ensure you have at least one <code>Input node</code> and one <code>Agent node</code> with a selected agent</li> <li>Click the <code>Save</code> button</li> <li>Enter a <code>Pipeline Name</code> (required)</li> <li>Add an optional <code>Description</code></li> <li>Click <code>Save</code> to store the pipeline</li> </ol> <p>Save Requirements:</p> <ul> <li>At least one Input node</li> <li>At least one Agent node with a selected agent</li> <li>Pipeline name is required</li> </ul> <p>Example Pipeline</p> <p>Here's an example of a simple research pipeline:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Input  \u2502 \u2500\u2500\u25ba \u2502 Research     \u2502 \u2500\u2500\u25ba \u2502 Writer      \u2502 \u2500\u2500\u25ba \u2502 Output \u2502\n\u2502  Node   \u2502     \u2502 Agent        \u2502     \u2502 Agent       \u2502     \u2502 Node   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <ol> <li><code>Input Node</code>: Receives <code>query</code> (string) and <code>context</code> (JSON)</li> <li><code>Research Agent</code>: Searches for information based on the query</li> <li><code>Writer Agent</code>: Takes research output and generates a formatted response</li> <li><code>Output Node</code>: Returns the final structured response</li> </ol> <p>Using Pipelines in Chat Inference</p> <p>In the <code>Chat Inference</code> tab, you can select a pipeline as the agent type and interact with it just like any other agent template. This allows you to test and validate your multi-agent workflows directly from the chat interface.</p> <p>Best Practices</p> <ol> <li>Name your nodes clearly: Use descriptive names that indicate each node's purpose</li> <li>Start simple: Begin with a linear pipeline before adding conditions</li> <li>Test incrementally: Save and test your pipeline as you build it</li> <li>Use conditions wisely: Add conditional logic for error handling and validation</li> <li>Document your pipelines: Add descriptions when saving to help others understand the workflow</li> </ol>"},{"location":"Architecture/","title":"Arcitecture","text":"<p>Agentic Foundry architecture includes a comprehensive orchestration system that integrates agents, tools, and memory components to create intelligent, scalable, and personalized AI-driven workflows.</p>"},{"location":"Architecture/#agents","title":"Agents","text":"<ul> <li>Reusable Templates: Developers can utilize predefined, reusable templates to streamline the creation of agents. These templates allow for the rapid definition of agents with specific roles or persona attributes, ensuring consistency and reducing development time.</li> <li>Customizable Roles: Agents can be tailored to meet the unique requirements of an application by defining their behaviors, objectives, and interaction styles. This flexibility enables developers to create agents that align with specific use cases or business goals.</li> </ul> <p>We mainly have three types of templates: <code>React</code>, <code>React Critic</code>, <code>Multi</code>, <code>Planner Executor</code>,<code>Meta</code>, <code>Planner Meta</code>.</p> <p>React Template: </p> <p>The ReAct(Reasoning and Acting) agent combines reasoning traces with action execution. It uses a step by step thought process to determine what tool to use, executes it, observe the result, and continues until it can return a final answer.</p> <p>ReAct Use Case</p> <p>When the task requires step-by-step reasoning and immediate action execution.</p> <p>Examples:</p> <ol> <li>Answering user queries by reasoning through available data and tools.</li> <li>Performing calculations or data lookups with a clear sequence of steps.</li> <li>Interactive troubleshooting or debugging tasks where the agent needs to reason and act iteratively.</li> </ol> <p>React Critic Template: </p> <p>Extends the React agent by introducing a Critic module that reviews and refines each reasoning-action step. The Critic evaluates intermediate outputs, provides feedback, and enables the agent to self-correct before finalizing a response. This results in higher accuracy and reliability, especially for complex, multi-step queries.</p> <p>React Critic Use Case</p> <p>When the task requires step-by-step reasoning and immediate action execution, with an added need for refining and validating each step.</p> <p>Examples:</p> <ol> <li>Answering complex user queries that involve multiple steps and require validation of each step.</li> <li>Performing calculations or data lookups where intermediate results need to be evaluated and possibly corrected.</li> <li>Interactive troubleshooting or debugging tasks where the agent needs to reason, act, and iteratively refine its actions based on feedback.</li> </ol> <p>Multi Template: </p> <p>The Multi Agent operates on the Planner-Executor-Critic paradigm. It begins with a Planner Agent that generates a step-by-step plan based on the user query. The Executor Agent then executes each step of the plan. The Critic evaluates the outputs by scoring the results of each step.</p> <p>Multi Agent Use Case</p> <p>When the task involves a complex workflow that requires planning, execution, and evaluation.</p> <p>Examples:</p> <ol> <li>Multi-step project management tasks where a detailed plan is needed.</li> <li>Executing a sequence of dependent tasks, such as data processing pipelines.</li> <li>Scenarios where outputs need to be evaluated and scored for quality or correctness.</li> </ol> <p>Planner Executor Template: </p> <p>Separates planning and execution into distinct agents. The Planner Agent generates a detailed step-by-step plan, while the Executor Agent carries out each step, handling tool invocations and intermediate results. This modular approach improves adaptability and transparency in structured problem-solving.</p> <p>Planner Executor Agent Use Case</p> <p>When the task requires a clear separation between planning and execution, allowing for independent optimization and management of each phase.</p> <p>Examples</p> <ol> <li>Complex data analysis tasks where planning the analysis steps separately from execution provides clearer insights and easier debugging.</li> <li>Software development processes where planning the development tasks and executing them are handled by different agents, allowing for specialized optimization.</li> <li>Any scenario where a detailed plan is beneficial, and the execution can be carried out independently, possibly in a different environment or context.</li> </ol> <p>Meta Template: </p> <p>Meta templates are used for agents that require higher-level reasoning or orchestration capabilities. These agents can manage other agents, coordinate tasks, or oversee complex processes, making them suitable for supervisory or managerial roles within the system.</p> <p>Meta Agent Use Case</p> <p>When the task requires higher-level orchestration, coordination of multiple agents, or managing complex processes.</p> <p>Examples:</p> <ol> <li>Supervising multiple agents working on different parts of a large project.</li> <li>Overseeing workflows that involve dynamic task allocation and coordination.</li> <li>Managing and optimizing resource allocation across multiple agents or tools.</li> </ol> <p>Planner Meta Template: </p> <p>Acts as a high-level orchestrator, combining advanced planning with dynamic agent management. The Planner Meta Agent coordinates multiple specialized worker agents, delegates sub-tasks, supervises execution, and aggregates results to deliver robust solutions for complex queries.</p> <p>Planner Meta Agent Use Case</p> <p>When the task requires advanced planning and the coordination of multiple agents, possibly with different specializations, to achieve a complex goal.</p> <p>Examples</p> <ol> <li>Managing a team of agents where some are specialized in data gathering, others in analysis, and some in reporting, all working together to complete a comprehensive business intelligence task.</li> <li>Orchestrating a multi-agent system where agents need to collaborate, share results, and build upon each other's work in a dynamic and possibly unpredictable environment.</li> <li>Any scenario where complex problem-solving requires the integration of multiple specialized agents, with a need for high-level oversight and coordination.</li> </ol>"},{"location":"Architecture/#tools","title":"Tools","text":"<ul> <li>Tool Management: A centralized interface is provided to manage all supported tools. This interface simplifies the process of enabling, disabling, or configuring tools, ensuring that developers have full control over the orchestration environment.</li> <li>Custom Tool Integration: The framework supports seamless integration of custom tools, allowing developers to onboard new tools without disrupting existing workflows. This ensures scalability and adaptability as new tools or technologies emerge.</li> </ul>"},{"location":"Architecture/#memory","title":"Memory","text":"<p>Agentic Foundry features a dual-layered memory architecture to support both short-term and long-term context retention for agents.</p> <ul> <li> <p>Session Memory: Retains context-specific data during the current session, ensuring coherent and contextually relevant interactions. This is essential for multi-turn conversations and complex workflows.</p> </li> <li> <p>Persistent (Long-Term) Memory: Goes beyond the session, storing user behaviors, preferences, and historical data over time. This enables the system to provide personalized experiences, maintain long-term context, and adapt to user needs dynamically. Persistent memory is further divided into:</p> <ul> <li>Semantic Memory: Stores facts, preferences, and contextual information for future reference and retrieval. Agents use semantic memory to remember user-provided details and reference past interactions. </li> <li>Episodic Memory: Captures specific query-response examples from real conversations, supporting few-shot learning and dynamic adaptation based on user feedback. Episodic memory enables agents to learn from both positive and negative conversational experiences. </li> </ul> </li> </ul> <p>This architecture ensures that agents can leverage both immediate context and accumulated knowledge, resulting in more adaptive, user-aligned, and effective responses across all agent templates.</p>"},{"location":"Architecture/#architecture-design","title":"Architecture Design","text":"<p>The orchestration system architecture demonstrates seamless integration between agents, tools, and memory components. This design emphasizes modularity, scalability, and efficient data flow across all system elements.</p> <p>Key Components</p> <ol> <li>Agents: Modular entities that leverage tools and memory to execute tasks and deliver intelligent responses</li> <li>Tools: Integrated utilities that extend agent capabilities and enable specialized functionality</li> <li>Memory: Dual-layered system supporting both session-based context retention and persistent long-term personalization</li> <li>Workflow: Streamlined data flow ensuring cohesive interactions between all components</li> </ol> <p>Design Principles</p> <p>The architecture prioritizes:</p> <ul> <li>Modularity: Independent components that can be developed and maintained separately</li> <li>Scalability: System design that adapts to growing requirements and complexity</li> <li>Extensibility: Framework that supports easy integration of new tools and capabilities</li> </ul> <p>This design enables developers to efficiently understand, implement, and extend the orchestration framework while maintaining system coherence and performance.</p>"},{"location":"DataConnectors/","title":"Data Connectors","text":"<p>The Agentic Foundry supports database connectivity through Data Connectors, allowing you to connect to and interact with different database systems for your agent workflows.</p>"},{"location":"DataConnectors/#supported-databases","title":"Supported Databases","text":"<p>Currently, the foundry supports two database types:</p> <ul> <li>PostgreSQL - Enterprise-grade relational database</li> <li>SQLite - Lightweight file-based database</li> <li>MySQL - Popular open-source relational database, widely used for web applications</li> <li>MongoDB - Leading NoSQL document database, ideal for flexible and scalable data storage</li> </ul>"},{"location":"DataConnectors/#creating-database-connections","title":"Creating Database Connections","text":""},{"location":"DataConnectors/#postgresql-connection","title":"PostgreSQL Connection","text":"<p>To create a new PostgreSQL connection:</p> <ol> <li>Connection Name: Enter a unique name for your connection</li> <li>Database Type: Select \"PostgreSQL\"</li> <li>Host: Database server hostname or IP address</li> <li>Port: Database port (default: 5432)</li> <li>Database: Database name to connect to</li> <li>Username: Database username</li> <li>Password: Database password</li> <li>Click Connect to establish the connection</li> </ol> <p>Example Configuration: <pre><code>Connection Name: my_postgres_db\nDatabase Type: PostgreSQL\nHost: localhost\nPort: 5432\nDatabase: myapp_production\nUsername: db_user\nPassword: ********\n</code></pre></p>"},{"location":"DataConnectors/#sqlite-connection","title":"SQLite Connection","text":"<p>To create a new SQLite connection:</p> <ol> <li>Connection Name: Enter a unique name for your connection</li> <li>Database Type: Select \"SQLite\"</li> <li>New SQLite DB File: Specify the path to your SQLite database file</li> <li>Click Connect to establish the connection</li> </ol> <p>Example Configuration: <pre><code>Connection Name: my_sqlite_db\nDatabase Type: SQLite\nNew SQLite DB File: database.db\n</code></pre></p>"},{"location":"DataConnectors/#mysql-connection","title":"MySQL Connection","text":"<p>To create a new MySQL connection:</p> <ol> <li>Connection Name: Enter a unique name for your connection</li> <li>Database Type: Select \"MySQL\"</li> <li>Host: Database server hostname or IP address</li> <li>Port: Database port</li> <li>Database: Database name to connect to</li> <li>Username: Database username</li> <li>Password: Database password</li> <li>Click Connect to establish the connection</li> </ol> <p>Example Configuration: <pre><code>Connection Name: mysql_db\nDatabase Type: MySQL\nHost: localhost\nPort: 3306\nDatabase: myapp_production\nUsername: db_user\nPassword: ********\n</code></pre></p>"},{"location":"DataConnectors/#mongodb-connection","title":"MongoDB Connection","text":"<p>To create a new MongoDB connection:</p> <ol> <li>Connection Name: Enter a unique name for your connection</li> <li>Database Type: Select \"MongoDB\"</li> <li>Host: Database server hostname or IP address</li> <li>Port: Database port</li> <li>Database: Database name to connect to</li> <li>Username: Database username</li> <li>Password: Database password</li> <li>Click Connect to establish the connection</li> </ol> <p>Example Configuration: <pre><code>Connection Name: mongo_db\nDatabase Type: MongoDB\nHost: localhost\nPort: 27017\nDatabase: myapp_production\nUsername: db_user\nPassword: ********\n</code></pre></p>"},{"location":"DataConnectors/#data-connector-features","title":"Data Connector Features","text":""},{"location":"DataConnectors/#run-button-functionality","title":"Run Button Functionality","text":"<p>The Run button in Data Connectors provides an interactive query interface:</p> <ol> <li>Select Connection Name: Choose from your active database connections</li> <li>Enter NLP Query: Write your request in natural language</li> <li>Generate Query: Click \"Generate Query\" to convert natural language to SQL</li> <li>Review and Edit: Examine the generated  query and make modifications if needed</li> <li>Run Query: Execute the query , it will show the output</li> </ol> <p>Example NLP Queries: - \"Create a table called users with id, name, and email columns\" - \"Insert a new user with name John and email john@example.com\" - \"Show all users from the users table\" - \"Update user with id 1 to have email newemail@example.com\"</p>"},{"location":"DataConnectors/#manage-button-functionality","title":"Manage Button Functionality","text":"<p>The Manage button allows you to control your database connections:</p>"},{"location":"DataConnectors/#available-actions","title":"Available Actions:","text":"<ul> <li>Activate: Enable a connection for to use that  in agent inferencing</li> <li>Deactivate: Disable a connection (prevents use in agents)</li> <li>Delete: Permanently remove the connection</li> </ul> <p>Connection Status</p> <p>A connection must be active to be used during agent inferencing. Inactive connections will not be available to agents.</p> <p>Delete Warning</p> <p>The delete option permanently removes the connection. This action cannot be undone.</p>"},{"location":"DataConnectors/#using-data-connectors-in-agent-inferencing","title":"Using Data Connectors in Agent Inferencing","text":"<p>Connection Requirements</p> <p>When using agents that contain database tools:</p> <ol> <li>Activate Connection: Ensure the required database connection is active in Data Connectors</li> <li>Provide DB Key: Supply the connection name as the database key during inferencing</li> <li>Tool Execution: The agent will use the specified connection to fetch database information</li> </ol> <p>Error Handling</p> <p>If a database connection is not properly configured:</p> <ul> <li>The agent will return an error message</li> <li>No output will be produced from database-related tools</li> <li>Ensure the connection is active and the correct connection name is provided</li> </ul> <p>Sample Tool Implementation</p> <p>Here's an example of how to create a database tool for your agents:</p> <pre><code>def fetch_all_from_xyz(connection_name: str):\n    \"\"\"\n    Fetches all records from the 'xyz' table in the specified database using the provided database key.\n\n    Args:\n        connection_name (str): The key used to identify and connect to the specific database.\n\n    Returns:\n        list: A list of dictionaries, where each dictionary represents a row from the 'xyz' table.\n    \"\"\"\n    from MultiDBConnection_Manager import get_connection_manager\n    from sqlalchemy import text\n\n    try:\n        manager = get_connection_manager()\n        session = manager.get_sql_session(connection_name)\n        result = session.execute(text('SELECT * FROM xyz'))\n        rows = result.fetchall()\n        session.close()\n        return [dict(row._mapping) for row in rows]\n    except Exception as e:\n        if 'session' in locals():\n            session.close()\n        return f'Error fetching data from database {connection_name}: {str(e)}'\n</code></pre>"},{"location":"DataConnectors/#important-notes-for-tool-development","title":"Important Notes for Tool Development","text":"<p>Required Imports</p> <p>Always include these import statements in your database tools:</p> <pre><code>from MultiDBConnection_Manager import get_connection_manager\nfrom sqlalchemy import text\n</code></pre> <p>Connection Manager Usage</p> <p>Follow this pattern for database operations:</p> <pre><code># Get the singleton instance of MultiDBConnectionManager\nmanager = get_connection_manager()\n\n# Get a SQLAlchemy session for the specified connection\nsession = manager.get_sql_session(connection_name)\n</code></pre> <p>Session Management</p> <p>Always close sessions after use:</p> <pre><code>try:\n    # Your database operations here\n    pass\nexcept Exception as e:\n    # Handle errors\n    pass\nfinally:\n    if 'session' in locals():\n        session.close()\n</code></pre> <p>Best Practices Checklist</p> <ul> <li>No need to manually activate a connection before using it in agents; when you call <code>get_sql_session(connection_name)</code>, the connection will be activated automatically if the <code>connection_name</code> exists in the database.</li> <li>Include required imports in your tool functions</li> <li>Always close database sessions after use</li> </ul>"},{"location":"ExportAgent/","title":"Export Agent","text":""},{"location":"ExportAgent/#agent-export-functionality","title":"Agent Export Functionality","text":"<p>Agentic Foundry allows you to export complete agent configurations, including agent data, tools, and required static files. This makes it easy to back up, migrate, or redeploy agents across different environments.</p> <p>Exporting is done via a backend API endpoint, which takes an Agent ID and generates a folder containing all necessary backend and frontend code for the agent. You can select agents and click 'Export' to download this folder. The export process ensures that all dependencies, configurations, and scripts are included, so the agent can be redeployed without manual setup.</p> <p>Exported agents are useful for:</p> <ul> <li>Backing up agent configurations and logic</li> <li>Migrating agents between development, staging, and production environments</li> <li>Sharing agent setups with other teams or organizations</li> <li>Rapidly redeploying agents after infrastructure changes</li> </ul>"},{"location":"ExportAgent/#exporting-agents","title":"Exporting Agents","text":"<p>Key Features:</p> <ul> <li>SSE Streaming: The exported agents include Server-Sent Events (SSE) support for real-time inference streaming, enabling pipeline-based processing and live response streaming</li> <li>Validator Support: Built-in validator configurations are included in the export, ensuring workflow integrity and data validation across agent operations</li> <li>Tool Dependency Export: All tool dependencies and base requirements are automatically packaged with the agent export, ensuring complete portability</li> </ul> <p>All agent types can be exported, including React, React Critic, Planner Executor Critic, Planner Executor, Meta, and Planner Meta. The export includes both backend and frontend code, organized for easy redeployment.</p> <p>The backend is provided as a folder containing all logic, configuration, and scripts needed for the agent. This folder includes configuration files, tool definitions, a Dockerfile for containerization, environment variables, and documentation. All dependencies are listed in requirements.txt for easy installation.</p> <p>The frontend includes the source code, public assets, deployment configuration (such as YAML files for Kubernetes), and environment files. This ensures the user interface and all related assets are ready for deployment.</p> <p>A typical exported folder structure includes:</p> <ul> <li>Agent_Backend: Contains the backend Python package (<code>exportagent</code>), configuration files, tool definitions with their dependencies, validator configurations, SSE streaming setup, Dockerfile for containerization, environment variables, and all scripts needed for agent operation. The wheel file is listed in <code>requirements.txt</code> so that installing dependencies will automatically install the agent backend along with all tool dependencies and base requirements.</li> <li>Agent_Frontend: Contains the frontend source code, public assets, deployment configuration, SSE client implementations, and all files required to run the agent's user interface.</li> </ul> <p>This structure allows teams to:</p> <ul> <li>Install the backend in any Python environment using pip and requirements.txt</li> <li>Deploy the frontend using standard web deployment tools</li> <li>Use Docker for containerized deployments</li> </ul> <p>The export process is designed to be comprehensive and portable, supporting both cloud and on-premises deployments. It simplifies agent management and ensures consistency across environments.</p>"},{"location":"Features/","title":"Features","text":"<p>Agentic Foundary  provides comprehensive capabilities for building and managing intelligent agents with minimal coding effort.</p>"},{"location":"Features/#features-overview","title":"Features Overview","text":"Low-Code Agent Creation Easy Agent &amp; Tool Management In-Platform Customization Reusable Components Human-in-the-Loop Dynamic Workflow Automation Transparent Execution Agentic Foundary Features Feedback-Driven Learning Orchestrator Agent Custom Knowledge Bases LLM-Based Evaluation Telemetry &amp; Monitoring Clear Documentation Flexible Model Support"},{"location":"Features/#feature-descriptions","title":"Feature Descriptions","text":""},{"location":"Features/#core-development-features","title":"Core Development Features","text":"<p>1. Low-Code Agent Creation:</p> <p>Reduced development time with a low-code approach. Simply provide the tool logic and workflow definitions, agentic framework automatically handles the rest.</p> <p>2. Easy Agent &amp; Tool Management: </p> <p>Seamlessly onboard, update, or remove components.</p> <p>3. In-Platform Customization:</p> <p>Customize tools, workflows, and control logic directly within the framework, no external IDEs or redeployment required.</p> <p>4. Reusable Components: </p> <p>Design agents and tools as modular, self-contained components. These can be reused across multiple workflows and projects, promoting consistency, accelerating development.</p>"},{"location":"Features/#workflow-control-features","title":"Workflow &amp; Control Features","text":"<p>5. Human-in-the-Loop: </p> <p>Integrate manual checkpoints into automated workflows to enable human review and intervention during critical decision-making steps. Maintain oversight and control where it matters most, especially in sensitive or high-risk operations.</p> <p>6. Dynamic Workflow Automation: </p> <p>Automate workflows that can adapt to real-time inputs, system feedback, or environmental changes. The platform supports dynamic branching, state-aware execution, and conditional task handling.</p> <p>7. Transparent Execution: </p> <p>Maintain full visibility into every agent decision. The system provides detailed execution traces, step-by-step logs, and visual workflow representations to help developers debug, audit, and validate agent behavior.</p> <p>8. Orchestrator Agent: </p> <p>Manage complex multiple agents systems through a central coordinating agent. The Orchestrator assigns tasks, handles inter-agent communication, resolves conflicts, and ensures that distributed agents work toward a unified goal.</p>"},{"location":"Features/#intelligence-learning-features","title":"Intelligence &amp; Learning Features","text":"<p>9. Feedback-Driven Learning: </p> <p>Continuously improve agent performance through direct user feedback. The system supports structured and unstructured feedback loops that fine-tune decision-making, language understanding, and tool usage over time.</p> <p>10. Custom Knowledge Bases: </p> <p>Equip agents with domain-specific intelligence.</p> <p>11. LLM-Based Evaluation: </p> <p>Assess agent quality with LLM-as-a-judge scoring.</p> <p>12. Flexible Model Support: </p> <p>Plug in different LLMs or SLMs as per task needs.</p>"},{"location":"Features/#operations-support-features","title":"Operations &amp; Support Features","text":"<p>13. Telemetry &amp; Monitoring: </p> <p>Monitor system performance and behavior using best-in-class tools like OpenTelemetry for distributed tracing, Arize Phoenix for model observability, and Grafana for real-time dashboards and alerts.</p> <p>14. Clear Documentation:</p> <p>Access structured, detailed documentation built with MkDocs. Covers configuration, deployment, integration guides, and troubleshooting, designed to support developers at every stage of the build process.</p>"},{"location":"GZIP_Compression/","title":"GZIP Compression for API Responses","text":"<p>GZIP compression is a widely used method to reduce the size of data transmitted between the backend and frontend, improving the speed and efficiency of web applications.</p>"},{"location":"GZIP_Compression/#how-gzip-compression-works","title":"How GZIP Compression Works","text":"<ol> <li>Middleware Integration: GZIP middleware is added to the backend server (FastAPI) to automatically compress API responses.</li> <li>Response Size Threshold: The backend is configured to compress responses only if the payload size exceeds a certain threshold (e.g., 500 KB). This ensures that only large responses are compressed, optimizing resource usage.</li> <li>Client Request Headers: The frontend must indicate support for GZIP by setting the request header:      <pre><code>{\n    \"Accept-Encoding\": \"gzip\"\n}\n</code></pre></li> <li>Compressed Response: If the response size is greater than 500 KB, the backend compresses the data and sends it to the frontend.</li> <li>Decompression on Frontend: The frontend automatically decompresses the GZIP response and displays the original data to the user.</li> </ol>"},{"location":"GZIP_Compression/#example-flow","title":"Example Flow","text":"<ol> <li>User requests data from the frontend.</li> <li>Frontend sends request with <code>Accept-Encoding: gzip</code> header.</li> <li> <p>Backend checks response size:</p> <ul> <li>If \u2264 500 KB: Sends uncompressed data.</li> <li>If &gt; 500 KB: Compresses data using GZIP and sends compressed response.</li> </ul> </li> <li> <p>Frontend receives and decompresses the response, then renders the original data.</p> </li> </ol>"},{"location":"GZIP_Compression/#benefits","title":"Benefits","text":"<ul> <li>Reduced Bandwidth Usage: Compressing large responses significantly reduces the amount of data transferred over the network.</li> <li>Faster Load Times: Smaller payloads result in faster API response times and improved user experience.</li> <li>Seamless Integration: Most modern browsers and HTTP clients natively support GZIP decompression.</li> </ul>"},{"location":"JWT/","title":"JWT API Authentication","text":""},{"location":"JWT/#overview","title":"Overview","text":"<p>JWT (JSON Web Token) is a standard for securing API endpoints and managing user authentication and authorization. JWTs are compact tokens that encode claims and are cryptographically signed to ensure integrity and authenticity.</p>"},{"location":"JWT/#how-jwt-authentication-works","title":"How JWT Authentication Works","text":"<ol> <li>User Registration: User provides credentials (username, email, password, role).</li> <li>User Login: User logs in; backend generates and returns a JWT token.</li> <li>Token Usage: Client stores the JWT and includes it in the <code>Authorization</code> header as a Bearer token for API requests.</li> <li>Backend Authorization: Backend validates the JWT on each request, granting or denying access based on token validity.</li> </ol>"},{"location":"JWT/#example-jwt-authentication-flow","title":"Example JWT Authentication Flow","text":"<ol> <li> <p>Register: <pre><code>POST /api/register\n{\n  \"username\": \"user1\",\n  \"password\": \"password123\"\n}\n</code></pre></p> </li> <li> <p>Login: <pre><code>POST /api/login\n{\n  \"username\": \"user1\",\n  \"password\": \"password123\"\n}\n// Response:\n{\n  \"token\": \"&lt;JWT_TOKEN&gt;\"\n}\n</code></pre></p> </li> <li> <p>Access Protected Endpoints:</p> <ul> <li>Use the JWT token in the <code>Authorization</code> header for subsequent API requests.</li> </ul> </li> </ol>"},{"location":"JWT/#benefits-of-jwt-authentication","title":"Benefits of JWT Authentication","text":"<ul> <li>Stateless and scalable (no server-side session storage)</li> <li>Secure (signed and optionally encrypted tokens)</li> <li>Flexible (supports custom claims)</li> <li>Interoperable across platforms and services</li> </ul>"},{"location":"Knowledge_Base/","title":"Knowledge Base Integration","text":"<p>A knowledge base is a structured repository of information that agents can use to enhance their reasoning, provide more accurate answers, and support advanced use cases. Integrating knowledge bases into Agentic Foundry allows agents to access domain-specific data, documents, or FAQs during inference.</p> <p>A knowledge base serves as an external memory for agents, enabling them to:</p> <ul> <li>Retrieve factual information and context.</li> <li>Provide consistent, reliable, and context-aware answers to user queries.</li> <li>Support advanced reasoning, document search, and question answering workflows.</li> </ul>"},{"location":"Knowledge_Base/#creating-a-knowledge-base","title":"Creating a Knowledge Base","text":"<p>Knowledge bases are created and managed from the Tool Page. This interface allows you to upload documents (PDF, TXT), enter a name for the new knowledge base, and add it to the centralized directory.</p>"},{"location":"Knowledge_Base/#using-knowledge-bases-in-inference","title":"Using Knowledge Bases in Inference","text":"<p>During chat inference, one or multiple knowledge bases can be selected for the agent to reference while answering queries. This is done using a dedicated icon in the chat interface, allowing the agent's knowledge to be tailored to the context of the question.</p> <p>Selecting the appropriate knowledge base(s) ensures the agent provides contextually accurate and precise answers, which is especially useful for organizations with multiple knowledge bases serving different departments or domains.</p>"},{"location":"Knowledge_Base/#execution-steps-how-the-agent-uses-knowledge-bases","title":"Execution Steps: How the Agent Uses Knowledge Bases","text":"<p>When a query is submitted, the agent analyzes the question and determines which of the selected knowledge bases are most relevant. The agent then retrieves information from the appropriate knowledge base(s) to construct its response.</p> <p>The execution steps can be viewed to see exactly how the agent is referencing the correct knowledge base for the query. This transparency helps users understand the reasoning process and verify that the agent is using the intended sources.</p> <p>For example, if a technical question about a product is asked, the agent will reference the product documentation knowledge base. If the query is about company policy, the HR or policy knowledge base will be used, provided both are selected.</p>"},{"location":"MCP_Registry/","title":"MCP Registry: Complete Overview","text":"<p>This document provides a comprehensive, single-file summary of the Infosys Agent Framework (IAF) MCP Integration. It is designed to give any reader a clear understanding of the Model Context Protocol (MCP) system, its architecture, features, API, setup, and configuration.</p>"},{"location":"MCP_Registry/#what-is-mcp","title":"What is MCP?","text":"<p>The Model Context Protocol (MCP) is a standardized protocol that enables AI agents to securely connect to and interact with external data sources, tools, and services. The IAF MCP Integration extends this protocol for enterprise-grade deployments, supporting robust tool discovery, security, and management.</p>"},{"location":"MCP_Registry/#key-features","title":"Key Features","text":"<ul> <li>Multi-server Support: Connect to multiple MCP servers (local, remote, file-based) simultaneously.</li> <li>Real-time Tool Discovery: Dynamic discovery and validation of tools from running servers.</li> <li>Type Safety: Strict typing and validation for all MCP operations.</li> <li>Enterprise Security: Approval workflows, permissions, and audit trails.</li> <li>Monitoring: Built-in telemetry, performance tracking, and health checks.</li> <li>Multi-tenant Support: Isolated environments for different users.</li> <li>Horizontal Scaling: Designed for high-throughput, scalable environments.</li> <li>Error Recovery: Robust error handling and retry mechanisms.</li> </ul>"},{"location":"MCP_Registry/#architecture-overview","title":"Architecture Overview","text":"<p>The IAF MCP Integration is built on a modular, layered architecture:</p> <ul> <li>Client Layer: Web interface, API clients, and CLI tools for user interaction.</li> <li>API Layer: FastAPI application and routers for handling requests.</li> <li>Service Layer: Services for managing MCP tools, agents, and tags.</li> <li>Repository Layer: Database access and management (PostgreSQL, file system).</li> <li>MCP Runtime Layer: Manages connections to multiple MCP servers and tool execution.</li> </ul> <p>This design ensures secure integration, high availability, and efficient resource utilization.</p>"},{"location":"MCP_Registry/#api-endpoints","title":"API Endpoints","text":"<p>The system exposes RESTful API endpoints for managing MCP tools and servers:</p> <ul> <li>Add, update, delete, and list MCP tools (local, remote, file-based)</li> <li>Support for file uploads, module references, and URL-based tools</li> <li>Comprehensive OpenAPI documentation</li> <li>Approval workflows and audit logging for all operations</li> </ul>"},{"location":"MCP_Registry/#configuration","title":"Configuration","text":"<ul> <li>Environment Variables: Highest priority for configuration.</li> <li>Configuration Files: .env and config.yaml for environment-specific settings.</li> <li>Default Values: Used if no overrides are provided.</li> <li>Database Settings: PostgreSQL connection, pool size, retry logic.</li> <li>Application Settings: App name, version, debug mode, host, and port.</li> </ul>"},{"location":"MCP_Registry/#how-it-all-fits-together","title":"How It All Fits Together","text":"<p>The IAF MCP Integration provides a secure, scalable, and flexible foundation for connecting AI agents to a wide range of external tools and services. Its modular architecture, robust API, and enterprise features make it suitable for production deployments in complex environments. With comprehensive documentation, quick start guides, and detailed configuration options, teams can rapidly onboard, manage, and scale their AI agent integrations.</p>"},{"location":"Model_server/","title":"Model Server","text":"<p>To optimize the deployment and usage of bi-encoder and cross-encoder models, we have enhanced our integration to support accessing hosted models via server URLs. Previously, models had to be manually downloaded from Hugging Face and their local paths specified in the <code>.env</code> file. This process was inefficient, consuming significant disk space (several GBs per model) and complicating deployments across multiple virtual machines (VMs), as each VM required its own copy of the models.</p> <p>To resolve these challenges, we introduced a model server based on FastAPI. With this architecture, models are hosted on a single machine (either local or remote), and clients connect to the model server by specifying its URL in their <code>.env</code> file. This eliminates redundant downloads and local storage, streamlining both setup and ongoing maintenance.</p> <p>Note</p> <p>To enable or use features such as Groundtruth Evaluation, Episodic Memory, Semantic Memory, and Knowledge Base, you need to set up the Model Server. This is especially required when local models are used for embeddings and reranking operations.</p>"},{"location":"Model_server/#how-it-works","title":"How It Works","text":"<ol> <li> <p>Model Server Deployment:     Deploy the model server script on the machine designated to host your Hugging Face models.</p> </li> <li> <p>Client Configuration:     On each client, specify the model server URL in the <code>.env</code> file. The application will then access models via the server, rather than loading them locally.</p> </li> <li> <p>Centralized Model Management:     All model-related operations (such as generating embeddings or reranking candidates) are handled by the server, ensuring consistency and efficiency across all environments.</p> </li> </ol> <p>The model server hosts both bi-encoder and cross-encoder models, allowing clients to request embeddings and reranking results through simple API calls. The client connects to the server, requests embeddings from the bi-encoder, and performs reranking with the cross-encoder, all via HTTP endpoints.</p>"},{"location":"Model_server/#example-model-server-script","title":"Example Model Server Script","text":"<p>Below is an example FastAPI script for hosting the <code>all-MiniLM-L6-v2</code> (bi-encoder) and <code>bge-reranker-large</code> (cross-encoder) models. Run this script on the server where the Hugging Face models will be hosted:</p> <p><pre><code>\"\"\"\nModel Server for hosting all-MiniLM-L6-v2 and bge-reranker-large models\n\"\"\"\n\nimport os\nfrom contextlib import asynccontextmanager\nfrom typing import List, Union\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nimport uvicorn\nfrom sentence_transformers import SentenceTransformer, CrossEncoder\nimport logging\nfrom dotenv import load_dotenv\nimport numpy as np\nimport torch\n\nload_dotenv()\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Global model variables\nembedding_model = None\ncross_encoder_model = None\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    \"\"\"Load models on startup and cleanup on shutdown\"\"\"\n    global embedding_model, cross_encoder_model\n    try:\n        embedding_model_name = os.getenv(\"SBERT_MODEL_PATH\")\n        cross_encoder_model_name = os.getenv(\"CROSS_ENCODER_PATH\")\n        embedding_model = SentenceTransformer(embedding_model_name)\n        logger.info(\"Embedding model loaded successfully\")\n        cross_encoder_model = CrossEncoder(cross_encoder_model_name)\n        logger.info(\"Cross encoder model loaded successfully\")\n        logger.info(\"All models loaded successfully!\")\n    except Exception as e:\n        logger.error(f\"Failed to load models: {str(e)}\")\n        raise e\n    yield\n    logger.info(\"Shutting down model server...\")\n\napp = FastAPI(title=\"Model Server\", version=\"1.0.0\", lifespan=lifespan)\n\n# Request/Response models\nclass EmbeddingRequest(BaseModel):\n    texts: Union[str, List[str]]\n    convert_to_tensor: bool = False\n\nclass EmbeddingResponse(BaseModel):\n    embeddings: List[List[float]]\n\nclass RerankRequest(BaseModel):\n    query: str\n    candidates: List[str]\n\nclass RerankResponse(BaseModel):\n    scores: List[float]\n\nclass CosineSimilarityRequest(BaseModel):\n    vector_a: List[float]\n    vector_b: Union[List[float], List[List[float]]]\n\nclass CosineSimilarityResponse(BaseModel):\n    similarity: Union[float, List[float]]\n\nclass TensorOpsRequest(BaseModel):\n    data: Union[List[float], List[List[float]]]\n    operation: str\n    kwargs: dict = {}\n\nclass TensorOpsResponse(BaseModel):\n    result: Union[List[float], List[List[float]], float, bool]\n\nclass ArrayOpsRequest(BaseModel):\n    data: Union[List[float], List[List[float]]]\n    operation: str\n    kwargs: dict = {}\n\nclass ArrayOpsResponse(BaseModel):\n    result: Union[List[float], List[List[float]], float, bool]\n\n@app.get(\"/health\")\nasync def health_check():\n    return {\n        \"status\": \"healthy\",\n        \"embedding_model_loaded\": embedding_model is not None,\n        \"cross_encoder_loaded\": cross_encoder_model is not None\n    }\n\n@app.post(\"/embeddings\", response_model=EmbeddingResponse)\nasync def get_embeddings(request: EmbeddingRequest):\n    if embedding_model is None:\n        raise HTTPException(status_code=500, detail=\"Embedding model not loaded\")\n    try:\n        texts = request.texts if isinstance(request.texts, list) else [request.texts]\n        embeddings = embedding_model.encode(texts, show_progress_bar=False)\n        if len(embeddings.shape) == 1:\n            embeddings = [embeddings.tolist()]\n        else:\n            embeddings = embeddings.tolist()\n        return EmbeddingResponse(embeddings=embeddings)\n    except Exception as e:\n        logger.error(f\"Error generating embeddings: {str(e)}\")\n        raise HTTPException(status_code=500, detail=f\"Error generating embeddings: {str(e)}\")\n\n@app.post(\"/rerank\", response_model=RerankResponse)\nasync def rerank_texts(request: RerankRequest):\n    if cross_encoder_model is None:\n        raise HTTPException(status_code=500, detail=\"Cross encoder model not loaded\")\n    try:\n        pairs = [[request.query, candidate] for candidate in request.candidates]\n        scores = cross_encoder_model.predict(pairs)\n        if hasattr(scores, 'tolist'):\n            scores = scores.tolist()\n        return RerankResponse(scores=scores)\n    except Exception as e:\n        logger.error(f\"Error in reranking: {str(e)}\")\n        raise HTTPException(status_code=500, detail=f\"Error in reranking: {str(e)}\")\n\n@app.post(\"/cosine_similarity\", response_model=CosineSimilarityResponse)\nasync def calculate_cosine_similarity(request: CosineSimilarityRequest):\n    \"\"\"Calculate cosine similarity between vectors\"\"\"\n    try:\n        vector_a = np.array(request.vector_a)\n        if isinstance(request.vector_b[0], list):\n            similarities = []\n            for vec_b in request.vector_b:\n                vector_b = np.array(vec_b)\n                norm_a = np.linalg.norm(vector_a)\n                norm_b = np.linalg.norm(vector_b)\n                if norm_a == 0 or norm_b == 0:\n                    similarity = 0.0\n                else:\n                    similarity = np.dot(vector_a, vector_b) / (norm_a * norm_b)\n                similarities.append(float(similarity))\n            return CosineSimilarityResponse(similarity=similarities)\n        else:\n            vector_b = np.array(request.vector_b)\n            norm_a = np.linalg.norm(vector_a)\n            norm_b = np.linalg.norm(vector_b)\n            if norm_a == 0 or norm_b == 0:\n                similarity = 0.0\n            else:\n                similarity = np.dot(vector_a, vector_b) / (norm_a * norm_b)\n            return CosineSimilarityResponse(similarity=float(similarity))\n\n    except Exception as e:\n        logger.error(f\"Error calculating cosine similarity: {str(e)}\")\n        raise HTTPException(status_code=500, detail=f\"Error calculating cosine similarity: {str(e)}\")\n\n@app.post(\"/tensor_ops\", response_model=TensorOpsResponse)\nasync def tensor_operations(request: TensorOpsRequest):\n    \"\"\"Handle tensor operations to replace torch operations\"\"\"\n    try:\n        data = request.data\n        operation = request.operation\n        kwargs = request.kwargs\n\n        if operation == \"to_tensor\":\n            tensor = torch.tensor(data)\n            result = tensor.tolist()\n        elif operation == \"sigmoid\":\n            tensor = torch.tensor(data)\n            result = torch.sigmoid(tensor).tolist()\n        elif operation == \"is_tensor\":\n            result = False\n        elif operation == \"flatten\":\n            if isinstance(data, list) and len(data) &gt; 0 and isinstance(data[0], list):\n                result = [item for sublist in data for item in sublist]\n            else:\n                result = data\n        else:\n            logger.warning(f\"Unknown tensor operation: {operation}\")\n            result = data\n\n        return TensorOpsResponse(result=result)\n\n    except Exception as e:\n        logger.error(f\"Error in tensor operation {request.operation}: {str(e)}\")\n        raise HTTPException(status_code=500, detail=f\"Error in tensor operation: {str(e)}\")\n\n@app.post(\"/array_ops\", response_model=ArrayOpsResponse)\nasync def array_operations(request: ArrayOpsRequest):\n    \"\"\"Handle array operations to replace numpy operations\"\"\"\n    try:\n        data = request.data\n        operation = request.operation\n        kwargs = request.kwargs\n\n        if operation == \"create_array\":\n            result = data\n        elif operation == \"mean\":\n            if isinstance(data[0], list):\n                axis = kwargs.get('axis', None)\n                np_array = np.array(data)\n                if axis is not None:\n                    result = np.mean(np_array, axis=axis).tolist()\n                else:\n                    result = float(np.mean(np_array))\n            else:\n                result = float(np.mean(data))\n        elif operation == \"std\":\n            if isinstance(data[0], list):\n                np_array = np.array(data)\n                axis = kwargs.get('axis', None)\n                if axis is not None:\n                    result = np.std(np_array, axis=axis).tolist()\n                else:\n                    result = float(np.std(np_array))\n            else:\n                result = float(np.std(data))\n        elif operation == \"reshape\":\n            shape = kwargs.get('shape', (-1,))\n            np_array = np.array(data)\n            result = np_array.reshape(shape).tolist()\n        elif operation == \"transpose\":\n            np_array = np.array(data)\n            result = np_array.T.tolist()\n        else:\n            logger.warning(f\"Unknown array operation: {operation}\")\n            result = data\n\n        return ArrayOpsResponse(result=result)\n\n    except Exception as e:\n        logger.error(f\"Error in array operation {request.operation}: {str(e)}\")\n        raise HTTPException(status_code=500, detail=f\"Error in array operation: {str(e)}\")\n\nif __name__ == \"__main__\":\n    host = os.getenv(\"MODEL_SERVER_HOST\")\n    port = int(os.getenv(\"MODEL_SERVER_PORT\"))\n\n    logger.info(f\"Starting Model Server on {host}:{port}\")\n    uvicorn.run(\"model_server:app\", host=host, port=port, reload=False)\n</code></pre> Below is the script for the Model Client, which enables communication with the FastAPI model server hosting two models: <code>all-MiniLM-L6-v2</code> (bi-encoder) and <code>bge-reranker-large</code> (cross-encoder). This client allows you to connect to the server, request embeddings from the bi-encoder, and perform reranking with the cross-encoder, all via simple API calls.</p> <pre><code>\"\"\"\nModel Client for communicating with the FastAPI model server\n\"\"\"\nimport os\nimport requests\nfrom typing import List, Union, Any\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass ModelServerClient:\n    \"\"\"Client for communicating with the model server\"\"\"\n    _warning_logged = False  \n    _connection_failed = {}  \n\n    def __init__(self, base_url: str = None):\n        self.base_url = base_url or os.getenv(\"MODEL_SERVER_URL\")\n        if self.base_url:\n            self.base_url = self.base_url.strip()\n            if not self.base_url or self.base_url.lower() == \"none\":\n                self.base_url = None\n\n        self.session = requests.Session()\n        self.server_available = False\n\n        if not self.base_url:\n            if not ModelServerClient._warning_logged:\n                logger.info(\"MODEL_SERVER_URL not configured. Remote model features will be unavailable.\")\n                ModelServerClient._warning_logged = True\n            return\n\n        try:\n            response = self.session.get(f\"{self.base_url}/health\", timeout=5)\n            if response.status_code == 200:\n                if self.base_url in ModelServerClient._connection_failed:\n                    del ModelServerClient._connection_failed[self.base_url]\n                logger.info(f\"Connected to model server at {self.base_url}\")\n                self.server_available = True\n            else:\n                if self.base_url not in ModelServerClient._connection_failed:\n                    logger.warning(f\"Model server at {self.base_url} responded with status {response.status_code}, remote features unavailable\")\n                    ModelServerClient._connection_failed[self.base_url] = True\n        except Exception as e:\n            if self.base_url not in ModelServerClient._connection_failed:\n                logger.error(f\"Failed to connect to model server at {self.base_url}: {e}\")\n                ModelServerClient._connection_failed[self.base_url] = True\n\nclass RemoteUtils:\n    \"\"\"Remote utilities to replace torch, numpy operations and sentence-transformers utilities\"\"\"\n\n    def __init__(self, client: ModelServerClient = None):\n        self.client = client or ModelServerClient()\n\n    def cos_sim(self, a: List[float], b: Union[List[float], List[List[float]]]) -&gt; Union[float, List[float]]:\n        \"\"\"Remote cosine similarity calculation to replace sentence_transformers.util.cos_sim\"\"\"\n        if not self.client.base_url or self.client.base_url == \"None\":\n            raise Exception(\"MODEL_SERVER_URL not configured. Cannot perform cosine similarity without remote server.\")\n        try:\n            def flatten_embedding(embedding):\n                if isinstance(embedding, list) and len(embedding) &gt; 0:\n                    if isinstance(embedding[0], list):\n                        return embedding[0]\n                    else:\n                        return embedding\n                return embedding\n            vector_a = flatten_embedding(a)\n            vector_b = b\n            if isinstance(b, list) and len(b) &gt; 0:\n                if isinstance(b[0], list):\n                    if len(b) == 1 and isinstance(b[0][0], (int, float)):\n                        vector_b = b[0]\n                    elif isinstance(b[0][0], list):\n                        vector_b = [flatten_embedding(vec) for vec in b]\n                    else:\n                        vector_b = b\n            payload = {\n                \"vector_a\": vector_a,\n                \"vector_b\": vector_b\n            }\n            response = self.client.session.post(\n                f\"{self.client.base_url}/cosine_similarity\",\n                json=payload,\n                timeout=30\n            )\n            if response.status_code != 200:\n                raise Exception(f\"Model server error: {response.status_code} - {response.text}\")\n            result = response.json()\n            return result[\"similarity\"]\n        except Exception as e:\n            logger.error(f\"Error calculating cosine similarity: {e}\")\n            raise e\n\n    def tensor_operations(self, data: Any, operation: str, **kwargs) -&gt; Any:\n        \"\"\"Remote tensor operations to replace torch operations\"\"\"\n        if not self.client.base_url or self.client.base_url == \"None\":\n            raise Exception(\"MODEL_SERVER_URL not configured. Cannot perform tensor operations without remote server.\")\n        try:\n            payload = {\n                \"data\": data,\n                \"operation\": operation,\n                \"kwargs\": kwargs\n            }\n            response = self.client.session.post(\n                f\"{self.client.base_url}/tensor_ops\",\n                json=payload,\n                timeout=30\n            )\n            if response.status_code != 200:\n                raise Exception(f\"Model server error: {response.status_code} - {response.text}\")\n            result = response.json()\n            return result[\"result\"]\n        except Exception as e:\n            logger.error(f\"Error performing tensor operation {operation}: {e}\")\n            raise e\n\n    def array_operations(self, data: Any, operation: str, **kwargs) -&gt; Any:\n        \"\"\"Remote array operations to replace numpy operations\"\"\"\n        if not self.client.base_url or self.client.base_url == \"None\":\n            raise Exception(\"MODEL_SERVER_URL not configured. Cannot perform array operations without remote server.\")\n        try:\n            payload = {\n                \"data\": data,\n                \"operation\": operation,\n                \"kwargs\": kwargs\n            }\n            response = self.client.session.post(\n                f\"{self.client.base_url}/array_ops\",\n                json=payload,\n                timeout=30\n            )\n            if response.status_code != 200:\n                raise Exception(f\"Model server error: {response.status_code} - {response.text}\")\n            result = response.json()\n            return result[\"result\"]\n        except Exception as e:\n            logger.error(f\"Error performing array operation {operation}: {e}\")\n            raise e\n\nclass RemoteSentenceTransformer:\n    \"\"\"Drop-in replacement for SentenceTransformer\"\"\"\n\n    def __init__(self, model_name: str = None, client: ModelServerClient = None):\n        self.model_name = model_name\n        self.client = client or ModelServerClient()\n\n    def encode(self, sentences: Union[str, List[str]], \n               convert_to_tensor: bool = False, \n               show_progress_bar: bool = False,\n               **kwargs) -&gt; Union[List[List[float]], List[float]]:\n        try:\n            payload = {\n                \"texts\": sentences,\n                \"convert_to_tensor\": False\n            }\n            response = self.client.session.post(\n                f\"{self.client.base_url}/embeddings\",\n                json=payload,\n                timeout=30\n            )\n            if response.status_code != 200:\n                raise Exception(f\"Model server error: {response.status_code} - {response.text}\")\n            result = response.json()\n            embeddings = result[\"embeddings\"]\n            if isinstance(sentences, str):\n                return embeddings[0] if embeddings else []\n            else:\n                return embeddings\n        except Exception as e:\n            logger.error(f\"Error encoding sentences: {e}\")\n            raise e\n\nclass RemoteCrossEncoder:\n    \"\"\"Drop-in replacement for CrossEncoder\"\"\"\n\n    def __init__(self, model_name: str = None, client: ModelServerClient = None):\n        self.model_name = model_name\n        self.client = client or ModelServerClient()\n\n    def predict(self, sentences: List[List[str]], **kwargs) -&gt; Union[List[float], float]:\n        try:\n            if len(sentences) == 0:\n                return []\n            if isinstance(sentences[0], str):\n                query, candidates = sentences[0], [sentences[1]]\n            else:\n                query = sentences[0][0]\n                candidates = [pair[1] for pair in sentences]\n            payload = {\n                \"query\": query,\n                \"candidates\": candidates\n            }\n            response = self.client.session.post(\n                f\"{self.client.base_url}/rerank\",\n                json=payload,\n                timeout=30\n            )\n            if response.status_code != 200:\n                raise Exception(f\"Model server error: {response.status_code} - {response.text}\")\n            result = response.json()\n            scores = result[\"scores\"]\n            if isinstance(sentences[0], str):\n                return scores[0] if scores else 0.0\n            return scores\n\n        except Exception as e:\n            logger.error(f\"Error predicting with cross encoder: {e}\")\n            raise e\n\nclass RemoteTensorUtils:\n    \"\"\"Utility class to replace torch tensor operations with remote calls\"\"\"\n\n    def __init__(self, client: ModelServerClient = None):\n        self.client = client or ModelServerClient()\n        self.remote_utils = RemoteUtils(client)\n\n    def tensor(self, data: List) -&gt; List:\n        \"\"\"Replace torch.tensor() with remote operation\"\"\"\n        return self.remote_utils.tensor_operations(data, \"to_tensor\")\n\n    def sigmoid(self, data: List) -&gt; List:\n        \"\"\"Replace torch.sigmoid() with remote operation\"\"\"\n        return self.remote_utils.tensor_operations(data, \"sigmoid\")\n\n    def is_tensor(self, obj: Any) -&gt; bool:\n        \"\"\"Check if object is tensor-like (in remote setup, check if it's a list of numbers)\"\"\"\n        return isinstance(obj, (list, tuple)) and len(obj) &gt; 0 and isinstance(obj[0], (int, float))\n\nclass RemoteNumpyUtils:\n    \"\"\"Utility class to replace numpy operations with remote calls\"\"\"\n\n    def __init__(self, client: ModelServerClient = None):\n        self.client = client or ModelServerClient()\n        self.remote_utils = RemoteUtils(client)\n\n    def array(self, data: List) -&gt; List:\n        \"\"\"Replace numpy.array() with remote operation\"\"\"\n        return self.remote_utils.array_operations(data, \"create_array\")\n\nclass RemoteSentenceTransformersUtil:\n    \"\"\"Utility class to replace sentence_transformers.util operations\"\"\"\n\n    def __init__(self, client: ModelServerClient = None):\n        self.client = client or ModelServerClient()\n        self.remote_utils = RemoteUtils(client)\n\n    def cos_sim(self, a: List[float], b: Union[List[float], List[List[float]]]) -&gt; Union[float, List[float]]:\n        \"\"\"Replace sentence_transformers.util.cos_sim with remote calculation\"\"\"\n        return self.remote_utils.cos_sim(a, b)\n\ndef get_remote_models_and_utils(base_url: str = None):\n    \"\"\"Factory function to get all remote model instances and utilities\"\"\"\n    client = ModelServerClient(base_url)\n    embedding_model = RemoteSentenceTransformer(client=client)\n    cross_encoder = RemoteCrossEncoder(client=client)\n    torch_utils = RemoteTensorUtils(client=client)\n    numpy_utils = RemoteNumpyUtils(client=client) \n    util = RemoteSentenceTransformersUtil(client=client)\n    logger.info(\"Remote models and utilities initialized successfully.\")\n    return {\n        \"embedding_model\": embedding_model,\n        \"cross_encoder\": cross_encoder,\n        \"torch\": torch_utils,\n        \"np\": numpy_utils,\n        \"util\": util,\n        \"client\": client\n    }\n\ndef get_remote_models(base_url: str = None):\n    \"\"\"Original function for backwards compatibility\"\"\"\n    components = get_remote_models_and_utils(base_url)\n    return components[\"embedding_model\"], components[\"cross_encoder\"]\n</code></pre>"},{"location":"Model_server/#model-server-setup-localvm-deployment","title":"Model Server Setup (Local/VM Deployment)","text":"<p>Use this approach when you want to host the embedding and reranker models on a dedicated server (localhost or VM).</p> <p>Local/VM Model Server Deployment</p> <p>1. Setup Model Server Environment</p> <ul> <li>Configure a model server (either on <code>localhost</code> or a dedicated VM). Ensure it runs on a distinct port from your primary application server.</li> <li>Verify the server machine possesses adequate RAM resources for efficient model inference.</li> </ul> <p>2. Download Required Models Manually</p> <p>Download both models manually to avoid SSL connectivity issues:</p> <ul> <li><code>all-MiniLM-L6-v2</code> model: Download Link</li> <li><code>bge-reranker-large</code> model: Download Link</li> </ul> <p>After downloading, extract both <code>.zip</code> archives into a chosen directory on your server machine.</p> <p>3. Configure Environment Variables (<code>.env</code>)</p> <p>Update your <code>.env</code> file to point to the local paths of your extracted models:</p> <pre><code>SBERT_MODEL_PATH=path/to/your/folder/all-MiniLM-L6-v2\nCROSS_ENCODER_PATH=path/to/your/folder/bge-reranker-large\n</code></pre> <p>Important</p> <p>Replace the placeholder paths with the actual folder paths where you extracted the models.</p> <p>4. Start Model Server</p> <p>Execute the <code>model_server.py</code> script to launch your model server. This will load the models into memory and expose them via API endpoints.</p> <p>Start the server with: <pre><code>python model_server.py\n</code></pre> Keep this process running for the models to remain available.</p> <p>Connect to Model Server from Backend</p> <p>Update <code>.env</code> file of backend with the model server details:</p> <pre><code>MODEL_SERVER_URL=\"http://your-model-server-ip:port\"\nMODEL_SERVER_HOST=\"your-model-server-ip\"   # Often derived from MODEL_SERVER_URL\nMODEL_SERVER_PORT=\"port_number\"            # Often derived from MODEL_SERVER_URL\n</code></pre> <p>Tip</p> <p>Always ensure correct paths and server details are supplied for seamless connectivity and model inference.</p>"},{"location":"Model_server/#key-benefits","title":"Key Benefits","text":"<ul> <li>Reduced Storage Requirements: Models are downloaded and stored only once on the server.</li> <li>Simplified Deployment: No need to manage model files on every VM or environment.</li> <li>Centralized Updates: Updating a model on the server instantly benefits all connected clients.</li> <li>Scalability: Multiple clients can access the same models concurrently via the server API.</li> </ul> <p>This architecture ensures efficient, scalable, and maintainable model usage across your infrastructure.</p>"},{"location":"RBAC/","title":"Role Base Access Control","text":"<p>Users with the <code>user</code> role cannot access the Tools, Agents, Vault and Data Connectors pages. They are allowed to interact with agents on the Inference page but cannot view execution steps or use the tool verifier and plan verifier features.</p> <p>More features will be added soon.</p>"},{"location":"SSE/","title":"Server-Sent Events (SSE) Streaming","text":"<p>Server-Sent Events (SSE) is a web technology that enables real-time, one-way communication from the server to the client. In the context of the Infy Agent Framework, SSE streams each step of the agent's internal processing to the user interface as it happens, providing complete visibility into the agent's execution pipeline.</p>"},{"location":"SSE/#the-importance-of-sse-in-agent-frameworks","title":"The Importance of SSE in Agent Frameworks","text":"<p>Traditional agent interactions create a <code>black box</code> experience where users submit a query and wait for the final response with no insight into the agent's progress. This approach has several limitations:</p> <ul> <li>Users cannot determine if the agent is actively working, stuck, or encountering errors</li> <li>Debugging becomes difficult without visibility into the processing steps</li> <li>Long-running queries feel unresponsive and may cause users to assume the system has failed</li> <li>There's no way to monitor which tools are being executed or validate their usage</li> </ul> <p>SSE streaming addresses these challenges by providing continuous updates throughout the agent's execution lifecycle. Users receive real-time information about which processing stage the agent is currently in, what tools are being called with their specific parameters, the output returned by each node, and confirmation when the agent completes each step.</p> <p>This transparency significantly improves user trust by making the agent's decision-making process visible, enables faster debugging by showing exactly where issues occur, and creates a more responsive experience even for complex, multi-step workflows.</p>"},{"location":"SSE/#benefits-of-sse-implementation","title":"Benefits of SSE Implementation","text":"<p>The implementation of SSE streaming transforms the user experience in several key ways:</p> <ul> <li> <p>Enhanced User Experience: Instead of waiting for a final response with no feedback, users see each processing step unfold in real-time, creating an interactive and engaging experience.</p> </li> <li> <p>Complete Visibility: The entire agent execution pipeline becomes transparent, allowing users to understand how their query is being processed and what decisions the agent is making.</p> </li> <li> <p>Improved Debugging: When issues arise, developers and users can immediately identify where in the process the failure occurred, dramatically reducing troubleshooting time.</p> </li> <li> <p>Better Perceived Performance: Even though the actual processing time may be the same, users perceive the system as faster and more responsive because they can see continuous progress.</p> </li> </ul>"},{"location":"SSE/#agent-templates-supporting-sse","title":"Agent Templates Supporting SSE","text":"<p>SSE streaming has been implemented across all major agent templates in the framework, each with specific streaming nodes tailored to their execution patterns:</p> <ul> <li> <p>React Agent provides streaming for Generating Context, Thinking..., Tool Call, Evaluating Response, Validating Response, and Memory Updation nodes. This template focuses on reactive decision-making with real-time tool execution visibility.</p> </li> <li> <p>React-Critic Agent extends the React template by adding a Reviewing Response node, allowing users to see both the initial agent reasoning and the subsequent critical review process.</p> </li> <li> <p>Planner-Executor Agent streams Generating Context, Generating Plan, Processing..., Replanning, Generating Response, and Memory Updation. This template emphasizes planning visibility, showing users how the agent creates and executes multi-step plans.</p> </li> <li> <p>Planner-Executor-Critic Agent combines planning with critical review, adding a Reviewing Response node to provide oversight of the planned execution.</p> </li> <li> <p>Meta Agent focuses on agent orchestration with streaming for Generating Context, Thinking..., Agent Call, Evaluating Response, and Memory Updation, allowing users to see how the meta-agent delegates tasks to sub-agents.</p> </li> <li> <p>Planner-Meta Agent merges planning with meta-agent capabilities, streaming Generating Context, Generating Plan, Agent Call, Generating Final Response, and Memory Updation to show both the planning and delegation processes.</p> </li> <li> <p>Hybrid Agent supports SSE streaming with nodes for Generating Context, Generating Plan, Processing..., Replanning, Generating Response, and Memory Updation. This pure Python-based template provides real-time visibility into both planning and execution phases within a single agent.</p> </li> </ul> <p>SSE Event Structure and Content</p> <p>Each SSE event transmitted to the client contains structured information designed to provide meaningful insights into the agent's current state. The events include the node name identifying the current processing stage, a status indicator showing whether the node has started, completed, or failed, and relevant content such as tool arguments, execution results, or context summaries.</p>"},{"location":"SSE/#detailed-node-descriptions","title":"Detailed Node Descriptions","text":"<p>The streaming nodes represent different stages of agent processing, each serving a specific purpose in the execution pipeline:</p> <ul> <li> <p>Generating Context handles the loading and summarization of conversation history, ensuring the agent has proper context for decision-making. Users can see how previous interactions influence current processing.</p> </li> <li> <p>Thinking... represents the agent's reasoning and decision-making process, showing users the internal logic and considerations that drive the agent's actions.</p> </li> <li> <p>Tool Call streams the execution of external tools or APIs, including the parameters being passed and the results returned, providing complete visibility into external system interactions.</p> </li> <li> <p>Evaluating Response occurs when response evaluation is enabled, showing users how the agent assesses the quality and appropriateness of generated responses.</p> </li> <li> <p>Validating Response provides insight into the validation process when response validation is configured, ensuring outputs meet specified criteria.</p> </li> <li> <p>Reviewing Response appears in critic-enabled agents, streaming the critical review process that evaluates and potentially improves the initial response.</p> </li> <li> <p>Generating Plan is specific to planner-based agents, showing users how multi-step plans are created and structured.</p> </li> <li> <p>Processing... streams the step-by-step execution of plans, allowing users to track progress through complex workflows.</p> </li> <li> <p>Replanning shows when and how agents revise their plans based on execution feedback, providing insight into adaptive behavior.</p> </li> <li> <p>Agent Call occurs in meta-agents when sub-agents are invoked, showing the delegation and coordination between different agent instances.</p> </li> <li> <p>Generating Response streams the final response formatting process, showing how raw outputs are refined into user-friendly responses.</p> </li> <li> <p>Generating Final Response is specific to Planner-Meta agents, combining outputs from multiple sources into a cohesive final response.</p> </li> <li> <p>Memory Updation shows when conversation history is being saved to memory, ensuring users understand when their interactions are being preserved.</p> </li> </ul>"},{"location":"SSE/#practical-applications-and-use-cases","title":"Practical Applications and Use Cases","text":"<p>SSE streaming proves particularly valuable in several scenarios:</p> <ul> <li> <p>For <code>long-running queries</code>, SSE eliminates the anxiety of waiting by showing continuous progress instead of a static loading screen. Users remain engaged and confident that processing is occurring.</p> </li> <li> <p>In <code>multi-tool workflows</code>, SSE provides visibility into tool execution order and dependencies, helping users understand complex processing chains and identify optimization opportunities.</p> </li> <li> <p>For <code>debugging agent behavior</code>, SSE enables precise identification of failure points, dramatically reducing the time needed to diagnose and resolve issues.</p> </li> <li> <p>In <code>production monitoring</code>, SSE offers real-time visibility into agent performance, allowing administrators to detect and address issues before they impact user experience.</p> </li> <li> <p>For <code>human-in-the-loop workflows</code>, particularly when Tool Interrupt is enabled, SSE shows tool arguments before execution, allowing human operators to review and approve actions before they occur.</p> </li> </ul>"},{"location":"SSE/#implementation-benefits","title":"Implementation Benefits","text":"<ul> <li> <p>The SSE implementation in the Infy Agent Framework represents a significant advancement in agent transparency and user experience. By providing real-time visibility into agent execution across all six supported templates, users gain unprecedented insight into how their queries are processed, what decisions are made, and how results are generated.</p> </li> <li> <p>This transparency not only improves user confidence and satisfaction but also enables more effective debugging, monitoring, and optimization of agent workflows. The consistent implementation across different agent types ensures that users can expect the same level of visibility regardless of which template best suits their needs.</p> </li> <li> <p>The result is a more interactive, transparent, and trustworthy agent framework that bridges the gap between complex AI processing and human understanding, making sophisticated agent capabilities accessible and comprehensible to users at all technical levels.</p> </li> </ul>"},{"location":"Time-To-Live/","title":"Time-To-Live (TTL) Implementation","text":"<p>The Time-To-Live (TTL) feature is implemented to manage the lifecycle of both short-term and long-term memory records within the system. This ensures efficient storage management and compliance with data retention policies.</p> <p>The agent conversation summary table maintains the <code>agent Id</code> and <code>session Id</code> for each chat session, storing a summary of the conversation. This table also includes an <code>updated on</code> column to track the last activity. If the <code>updated on</code> timestamp for a record is older than 30 days, both the short-term and long-term memory records associated with that <code>agent Id</code> and <code>session Id</code> are deleted as part of the TTL process.</p>"},{"location":"Time-To-Live/#short-term-memory-checkpoints","title":"Short-Term Memory (Checkpoints)","text":"<p>Short-term memory is managed through checkpoint tables (<code>checkpoints</code>, <code>checkpoint_writes</code>, and <code>checkpoint_blobs</code>). These checkpoints store chat history, which is displayed in the Inference Chat History tab.</p> <p>Deletion Policy</p> <ul> <li><code>Checkpoints older than 30 days</code> are automatically deleted from the database.</li> <li>Before deletion, records are moved to the <code>Recycle</code>, where they are retained for 15 days. During this period, records can be restored if needed.</li> </ul>"},{"location":"Time-To-Live/#long-term-memory","title":"Long-Term Memory","text":"<p>For each agent, a dedicated table is created upon onboarding to store all long-term chat history. When checkpoints are deleted, the corresponding long-term chat history is also removed to maintain consistency.</p> <p>Deletion Policy</p> <ul> <li><code>Long-term chat history older than 30 days</code> is deleted from the database.</li> <li>Deleted records are first moved to the <code>Recycle</code> and kept for 15 days, allowing for restoration within this window.</li> </ul>"},{"location":"Time-To-Live/#tools-agents-and-mcp-servers","title":"Tools, Agents, and MCP Servers","text":"<p>TTL is also applied to tools, agents, and MCP servers:</p> <ul> <li>When a tool, agent, or MCP server is deleted, it is moved to the <code>Recycle Bin</code>.</li> <li>After 45 days in the Recycle Bin, the tool, agent, or MCP server is permanently deleted, including the removal of the agent's table from the database.</li> </ul> <p>Recycle Manager</p> <p>The Recycle Manager acts as a holding area for deleted records (checkpoints, long-term records, tools, agents, and MCP servers). All items remain in the recycle area for <code>15 days</code> before permanent deletion. Restoration is possible during this period based on the number of days since deletion.</p>"},{"location":"Time-To-Live/#automated-cleanup-with-scheduler","title":"Automated Cleanup with Scheduler","text":"<p>A daily task scheduler (cron job) is implemented to automate the cleanup process:</p> <ul> <li>The cleanup script runs daily at 3 AM, ensuring timely deletion and recycling of records according to the TTL policies.</li> </ul> <p>Summary of TTL Policies:</p> <ol> <li>Short-term memory (checkpoints): Deleted after 30 days, retained in recycle for 15 days.</li> <li>Long-term memory (chat history): Deleted after 30 days, retained in recycle for 15 days.</li> <li>Tools, agents &amp; MCP Servers: Moved to recycle bin on deletion, permanently deleted after 15 days.</li> <li>Automated cleanup: Scheduled daily at 3 AM.</li> </ol> <p>This TTL implementation ensures efficient data management, reduces storage overhead, and provides a safety net for accidental deletions through the recycle manager.</p>"},{"location":"Tool%20Interrupt/","title":"Tool Interrupt","text":"<p>The Tool Interrupt feature provides users with enhanced control over how tools are executed within the framework. This feature allows users to review, modify, and approve tool executions before they are processed, giving greater transparency and control over the agent's behavior.</p>"},{"location":"Tool%20Interrupt/#overview","title":"Overview","text":"<p>Tool Interrupt is a toggle feature available in the inference section of the framework that changes how tools are called and executed when processing user queries.</p> <p>Toggle Location :</p> <p>The Tool Interrupt toggle button can be found in the Inference section of the framework interface.</p>"},{"location":"Tool%20Interrupt/#operating-modes","title":"Operating Modes","text":""},{"location":"Tool%20Interrupt/#without-tool-interrupt-default-mode","title":"Without Tool Interrupt (Default Mode)","text":"<p>When Tool Interrupt is disabled, the system operates in automatic mode:</p> <ul> <li>User submits a query</li> <li>The agent automatically identifies required tools</li> <li>Tools are called sequentially with parameters derived from the user query</li> <li>Final answer is provided directly without user intervention</li> </ul> <p>Example: - User Query: <code>2-10*5</code> - System Response: <code>The answer is -48</code></p>"},{"location":"Tool%20Interrupt/#with-tool-interrupt-interactive-mode","title":"With Tool Interrupt (Interactive Mode)","text":"<p>When Tool Interrupt is enabled, the system operates in interactive mode :</p> <ul> <li>User submits a query</li> <li>System displays the first tool to be called with its parameters</li> <li>User can review and modify parameters before execution</li> <li>User approves each tool execution step-by-step</li> <li>System shows dependent tools in sequence</li> <li>Final answer is provided after all approvals</li> </ul>"},{"location":"Tool%20Interrupt/#tool-interrupt-workflow","title":"Tool Interrupt Workflow","text":"<p>Step 1: Tool Identification</p> <p>When a user submits a query, the system identifies the first tool that needs to be called and displays:</p> <ul> <li>Tool Name: The specific tool to be executed</li> <li>Parameters: The arguments that will be passed to the tool</li> </ul> <p>Step 2: Parameter Review and Editing Users have two options:</p> <p>Option A: Edit Parameters</p> <ul> <li>Click the \"Edit\" button</li> <li>Modify the tool arguments as needed</li> <li>Confirm changes</li> </ul> <p>Option B: Approve Parameters</p> <ul> <li>Click the \"\ud83d\udc4d\" (thumbs up) button to approve current parameters</li> <li>System proceeds with the tool execution</li> </ul> <p>Step 3: Sequential Tool Execution</p> <ul> <li>After approval, the tool executes with the specified parameters</li> <li>If additional tools are required, the system displays the next tool in the sequence</li> <li>Process repeats until all dependent tools are executed</li> </ul> <p>Step 4: Final Result</p> <ul> <li>Once all tools have been executed, the system provides the final answer</li> </ul> <p>Practical Example: Calculator Agent</p> <p>Let's walk through a detailed example using a Calculator Agent with the following tools:</p> <ul> <li><code>add(a, b)</code> - Addition</li> <li><code>sub(a, b)</code> - Subtraction  </li> <li><code>mult(a, b)</code> - Multiplication</li> <li><code>div(a, b)</code> - Division</li> </ul> <p>User Query: <code>2-10*5</code></p> <p>Without Tool Interrupt: <pre><code>Input: 2-10*5\nOutput: The answer is -48\n</code></pre></p> <p>With Tool Interrupt:</p> <p>Step 1: System identifies first operation <pre><code>Tool to call: mult(a, b)\nParameters: a=10, b=5\n</code></pre></p> <p>User Options:</p> <ul> <li>Edit: Modify parameters (e.g., change to a=5, b=2)</li> </ul> <p>Click on the Edit option as shown below to modify the parameters:</p> <p></p> <p>The image below shows where you can edit the parameters:</p> <p></p> <ul> <li>\ud83d\udc4d: Approve current parameters.</li> </ul> <p></p> <p>Step 2: If user approves original parameters <pre><code>Tool executed: mult(10, 5) = 50\nNext tool: sub(a, b)\nParameters: a=2, b=50\n</code></pre></p> <p>Step 3: User approves subtraction <pre><code>Tool executed: sub(2, 50) = -48\nFinal Answer: -48\n</code></pre></p> <p>Alternative Step 2: If user edits multiplication parameters <pre><code>Modified parameters: mult(5, 2) = 10\nNext tool: sub(a, b)\nParameters: a=2, b=10\n</code></pre></p> <p>Final Result with edited parameters: <pre><code>Tool executed: sub(2, 10) = -8\nFinal Answer: -8\n</code></pre></p>"},{"location":"Tool%20Interrupt/#benefits-of-tool-interrupt","title":"Benefits of Tool Interrupt","text":"<p>Enhanced Control</p> <ul> <li>Users can intervene in the tool execution process</li> <li>Ability to correct parameters before execution</li> <li>Prevention of unintended tool calls</li> </ul> <p>Transparency</p> <ul> <li>Clear visibility into which tools are being called</li> <li>Understanding of parameter values being used</li> <li>Step-by-step execution visibility</li> </ul> <p>Debugging and Testing - Ability to test different parameter combinations - Easy identification of tool execution issues - Validation of tool selection logic</p> <p>Educational Value - Understanding of how complex queries are broken down - Learning about tool dependencies and execution order - Insight into agent decision-making process</p>"},{"location":"Tool%20Interrupt/#best-practices","title":"Best Practices","text":"<p>Enable Tool Interrupt when: - Testing new agent configurations - Working with sensitive data or operations - Learning how tools interact with each other - Debugging complex queries - Need precise control over tool parameters</p> <p>Disable Tool Interrupt when: - Running routine, well-tested operations - Processing bulk queries - Working with trusted tool configurations - Need fast, automated responses</p> <p>Parameter Editing Guidelines :</p> <ul> <li>Review Carefully: Always review the suggested parameters before editing</li> <li>Understand Dependencies: Consider how parameter changes might affect subsequent tools</li> <li>Test Incrementally: Make small changes and observe results</li> <li>Document Changes: Keep track of parameter modifications for future reference</li> </ul>"},{"location":"Tool_Interrupt/","title":"Tool Interrupt","text":"<p>The Tool Interrupt feature provides users with enhanced control over how tools are executed within the framework. This feature allows users to review, modify, and approve tool executions before they are processed, giving greater transparency and control over the agent's behavior.</p>"},{"location":"Tool_Interrupt/#overview","title":"Overview","text":"<p>Tool Interrupt is a toggle feature available in the inference section of the framework that changes how tools are called and executed when processing user queries.</p> <p>Toggle Location :</p> <p>The Tool Interrupt toggle button can be found in the Inference section of the framework interface.</p>"},{"location":"Tool_Interrupt/#operating-modes","title":"Operating Modes","text":"<p>1. Without Tool Interrupt (Default Mode)</p> <p>When Tool Interrupt is disabled, the system operates in automatic mode:</p> <ul> <li>User submits a query</li> <li>The agent automatically identifies required tools</li> <li>Tools are called sequentially with parameters derived from the user query</li> <li>Final answer is provided directly without user intervention</li> </ul> <p>Example:</p> <ul> <li>User Query: <code>2-10*5</code></li> <li>System Response: <code>The answer is -48</code></li> </ul> <p>2. With Tool Interrupt (Interactive Mode)</p> <p>When Tool Interrupt is enabled, the system operates in interactive mode :</p> <ul> <li>User submits a query</li> <li>System displays the first tool to be called with its parameters</li> <li>User can review and modify parameters before execution</li> <li>User approves each tool execution step-by-step</li> <li>System shows dependent tools in sequence</li> <li>Final answer is provided after all approvals</li> </ul>"},{"location":"Tool_Interrupt/#tool-interrupt-workflow","title":"Tool Interrupt Workflow","text":"<p>Step 1: Tool Identification</p> <p>When a user submits a query, the system identifies the first tool that needs to be called and displays:</p> <ul> <li>Tool Name: The specific tool to be executed</li> <li>Parameters: The arguments that will be passed to the tool</li> </ul> <p>Step 2: Parameter Review and Editing Users have two options:</p> <p>Option A: Edit Parameters</p> <ul> <li>Click the \"Edit\" button</li> <li>Modify the tool arguments as needed</li> <li>Confirm changes</li> </ul> <p>Option B: Approve Parameters</p> <ul> <li>Click the \"\ud83d\udc4d\" (thumbs up) button to approve current parameters</li> <li>System proceeds with the tool execution</li> </ul> <p>Step 3: Sequential Tool Execution</p> <ul> <li>After approval, the tool executes with the specified parameters</li> <li>If additional tools are required, the system displays the next tool in the sequence</li> <li>Process repeats until all dependent tools are executed</li> </ul> <p>Step 4: Final Result</p> <ul> <li>Once all tools have been executed, the system provides the final answer</li> </ul> <p>Practical Example: Calculator Agent</p> <p>Let's walk through a detailed example using a Calculator Agent with the following tools:</p> <ul> <li><code>add(a, b)</code> - Addition</li> <li><code>sub(a, b)</code> - Subtraction  </li> <li><code>mult(a, b)</code> - Multiplication</li> <li><code>div(a, b)</code> - Division</li> </ul> <p>User Query: <code>2-10*5</code></p> <p>Without Tool Interrupt: <pre><code>Input: 2-10*5\nOutput: The answer is -48\n</code></pre></p> <p>With Tool Interrupt:</p> <p>Step 1: System identifies first operation <pre><code>Tool to call: mult(a, b)\nParameters: a=10, b=5\n</code></pre> User Options:</p> <ul> <li>Edit: Click to modify the tool parameters before execution</li> <li>\ud83d\udc4d Approve: Click to proceed with the current parameters as-is</li> </ul> <p>Users can either edit the parameters to customize the tool execution or approve them to continue with the suggested values.</p> <p>Step 2: If user approves original parameters <pre><code>Tool executed: mult(10, 5) = 50\nNext tool: sub(a, b)\nParameters: a=2, b=50\n</code></pre></p> <p>Step 3: User approves subtraction <pre><code>Tool executed: sub(2, 50) = -48\nFinal Answer: -48\n</code></pre></p> <p>Alternative Step 2: If user edits multiplication parameters <pre><code>Modified parameters: mult(5, 2) = 10\nNext tool: sub(a, b)\nParameters: a=2, b=10\n</code></pre></p> <p>Final Result with edited parameters: <pre><code>Tool executed: sub(2, 10) = -8\nFinal Answer: -8\n</code></pre></p>"},{"location":"Tool_Interrupt/#benefits-of-tool-interrupt","title":"Benefits of Tool Interrupt","text":"<p>Enhanced Control</p> <ul> <li>Users can intervene in the tool execution process</li> <li>Ability to correct parameters before execution</li> <li>Prevention of unintended tool calls</li> </ul> <p>Transparency</p> <ul> <li>Clear visibility into which tools are being called</li> <li>Understanding of parameter values being used</li> <li>Step-by-step execution visibility</li> </ul> <p>Debugging and Testing - Ability to test different parameter combinations - Easy identification of tool execution issues - Validation of tool selection logic</p> <p>Educational Value - Understanding of how complex queries are broken down - Learning about tool dependencies and execution order - Insight into agent decision-making process</p>"},{"location":"Tool_Interrupt/#selective-tool-interrupt","title":"Selective Tool Interrupt","text":"<p>Granular Control</p> <ul> <li>Enabled selective tool interruption in LangGraph, allowing users to choose which specific tools require approval before execution.</li> <li>Expanded support to include MCP tools and agents, providing comprehensive coverage across all tool types in the framework.</li> <li>Users can configure interrupt behavior on a per-tool basis, enabling automatic execution for trusted tools while maintaining manual approval for sensitive operations.</li> </ul>"},{"location":"Tool_Interrupt/#agent-interrupt-for-meta-templates","title":"Agent Interrupt (For Meta Templates)","text":"<p>For <code>Meta Agent</code> and <code>Meta Planner Agent</code> templates, the concept of Tool Interrupt is replaced by <code>Agent Interrupt</code>. Since Meta templates orchestrate multiple worker agents rather than calling tools directly, Agent Interrupt provides control over which worker agents are invoked during query processing.</p> <p>Overview</p> <p>Agent Interrupt allows users to review and approve agent invocations before they are executed. This ensures transparency and control over how the Meta Agent delegates tasks to its worker agents.</p> <p>How It Works</p> <p>When Agent Interrupt is <code>enabled</code>:</p> <ul> <li>User submits a query to the Meta Agent</li> <li>The system displays which worker agent will be called</li> <li>User can approve or modify the agent selection before execution</li> <li>Process continues for each agent invocation in the workflow</li> </ul> <p>Selective Agent Interrupt</p> <p>Similar to Selective Tool Interrupt, Selective Agent Interrupt provides granular control over which worker agents require approval before invocation.</p> <p>Granular Control</p> <ul> <li>Users can select specific worker agents that should trigger an interrupt before being called.</li> <li>Trusted agents can be configured for automatic execution, while sensitive or critical agents require manual approval.</li> <li>This allows for a balance between automation and oversight in complex multi-agent workflows.</li> </ul>"},{"location":"Tool_Interrupt/#best-practices","title":"Best Practices","text":"<p>Enable Tool Interrupt when:</p> <ul> <li>Testing new agent configurations</li> <li>Working with sensitive data or operations</li> <li>Learning how tools interact with each other</li> <li>Debugging complex queries</li> <li>Need precise control over tool parameters</li> </ul> <p>Disable Tool Interrupt when:</p> <ul> <li>Running routine, well-tested operations</li> <li>Processing bulk queries</li> <li>Working with trusted tool configurations</li> <li>Need fast, automated responses</li> </ul> <p>Parameter Editing Guidelines :</p> <ul> <li>Review Carefully: Always review the suggested parameters before editing</li> <li>Understand Dependencies: Consider how parameter changes might affect subsequent tools</li> <li>Test Incrementally: Make small changes and observe results</li> <li>Document Changes: Keep track of parameter modifications for future reference</li> </ul>"},{"location":"Tool_Validation/","title":"Tool Validation Rules","text":""},{"location":"Tool_Validation/#overview","title":"Overview","text":"<p>This document outlines the comprehensive validation process for onboarding new tools into our system. Before any tool is added to our platform, it must pass through a rigorous validation checklist to ensure quality, security, and reliability. This process helps maintain high standards across all tools and protects both the system and its users from potential issues.</p>"},{"location":"Tool_Validation/#validation-process","title":"Validation Process","text":"<p>The validation process follows a structured approach to ensure thorough evaluation:</p> <ol> <li>Code Analysis: Tool code is systematically analyzed against our established validation criteria using both automated and manual review processes</li> <li>Result Classification: Each validation item is carefully evaluated and marked as Pass, Warning, or Error based on severity and impact</li> <li>User Decision: If issues are found during validation, the user is presented with detailed findings and must decide whether to proceed with deployment</li> <li>Tool Addition: Based on the validation results and user consent, the tool is either approved for addition to the platform or rejected for further development</li> </ol>"},{"location":"Tool_Validation/#validation-checklist","title":"Validation Checklist","text":"<p>All tools must pass these comprehensive validation rules before onboarding. Each rule is classified by severity level to help prioritize remediation efforts:</p> <p>1. Testing :</p> <ul> <li>This is an Error level validation</li> <li>Must include comprehensive test cases to validate the tool's functionality</li> <li>Tests should cover edge cases, error conditions, and expected behaviors</li> <li>Unit tests, integration tests, and end-to-end tests are recommended where applicable</li> <li>Test coverage should be sufficient to ensure reliability in production environments</li> </ul> <p>2. Tool Name :</p> <ul> <li>This is a Warning level validation</li> <li>Tool names must be descriptive, clear, and unambiguous to avoid confusion</li> <li>Names should accurately reflect the tool's purpose and functionality</li> <li>Avoid overloaded names that could conflict with existing tools or create ambiguity</li> <li>Consider using consistent naming conventions across related tools</li> <li>Names should be professional and appropriate for business environments</li> </ul> <p>3. Arguments :</p> <ul> <li>This is an Error level validation</li> <li>All function inputs must be explicitly named and properly typed for clarity and safety</li> <li>Use JSON-serializable types exclusively (str, int, float, bool, list, dict) to ensure compatibility</li> <li>Provide clear parameter descriptions and examples where helpful</li> <li>Validate input parameters and provide meaningful error messages for invalid inputs</li> <li>Consider using type hints and documentation to improve code maintainability</li> </ul> <p>4. Fail-Safes :</p> <ul> <li>This is a Warning level validation</li> <li>Include clear, actionable error messages that help users understand and resolve issues</li> <li>Implement robust fallback mechanisms for handling invalid inputs or unexpected conditions</li> <li>Enable meaningful error handling that gracefully degrades functionality when possible</li> <li>Provide logging and debugging information to assist with troubleshooting</li> <li>Consider timeout mechanisms for operations that might hang indefinitely</li> </ul> <p>5. Side Effects :</p> <ul> <li>This is an Error level validation</li> <li>Avoid tools with dangerous, destructive, or irreversible side effects unless absolutely necessary for functionality</li> <li>Tools that delete data, modify system configurations, or send irreversible commands require explicit justification and approval</li> <li>Implement confirmation mechanisms for potentially destructive operations</li> <li>Document all side effects clearly in the tool's documentation</li> <li>Consider implementing dry-run or preview modes where applicable</li> </ul> <p>6. Credential Handling :</p> <ul> <li>This is a Warning level validation</li> <li>Use secure environment variables or dedicated secret management systems for sensitive information</li> <li>Never hardcode API keys, passwords, or other credentials directly in the source code</li> <li>Implement proper credential rotation and expiration handling where applicable</li> <li>Follow principle of least privilege when accessing external services</li> <li>Ensure credentials are not logged or exposed in error messages</li> </ul> <p>7. Import Statements:</p> <ul> <li>This is an Error level validation</li> <li>Tools must include explicit and proper import statements for all dependencies</li> <li>All imported modules must be clearly documented and justified</li> <li>Avoid importing unnecessary or potentially dangerous modules</li> <li>Use specific imports rather than wildcard imports where possible</li> <li>Ensure all dependencies are properly versioned and documented</li> </ul> <p>8. Output and Input Handling:</p> <ul> <li>This is an Error level validation</li> <li>Avoid using <code>print</code> statements within the tool as they can interfere with system operations</li> <li>Tools must not directly prompt for or accept user inputs within their execution code</li> <li>Use proper logging mechanisms instead of print statements for debugging information</li> <li>Return structured data that can be properly processed by the calling system</li> <li>Handle output formatting consistently across all tools</li> </ul>"},{"location":"Tool_Validation/#rule-classifications","title":"Rule Classifications","text":"<p>Our validation system uses three distinct severity levels to categorize issues:</p> <ul> <li>Error: Critical issues that strongly indicate revision is needed before deployment. These represent potential security vulnerabilities, functional failures, or system incompatibilities that could cause significant problems</li> <li>Warning: Issues that should be addressed to improve quality and maintainability but don't necessarily block deployment. These represent best practices violations or potential future problems</li> <li>Pass: Requirements are fully met and the tool is ready for deployment without concerns in this area</li> </ul>"},{"location":"Vault/","title":"Vault Management","text":"<p>The Vault is a secure storage system designed to manage sensitive information such as API keys, URLs, and other credentials that are required by tools within the platform. Instead of hardcoding sensitive values directly into tools, the Vault provides a secure way to store and retrieve these values using secret names.</p>"},{"location":"Vault/#overview","title":"Overview","text":"<p>The Vault serves as a centralized repository for managing secrets, providing two distinct storage options:</p> <ul> <li>Private Vault: Personal storage accessible only to the individual user</li> <li>Public Vault: Shared storage accessible across the organization</li> </ul>"},{"location":"Vault/#key-features","title":"Key Features","text":"<ul> <li>Secure Storage: Safely store API keys, URLs, and other sensitive data</li> <li>Masked Display: Values are displayed with masking for security</li> <li>Easy Retrieval: Access stored values using simple function calls</li> <li>Access Control: Separate private and public storage with appropriate permissions</li> </ul>"},{"location":"Vault/#vault-sections","title":"Vault Sections","text":""},{"location":"Vault/#private-vault","title":"Private Vault","text":"<p>The Private Vault is designed for personal, sensitive information that should only be accessible to the individual user account.</p> <p>Characteristics:</p> <ul> <li>Personal storage space</li> <li>Only the owner can view and access stored values</li> <li>Ideal for personal API keys, private URLs, and user-specific credentials</li> <li>Enhanced security through user-level isolation</li> </ul> <p>Use Cases:</p> <ul> <li>Personal API keys (e.g., OpenAI API key, personal weather service key)</li> <li>Private database connection strings</li> <li>User-specific authentication tokens</li> <li>Personal service URLs</li> </ul>"},{"location":"Vault/#public-vault","title":"Public Vault","text":"<p>The Public Vault is designed for shared information that can be accessed by all users within the organization.</p> <p>Characteristics:</p> <ul> <li>Organization-wide accessible storage</li> <li>All users can view and access stored values</li> <li>Suitable for common endpoints, shared API keys, and public resources</li> <li>Facilitates collaboration and standardization</li> </ul> <p>Use Cases:</p> <ul> <li>Shared API endpoints</li> <li>Common service URLs</li> <li>Organization-wide API keys</li> <li>Standard configuration values</li> </ul>"},{"location":"Vault/#creating-vault-entries","title":"Creating Vault Entries","text":"<p>Adding a New Secret</p> <ol> <li>Select Vault Type: Choose between Private or Public vault</li> <li>Enter Name: Provide a descriptive name for your secret (e.g., <code>weather_api_key</code>, <code>database_url</code>)</li> <li>Enter Value: Input the actual value (API key, URL, etc.)</li> <li>Save: Store the secret in the selected vault</li> </ol> <p>Example:</p> <pre><code>Name: weather_api_key\nValue: sk-1234567890abcdef\nType: Private\n</code></pre>"},{"location":"Vault/#in-tool-development","title":"In Tool Development","text":"<p>When developing tools that require sensitive information, use the appropriate retrieval functions instead of hardcoding values.</p> <p>Private Vault Retrieval</p> <p>Use <code>get_user_secrets()</code> to retrieve values from the private vault:</p> <pre><code># Syntax: get_user_secrets('secret_name', 'default_value')\napi_key = get_user_secrets('weather_api_key', 'no_api_key_found')\ndatabase_url = get_user_secrets('personal_db_url', 'localhost:5432')\nauth_token = get_user_secrets('personal_auth_token', 'default_token')\n</code></pre> <p>Public Vault Retrieval</p> <p>Use <code>get_public_secrets()</code> to retrieve values from the public vault:</p> <pre><code># Syntax: get_public_secrets('secret_name', 'default_value')\nbase_url = get_public_secrets('weather_api_base_url', 'https://default-weather-api.com')\nshared_endpoint = get_public_secrets('common_endpoint', 'https://api.example.com')\norg_api_key = get_public_secrets('organization_api_key', 'default_key')\n</code></pre>"},{"location":"Vault/#practical-examples","title":"Practical Examples","text":"<p>Weather Tool Implementation</p> <pre><code>def get_weather_data(city):\n    # Retrieve API key from private vault\n    api_key = get_user_secrets('weather_api_key', 'no_api_key_found')\n\n    # Retrieve base URL from public vault\n    base_url = get_public_secrets('weather_api_base_url', 'https://api.openweathermap.org')\n\n    # Use the retrieved values\n    endpoint = f\"{base_url}/data/2.5/weather\"\n    params = {\n        'q': city,\n        'appid': api_key,\n        'units': 'metric'\n    }\n\n    response = requests.get(endpoint, params=params)\n    return response.json()\n</code></pre> <p>Database Connection Tool</p> <pre><code>def connect_to_database():\n    # Private database credentials\n    db_username = get_user_secrets('db_username', 'default_user')\n    db_password = get_user_secrets('db_password', 'default_pass')\n\n    # Shared database host from public vault\n    db_host = get_public_secrets('shared_db_host', 'localhost')\n    db_port = get_public_secrets('shared_db_port', '5432')\n\n    connection_string = f\"postgresql://{db_username}:{db_password}@{db_host}:{db_port}/mydb\"\n    return connection_string\n</code></pre>"},{"location":"Vault/#security-features","title":"Security Features","text":"<p>Masked Display</p> <p>For security purposes, stored values are displayed with masking in the user interface:</p> <pre><code>Name: weather_api_key\nValue: sk-123***********def\nType: Private\n</code></pre> <p>Access Control</p> <ul> <li>Private Vault: Only the owner can access their private secrets</li> <li>Public Vault: All organization members can access public secrets</li> <li>No Cross-Access: Users cannot access other users' private secrets</li> </ul>"},{"location":"Vault/#using-tools-created-by-other-users","title":"Using Tools Created by Other Users","text":"<p>When you want to use a tool that was created by another user, you need to understand how the Vault system works in this scenario.</p> <p>Key Points</p> <ul> <li>Each user has their own private vault that others cannot access</li> <li>If a tool uses private vault keys, you must create your own keys with the same names</li> <li>The tool will work for you once you provide your own values for the required keys</li> </ul> <p>Process</p> <ol> <li>Identify Required Keys: Check the tool documentation or code to see what vault keys it uses</li> <li>Create Your Own Keys: Add the same key names to your vault with your own values</li> <li>Use the Tool: The tool will now work with your provided values</li> </ol> <p>Example Scenario</p> <p>If another user created a weather tool that uses:</p> <pre><code>api_key = get_user_secrets('weather_api_key', 'no_api_key_found')\nbase_url = get_public_secrets('weather_service_url', 'https://default-api.com')\n</code></pre> <p>For you to use this tool:</p> <p>Check the tool requirements: The tool needs:</p> <ul> <li>Private key: <code>weather_api_key</code></li> <li>Public key: <code>weather_service_url</code></li> </ul> <p>Create your own vault entries:</p> <ul> <li>Add <code>weather_api_key</code> to your private vault with your own API key</li> <li>If <code>weather_service_url</code> doesn't exist in public vault, request admin to add it</li> </ul> <p>Tool usage: The tool will now use your API key and work for your account</p> <p>Important Notes</p> <p>Private Keys</p> <p>You must create your own private keys with the exact same names</p> <p>Public Keys</p> <p>These are shared across the organization, so they should already exist</p> <p>Key Names Must Match</p> <p>The key names in your vault must exactly match what the tool expects</p> <p>Your Own Values</p> <p>Use your own API keys, credentials, and URLs - never share private credentials</p>"},{"location":"Vault/#step-by-step-guide-using-someone-elses-tool","title":"Step-by-Step Guide: Using Someone Else's Tool","text":"<p>Step 1: Tool Analysis</p> <pre><code># Example: Someone shared a translation tool\ndef translate_text(text, target_language):\n    api_key = get_user_secrets('translation_api_key', 'no_key')\n    endpoint = get_public_secrets('translation_endpoint', 'default_url')\n    # ... rest of the tool code\n</code></pre> <p>Step 2: Identify Requirements</p> <p>From the code above, you need:</p> <ul> <li>Private: <code>translation_api_key</code> (your personal API key)</li> <li>Public: <code>translation_endpoint</code> (shared endpoint URL)</li> </ul> <p>Step 3: Set Up Your Vault</p> <ol> <li>Go to your Private Vault</li> <li>Add new entry:</li> <li>Name: <code>translation_api_key</code></li> <li>Value: <code>your-actual-translation-api-key-here</code></li> <li>Check if <code>translation_endpoint</code> exists in Public Vault</li> <li>If missing, contact admin to add it</li> </ol> <p>Step 4: Test the Tool</p> <p>Run the tool to verify it works with your credentials.</p>"},{"location":"Vault/#tool-documentation-best-practices","title":"Tool Documentation Best Practices","text":"<p>For Tool Creators</p> <p>Always document the required vault keys in your tool description:</p> <pre><code>## Required Vault Keys\n\n### Private Keys\n- `weather_api_key`: Your OpenWeatherMap API key\n- `personal_db_password`: Your database password\n\n### Public Keys\n- `weather_base_url`: Weather service endpoint (admin managed)\n- `shared_db_host`: Database host address (admin managed)\n</code></pre> <p>For Tool Users</p> <p>Before using any tool, check the documentation for required vault keys and ensure you have all necessary credentials.</p>"},{"location":"Vault/#common-scenarios","title":"Common Scenarios","text":"<p>Scenario 1: Using a Shared Weather Tool</p> <p>Tool Requirements:</p> <ul> <li>Private: <code>openweather_api_key</code></li> <li>Public: <code>weather_service_endpoint</code></li> </ul> <p>Your Setup:</p> <ol> <li>Obtain your own OpenWeatherMap API key</li> <li>Add to Private Vault: <code>openweather_api_key</code> = <code>your-api-key</code></li> <li>Verify Public Vault has: <code>weather_service_endpoint</code></li> <li>Use the tool with your credentials</li> </ol> <p>Scenario 2: Database Analysis Tool</p> <p>Tool Requirements:</p> <ul> <li>Private: <code>db_username</code>, <code>db_password</code></li> <li>Public: <code>analytics_db_host</code>, <code>analytics_db_port</code></li> </ul> <p>Your Setup:</p> <ol> <li>Get database credentials from your admin</li> <li>Add to Private Vault:</li> <li><code>db_username</code> = <code>your-db-username</code></li> <li><code>db_password</code> = <code>your-db-password</code></li> <li>Check Public Vault for connection details</li> <li>Tool connects using your credentials to shared database</li> </ol> <p>Scenario 3: AI Service Integration</p> <p>Tool Requirements:</p> <ul> <li>Private: <code>openai_api_key</code>, <code>anthropic_api_key</code></li> <li>Public: <code>ai_service_baseurl</code></li> </ul> <p>Your Setup:</p> <ol> <li>Get API keys from respective AI service providers</li> <li>Add to Private Vault with exact key names</li> <li>Tool uses your API keys with shared endpoint configuration</li> </ol>"},{"location":"core_building_blocks/","title":"Core Building Blocks","text":""},{"location":"core_building_blocks/#enterprise-ai-agent-building-blocks","title":"Enterprise AI Agent Building Blocks","text":"<p>These foundational components are critical for the creation and operation of autonomous agents. Each block encapsulates a key function, from task orchestration and memory management to evaluation, safety measures, and scalability. Together, they form a robust system that ensures agents function reliably, securely, and efficiently in dynamic environments.</p>"},{"location":"core_building_blocks/#1-agent-orchestration-mcp-registry","title":"1. Agent Orchestration &amp; MCP Registry","text":"<p>The Agent Orchestration &amp; MCP Registry is the foundational layer that manages how agents collaborate, execute tasks, and communicate across platforms. It establishes a centralized system for coordinating multiple agents, ensuring they work together harmoniously, share responsibilities, and follow pre-defined workflows in a flexible and adaptive manner.</p> Multi-Agent Coordination <p>Delegation, collaboration, and planning with team-based RBAC</p> Dynamic Orchestration <p>Intelligent planners (ReAct, CoT, task graphs) for adaptive workflows</p> Platform Integration <p>Runtime coordination through LangGraph, MCP, and workflow engines</p> Natural Language MCP <p>Convert agent functions into servers through conversation</p> <p>Multi-Agent Coordination: </p> <p>This involves intelligent delegation and management of tasks between multiple agents using Role-Based Access Control (RBAC). By defining roles and responsibilities, this module ensures that agents work in collaboration, respecting boundaries and optimizing their individual contributions to larger projects. It allows for complex, coordinated actions like joint problem-solving and information sharing across different agents or groups.</p> <p>Dynamic Orchestration: </p> <p>This block uses advanced planners such as ReAct, Chain-of-Thought (CoT) reasoning, and task graphs to adapt workflows based on real-time needs. These tools help the system dynamically adjust task priorities, reassign tasks based on resource availability, and optimize time-sensitive operations.</p> <p>Platform Integration: </p> <p>Integration with LangGraph, MCP, and other workflow engines ensures smooth communication across systems, allowing agents to connect with diverse platforms for task execution, information retrieval, or service integration. This ensures that agents can leverage external systems and resources while staying within the defined workflow.</p> <p>Natural Language MCP: </p> <p>This component allows agents to interact with each other and users using natural language interfaces (NLIs). This feature transforms complex tasks and commands into a more intuitive, human-readable form, simplifying agent control and improving accessibility for non-technical users. Users can converse with agents to control them, get insights, or configure operations without needing deep technical expertise.</p>"},{"location":"core_building_blocks/#2-planner-tool-verifier","title":"2. Planner &amp; Tool Verifier","text":"<p>The Planner &amp; Tool Verifier module focuses on evaluating and verifying the feasibility, logic, and execution of agent-generated plans. It ensures that agent decisions are grounded in reality, avoiding contradictions or inefficient actions. It also helps ensure that the tools agents call are used appropriately and effectively.</p> Feasibility Analysis <p>Instant evaluation of every plan step for practical viability</p> Logic Chain Validation <p>Ensures logical connections with no gaps or contradictions</p> Tool Call Verification <p>User verification and argument editing before execution</p> Learning System <p>Learns from human corrections and preferences</p> <p>Feasibility Analysis: </p> <p>Before execution, each plan step is evaluated for feasibility, considering real-world constraints and data availability. The system checks whether the task is achievable within the given resources, time, and environment.</p> <p>Logic Chain Validation: </p> <p>This ensures that plans are logically sound by confirming that each step follows from the last. It helps prevent logical gaps and contradictions, ensuring that no assumptions are made without proper validation. This guarantees that agents execute tasks in a structured and coherent manner.</p> <p>Tool Call Verification: </p> <p>This feature enables agents to verify the tools they intend to use before executing them. This includes user validation of parameters and inputs to ensure that the right tool is being invoked with the correct arguments. If discrepancies are found, agents can prompt the user to modify or confirm the inputs before proceeding.</p> <p>Learning System: </p> <p>Over time, the system learns from human interactions and feedback, adapting its decision-making processes. By learning from corrections and preferences, the planner can refine its judgment, becoming more efficient and accurate in future interactions. This feature ensures continuous improvement in how the agent handles new tasks and situations.</p>"},{"location":"core_building_blocks/#3-knowledge-memory-management","title":"3. Knowledge &amp; Memory Management","text":"<p>Knowledge &amp; Memory Management ensures agents retain contextual information and use it to make informed decisions. This module is critical for ensuring that agents don\u2019t operate in isolation from previous interactions, creating a coherent and continuous understanding of tasks over time.</p> Enterprise Database <p>Reliable, scalable memory persistence with high performance</p> Context-Aware Actions <p>Every decision considers full historical context</p> <p>Enterprise Database: </p> <p>The memory system uses a reliable, high-performance enterprise-grade database to store knowledge, including past actions, decisions, interactions, and outcomes. This provides scalability, allowing agents to manage vast amounts of data while maintaining quick access to relevant information.</p> <p>Context-Aware Actions: </p> <p>Every decision made by the agent is informed by historical context. This ensures that agents take into account past events, preferences, or mistakes when making decisions. For example, if a task was performed incorrectly in the past, the agent can take corrective actions or suggest different approaches based on previous failures or successes.</p>"},{"location":"core_building_blocks/#4-agent-evaluation","title":"4. Agent Evaluation","text":"<p>The Agent Evaluation module helps monitor and assess the performance of agents in real-time, ensuring they are working optimally. This system evaluates not only the results of tasks but also the efficiency of the processes used to achieve them.</p> LLM-as-a-Judge <p>Comprehensive performance assessment across agents and models</p> Tool Utilization Metrics <p>Selection accuracy, usage efficiency, precision, success rate</p> Agent Efficiency Score <p>Task decomposition, reasoning quality, robustness metrics</p> Interactive Dashboard <p>Real-time visualization with advanced filtering capabilities</p> <p>LLM-as-a-Judge: </p> <p>Leveraging advanced Large Language Models (LLMs), this module evaluates agent performance comprehensively. It provides detailed assessments of how well agents perform their tasks, examining reasoning quality, the accuracy of output, and alignment with objectives.</p> <p>Tool Utilization Metrics: </p> <p>By analyzing key performance indicators (KPIs) such as tool selection accuracy, usage efficiency, and overall success rates, this block helps identify the most effective tools for specific tasks and pinpoints areas of inefficiency. It ensures that agents are always using the right tools for the job.</p> <p>Agent Efficiency Score: </p> <p>This score assesses the overall efficiency of an agent. It considers factors like task decomposition (how well the agent breaks down complex tasks), reasoning quality (how logically sound and coherent its thought processes are), and robustness (how effectively the agent can handle disruptions or unexpected conditions).</p> <p>Interactive Dashboard: </p> <p>A real-time, interactive dashboard provides insights into agent performance. With advanced filtering and visualization capabilities, users can track agent performance, identify trends, and act upon real-time data to optimize system operations.</p>"},{"location":"core_building_blocks/#5-agent-telemetry","title":"5. Agent Telemetry","text":"<p>Enables real-time observability into agent behavior and actions. It integrates telemetry frameworks for logging, monitoring, tracing, and generating governance-ready logs.</p> OpenTelemetry Integration <p>Framework-level logging with Elasticsearch &amp; Grafana</p> Arize-Phoenix Tracing <p>Detailed agent-level behavior insights and analysis</p> Real-time Monitoring <p>Live tracking of agent actions and performance</p> Audit-Ready Logs <p>Compliance-ready logging for governance requirements</p> <p>OpenTelemetry Integration: </p> <p>Leveraging frameworks like OpenTelemetry, this system integrates seamlessly with logging tools such as Elasticsearch and Grafana. It enables detailed logging and monitoring of agent behavior, actions, and system performance, ensuring that every step is recorded and traceable.</p> <p>Arize-Phoenix Tracing: </p> <p>This enables deep visibility into agent-level behavior and decision-making processes. By tracing how agents arrive at conclusions or take actions, users can analyze decision pathways and improve process transparency.</p> <p>Real-time Monitoring: </p> <p>Provides live tracking of agent actions and performance, allowing for proactive intervention when necessary. Users can monitor agent behavior in real-time, ensuring that any issues or inefficiencies are quickly addressed.</p> <p>Audit-Ready Logs: </p> <p>To comply with regulatory requirements, this module generates logs that are formatted for easy auditing. It ensures the system is fully compliant with governance and legal standards, providing an accurate record of all agent activities.</p>"},{"location":"core_building_blocks/#6-rai-guardrails","title":"6. RAI Guardrails","text":"<p>Protects agent systems from unsafe, biased, or inaccurate behaviors using automated red teaming, PII protection, hallucination detection, and fairness strategies.</p> Automated Red Teaming <p>Continuous vulnerability scanning and security assessment</p> Hallucination Detection <p>Detect and mitigate LLM drift and inaccuracies</p> PII Protection <p>Analyze, anonymize, and hash personal data in interactions</p> Bias Mitigation <p>Detect and reduce bias in LLMs and ML models</p> <p>Automated Red Teaming: </p> <p>This continuously assesses vulnerabilities in the system by simulating potential attacks or misuse. Automated red-teaming helps identify weaknesses in the agent system\u2019s defenses, improving overall system security.</p> <p>Hallucination Detection: </p> <p>Agents that rely on LLMs are prone to generating \u201challucinations\u201d (incorrect or fabricated information). This system detects and mitigates hallucinations, ensuring that agents only provide valid, fact-based output.</p> <p>PII Protection: </p> <p>This block helps safeguard personal data by anonymizing and hashing sensitive information before it\u2019s used by agents. It ensures compliance with data privacy regulations (e.g., GDPR) and protects against accidental data breaches.</p> <p>Bias Mitigation: </p> <p>This module detects and reduces biases in machine learning models and LLMs. By ensuring that agents do not exhibit bias in decision-making, it promotes fairness and inclusivity, which is especially important in sensitive applications like hiring or legal decisions.</p>"},{"location":"core_building_blocks/#7-optimization-scalability","title":"7. Optimization &amp; Scalability","text":"<p>Ensures your system scales with efficiency. Supports advanced prompt optimization, role-based personas, message-driven workflows, and scalable service orchestration.</p> Prompt Optimizer <p>Real-time refinement with self-improving AI capabilities</p> Role-Based Prompting <p>Automatic persona assumption for optimal task execution</p> Azure Service Bus <p>Reliable message passing with event-driven workflows</p> KEDA + Dapr <p>Auto-scaling and seamless service communication</p> <p>Prompt Optimizer: </p> <p>This tool continuously refines prompts based on agent performance and user feedback. It helps enhance task accuracy and efficiency by making iterative adjustments to the prompts used in tasks, ensuring that agents perform at their highest capability.</p> <p>Role-Based Prompting: </p> <p>This system automatically adjusts the agent\u2019s persona based on the task at hand. By tailoring the agent\u2019s behavior and communication style, it ensures that tasks are executed in the most efficient manner possible, considering the specific context.</p> <p>Azure Service Bus: </p> <p>A cloud-based message-passing infrastructure that ensures reliable communication between services. It facilitates event-driven workflows, ensuring that messages and data are passed efficiently and without bottlenecks.</p> <p>KEDA + Dapr: </p> <p>These technologies provide auto-scaling capabilities to dynamically adjust resources based on real-time load and demand. This ensures that the system can handle sudden spikes in traffic without degradation of performance.</p>"},{"location":"memory_management/","title":"Memory Management","text":""},{"location":"memory_management/#1-semantic-memory","title":"1. Semantic Memory","text":"<p>Semantic memory is a core component of agent intelligence, enabling agents to store, recall, and utilize facts or information provided by users across sessions. This persistent memory allows agents to deliver more personalized, context-aware, and efficient interactions.</p> <p>During agent onboarding, the model generates a system prompt for each agent. This system prompt includes the agent's goal, workflow description, and a dynamically generated list of tools required for the agent's operation. By default, the system prompt now also includes the Manage Memory and Search Memory tools, ensuring that every agent is equipped with semantic memory capabilities from the outset.</p> <p>Purpose</p> <p>Semantic memory is designed for storing facts, preferences, or contextual information shared by the user for future reference and retrieval.</p> <p>By leveraging semantic memory, agents can:</p> <ul> <li>Remember user-provided details (e.g., preferences, important facts)</li> <li>Reference past interactions to provide continuity and relevance</li> <li>Reduce repetitive questioning and improve user experience</li> </ul> <p>Implementation</p> <p>Semantic memory in Agentic Foundry is implemented using two specialized tools:</p> <p>1. Manage Memory Tool</p> <ul> <li>Function: Stores facts or information shared by the user during conversations.</li> <li>Trigger: In chat inference, when a user interacts with the agent and provides a query containing facts or information, the agent automatically calls the Manage Memory Tool and stores this information in long-term memory.</li> <li>Storage:<ul> <li>The implementation uses Redis cache for fast, in-memory storage and a Postgres database for persistent storage.</li> <li>User interactions are initially stored in Redis, which provides low-latency access and efficient indexing for quick retrieval during active sessions.</li> <li>When a defined threshold (such as memory size or time interval) is reached, Redis synchronizes its data with the Postgres database, ensuring long-term persistence and durability.</li> <li>All new entries are first written to Redis; updates and deletions are performed on both Redis and Postgres to maintain consistency across the in-memory and persistent layers.</li> <li>This hybrid approach ensures that frequently accessed or recently updated information is quickly available, while all critical data is reliably stored in Postgres for future reference and compliance.</li> <li>Information is indexed by user ID, session, or semantic tags to support efficient search and retrieval operations.</li> </ul> </li> </ul> <p>Example Use Cases</p> <ul> <li>Remembering a user's preferred tool or configuration</li> <li>Storing important project or workflow details</li> </ul> <p>2. Search Memory Tool</p> <ul> <li>Function: Retrieves previously stored information to answer user queries.</li> <li>Trigger: The agent calls this tool when the user asks questions related to previously stored information, or when relevant context is needed for reasoning.</li> <li>Operation: The tool searches stored memory using semantic similarity, enabling retrieval of relevant facts even if the query is phrased differently from the original input.</li> </ul> <p>Example Use Cases</p> <ul> <li>Answering questions about previously shared preferences or project details</li> <li>Providing reminders or recalling key information from earlier sessions</li> </ul> <p>Semantic Memory in All Agent Templates</p> <p>All six agent templates React, React Critic, Planner Executor Critic, Planner Critic, Meta, and Planner Meta have semantic memory implemented. This ensures that regardless of the agent type, the ability to remember and retrieve user-specific information is always available. Each template is configured to:</p> <ul> <li>Capture and store relevant user data using the Manage Memory Tool</li> <li>Retrieve and utilize stored information via the Search Memory Tool</li> <li>Maintain context and continuity across sessions and conversations</li> </ul> <p>During agent onboarding, these tools are integrated into the system prompt and configuration, ensuring seamless access to semantic memory features for all agent types.</p>"},{"location":"memory_management/#2-episodic-memory","title":"2. Episodic Memory","text":"<p>Episodic memory is a foundational capability implemented across all agent templates. It enables agents to learn from past conversational experiences by storing and utilizing specific query-response examples. This supports few-shot learning, allowing agents to improve future responses based on real user interactions and feedback, and to adapt dynamically to user preferences and expectations.</p> <p>Purpose</p> <p>The primary objective of episodic memory is to enhance agent learning and adaptability by capturing conversational examples\u2014both positive and negative. These examples inform and refine future agent behavior, resulting in more context-aware, user-aligned, and effective responses.</p> <p>Automatic Conversation Analysis</p> <p>Episodic memory also supports automatic extraction and storage of conversational examples without explicit user feedback. This approach is implemented for all agent templates and leverages both short-term memory and LLM-based analysis:</p> <ul> <li>The system maintains a short-term memory buffer, typically containing the latest four conversations.</li> <li>When analyzing for learning opportunities, the system makes an LLM call to process these recent conversations and extract appropriate query-to-final-response pairs.</li> <li>The analysis focuses on identifying explicit or implicit user feedback (positive or negative) and links it to the original, meaningful user query and the final AI response that was evaluated.</li> <li>Only substantive queries are considered; clarifications, format requests, and meta-requests are ignored.</li> <li>Tool usage patterns from successful interactions are preserved for future learning.</li> </ul> <p>Conversation Analysis Criteria</p> <ul> <li>Positive Indicators: Explicit signals (e.g., \"helpful\", \"thank you\", \"good\"), engagement, or resolution.</li> <li>Negative Indicators: Explicit negatives (e.g., \"wrong\", \"not correct\"), rejections, corrections, or signs of dissatisfaction.</li> <li>No Feedback: If the user moves on without evaluative feedback, the pair is ignored.</li> </ul> <p>Similarity Scoring and Example Retrieval</p> <p>To retrieve the most relevant examples from episodic memory, the system employs advanced similarity scoring techniques:</p> <ul> <li>Bi-Encoders: Used to generate embeddings for both stored examples and the incoming user query, enabling efficient retrieval of candidate examples based on vector similarity in the embedding space.</li> <li>Cross-Encoders: Applied to the top candidate examples for more precise, context-aware similarity scoring between the user query and stored examples. This ensures that the most contextually relevant positive and negative examples are selected for few-shot learning.</li> </ul> <p>By leveraging bi-encoders and cross-encoders, the agent can accurately identify and utilize examples that are most similar to the current user query, resulting in more effective and contextually appropriate responses.</p> <p>Learning Application and Continuous Improvement</p> <ul> <li>Positive Example Usage:<ul> <li>The agent follows successful response formats, tool usage, and explanation styles from positive examples.</li> </ul> </li> <li>Negative Example Learning:<ul> <li>The agent avoids unsuccessful approaches, prevents repeated errors, and develops alternative strategies for similar contexts.</li> </ul> </li> </ul> <p>Retrieval and Application Process</p> <ol> <li>The new user query is analyzed for similarity to stored examples.</li> <li>The system finds relevant positive and negative examples using similarity scoring.</li> <li>Retrieved examples are formatted as guidance context for the agent.</li> <li>The agent uses these examples to inform response style, tool selection, and approach.</li> <li>Each new interaction becomes a potential example for future learning, supporting continuous improvement and adaptation.</li> </ol> <p>Manual Feedback System</p> <p>The chat inference interface allows users to provide feedback on each AI response. Users can mark a response as positive or negative, which stores the current query-response pair as a positive or negative example. If the same feedback is clicked again, the stored example is deleted. Switching feedback updates the example type. The system processes these actions through backend API calls and provides visual feedback to indicate the current state. This mechanism helps the agent learn from user preferences and improve future responses.</p> <p>Episodic Memory Chat Inference</p> <p>The following example illustrates how episodic memory operates during chat inference:</p> <ul> <li>The user submits a query and then requests the response in a specific format (e.g., JSON).</li> <li>The system stores the original query and the final response (in JSON) as an episodic example, based on explicit user satisfaction.</li> <li>For subsequent queries, if a similar context is detected, the agent references the stored example:<ul> <li>If the previous example was positive, the agent follows the same response format.</li> <li>If the previous example was negative, the agent avoids generating a similar response.</li> </ul> </li> </ul>"},{"location":"prompt_optimization/","title":"Prompt Optimizer Overview","text":"<p>The Prompt Optimizer is an automated system that improves the instructions, or system prompt, used by your AI agent. It does this by generating, testing, and evolving multiple prompt versions to find the most accurate, reliable, and efficient configuration for your use case. The optimizer uses a data-driven approach and continuous testing to reduce hallucinations, malfunctions, and inefficiency in agent responses.</p>"},{"location":"prompt_optimization/#user-input-and-configuration","title":"User Input and Configuration","text":"<p>To start the optimization process, users configure several key settings:</p> <ul> <li>LLM Provider: Choose from available language model providers.</li> <li>Agent Type: Select either a Foundry Agent (by providing an Agent ID) or a Logical Agent (by entering an initial prompt).</li> <li>Population Size: Set how many prompt candidates are generated and tested per cycle.</li> <li>Number of Cycles: Specify how many optimization rounds to run.</li> <li>Score Threshold: Define the minimum average score required for a prompt to be considered successful.</li> <li>Dataset: Provide agent queries and expected responses for evaluation.</li> </ul> <p>These settings guide the optimizer in generating, evaluating, and refining prompts to achieve the best performance.</p> <p>Purpose and Motivation</p> <p>Poorly optimized prompts can cause hallucinations, malfunctions, and inefficiency in agent responses. Hallucinations occur when the AI makes up facts or actions. Malfunctions happen when the AI misuses tools, follows broken reasoning, or produces inconsistent output. Inefficiency results in responses that are too verbose, unclear, or slow. The Prompt Optimizer addresses these issues through data-driven refinement and continuous testing.</p>"},{"location":"prompt_optimization/#pareto-sampling","title":"Pareto Sampling","text":"<p>Pareto sampling is a method used to select the most robust prompt candidates for the next cycle. A prompt is considered dominant if it performs well across all metrics without being outperformed by another prompt. This ensures that the selected prompts are balanced and reliable, avoiding the risk of over-optimization in a single area.</p>"},{"location":"prompt_optimization/#lesson-manager","title":"Lesson Manager","text":"<p>The lesson manager plays a critical role in refining prompts that do not meet the desired performance threshold. It extracts insights from failed or low-performing prompts and applies these lessons to generate new candidates. By removing duplicate lessons and focusing on actionable improvements, the lesson manager ensures that each cycle produces stronger and more effective prompts.</p>"},{"location":"prompt_optimization/#working-process","title":"Working Process","text":"<p>The working process employs an LLM as a judge approach to evaluate and optimize prompts through multiple optimization cycles. Each prompt candidate is tested on the provided dataset, generating four key performance metrics for every response: </p> <ul> <li>Accuracy \u2013 factual correctness.</li> <li>Tool Usage \u2013 choosing and using tools/API calls correctly.</li> <li>Clarity \u2013 clear and logical output.</li> <li>Brevity \u2013 concise and to the point.</li> </ul> <p>Process Example:</p> <p>Consider an optimization scenario where the population size is set to five prompt candidates, the user defines a performance threshold, and configures the system to run multiple optimization cycles. Here's how the process unfolds:</p> <p>Initial Generation and Evaluation:</p> <p>The optimizer begins by creating five distinct prompt candidates, each with different approaches to instruction formatting, tone, and structure. All five candidates are then tested against the provided dataset. For each prompt's response to every query in the dataset, the LLM judge evaluates four metrics: <code>accuracy</code>, <code>tool usage</code>, <code>clarity</code>, and <code>brevity</code>. This comprehensive evaluation produces four performance scores per prompt, generating a total of twenty individual metric scores across all candidates in the cycle.</p> <p>Threshold Analysis and Lesson Generation:</p> <p>The system calculates the average performance score for each prompt candidate. Any prompt whose average score falls below the user-defined threshold is flagged for improvement. For these underperforming prompts, the optimizer analyzes their specific weaknesses and generates targeted lessons. These lessons capture insights about what went wrong and provide guidance for creating better prompts in future iterations.</p> <p>Pareto-Based Selection:</p> <p>The optimizer then applies Pareto-based sampling to identify the most balanced and robust prompt candidates. Rather than simply selecting the highest-scoring prompts, this method identifies prompts that perform consistently well across all four metrics without significant weaknesses in any area. Typically, two to three dominant candidates emerge from this selection process and advance to the next optimization cycle.</p> <p>Advanced Improvement Techniques:</p> <p>When the system identifies failures or consistently low-performing examples, it activates two specialized improvement mechanisms:</p> <p>The reflection agent conducts a deep analysis of all available performance data, including individual metric scores, average scores across all candidates, and complete execution trace data. By examining these patterns, it identifies specific issues such as unclear instructions, improper tool usage guidance, or verbose language. Based on these insights, the reflection agent generates new prompt candidates specifically designed to address the identified weaknesses.</p> <p>The lesson manager serves as a complementary improvement mechanism. It systematically extracts and processes lessons from previous optimization cycles, removes duplicate insights, and refines the accumulated knowledge. When the reflection agent doesn't generate sufficient new candidates, the lesson manager leverages this refined knowledge base to create additional high-quality prompt alternatives.</p> <p>Convergence and Completion:</p> <p>The optimization process continues through multiple cycles until convergence is achieved. Convergence occurs when all prompt candidates consistently meet or exceed the performance threshold across all metrics. At this point, no additional reflection or lesson generation is necessary, and the system selects the optimal prompt as the final output.</p> <p>In convergence cases where all prompts exceed the threshold, no reflection or lesson manager steps are triggered, and no additional lessons are generated. The process concludes with the selection of the optimal prompt.</p> <p>The optimizer provides a comprehensive final output, including the optimized prompt, a detailed evaluation report, and a comparison between the initial and final prompts, highlighting all improvements made during the optimization process.</p>"},{"location":"prompt_optimization/#optimization-benefits-and-deliverables","title":"Optimization Benefits and Deliverables","text":"<p>Hallucination and Malfunction Prevention</p> <p>The optimizer ensures correct tool use, clear and logical reasoning, and robust error handling. Prompts are optimized to handle edge cases and invalid inputs gracefully. Balanced multi-metric optimization ensures that a prompt must perform well in all key areas, not just one.</p> <p>Output Components</p> <p>You receive a final optimized prompt tailored to your workflows and tested on real scenarios. An audit trail is provided in Excel or JSON format, logging all candidates, scores, and prompt evolution. A before and after diff shows exactly what changed in the instructions. The process reduces hallucinations and failures, and metrics can be customized for your priorities, such as safety, detail level, or tone.</p> <p>Business Impact</p> <p>The Prompt Optimizer delivers higher accuracy, consistent performance across tasks and edge cases, and time savings through automation. It provides transparency with full visibility into what changed, why, and how it improves performance. The process is scalable and can be re-run whenever tasks, tools, or requirements change.</p> <p>Final Deliverables</p> <p>The deliverables include:</p> <ul> <li>optimized_prompt.txt \u2013 the final prompt ready for deployment</li> <li>prompt_diff_report.json \u2013 before and after changes</li> <li>optimization_log.xlsx \u2013 detailed scoring and evolution history</li> </ul>"},{"location":"Agents%20Design/Multi%20Agent%20Design/","title":"Multi Agent Design","text":""},{"location":"Agents%20Design/Multi%20Agent%20Design/#multi-agent-design","title":"Multi Agent Design","text":"Multi Agent Without Feedback Multi Agent With Feedback <p>The Multi Agent operates on the Planner-Executor-Critic paradigm, which involves three key components:</p> <ol> <li>Planner Agent: Generates a detailed step-by-step plan based on the user query.</li> <li>Executor Agent: Executes each step of the plan sequentially.</li> <li>Critic: Evaluates the outputs of each step by scoring the results.</li> </ol> <p>The framework supports two types of Multi Agents, differentiated by their feedback mechanism: With Feedback (Human-in-the-Loop) and Without Feedback (Fully Automated). Users have the flexibility to enable or disable the Human-in-the-Loop feature based on their requirements.</p> <p>How It Works:</p>"},{"location":"Agents%20Design/Multi%20Agent%20Design/#1-multi-agent-without-feedback","title":"1. Multi Agent Without Feedback","text":"<ol> <li>The user provides a query, and the <code>Planner Agent</code> generates a clear, step-by-step plan.</li> <li>The plan is passed to the <code>Executor Agent</code>, which executes each step sequentially.</li> <li>The <code>Response Generator Agent</code> processes the results and sends them to the <code>Critic</code> for evaluation.</li> <li>The <code>Critic</code> scores the response:</li> <li>If the score meets the threshold, the final response is returned to the user.</li> <li>If the score is below the threshold, the response is sent back to the <code>Critic-Based Planner Agent</code> for refinement. The process repeats until an accurate response is generated.</li> </ol> <p>This entire process happens internally and is not visible to the user. The user only sees the final response, ensuring a seamless experience.</p>"},{"location":"Agents%20Design/Multi%20Agent%20Design/#2-multi-agent-with-feedback","title":"2. Multi Agent With Feedback","text":"<ol> <li>After the <code>Planner Agent</code> generates a step-by-step plan, the user can review the plan.</li> <li>If the <code>Human-in-the-Loop</code> option is enabled:</li> <li><code>Approval</code>: If the user is satisfied with the plan, they can approve it by clicking a thumbs-up or like button. The approved plan is then passed to the <code>Executor Agent</code>, which executes each step sequentially. The results are evaluated by the <code>Critic</code>, and the process continues as in the <code>Without Feedback</code> mode.</li> <li><code>Rejection</code>: If the user is not satisfied with the plan, they can reject it by clicking a thumbs-down button and providing feedback. This feedback is sent to the <code>Re-Planner Agent</code>, which refines the plan based on the user's input. The process repeats until the user approves the plan.</li> <li>Once the plan is finalized and approved, the system proceeds with execution and evaluation, ultimately delivering the final response to the user.</li> </ol> <p>This mode allows users to actively participate in the planning process, ensuring the generated plan aligns with their expectations. By incorporating user feedback at critical stages, the <code>With Feedback</code> mode enhances accuracy, adaptability, and user satisfaction.</p>"},{"location":"Agents%20Design/Orchestration%20Meta%20Agent%20Design/","title":"Orchestration Meta Agent Design","text":""},{"location":"Agents%20Design/Orchestration%20Meta%20Agent%20Design/#meta-agent-design","title":"Meta Agent Design","text":"<p>The <code>Meta Agent</code>, also known as the <code>Supervisor Agent</code>, orchestrates a combination of worker agents \u2014 which can be ReAct agents, Multi-Agents, or hybrids of both. It manages and supervises these worker agents by deciding which agent(s) to invoke based on the user's query and coordinates their responses to deliver the final answer.</p> <p>How It Works:</p> <ol> <li> <p>User Query Input:</p> <p>The user query is received by the Meta Agent, which acts as the central supervisor.</p> </li> <li> <p>Worker Agent Selection:</p> <p>The Meta Agent analyzes the query and decides which type of worker agent (ReAct, Multi-Agent, or a combination) is best suited to handle the task based on the query\u2019s complexity, required capabilities, and context.</p> </li> <li> <p>Delegation to Worker Agent(s):</p> <p>The selected worker agent(s) are called upon to process the query. Each worker agent internally uses their bound tools \u2014 small Python functions implementing specific logic or actions \u2014 to perform their tasks.</p> </li> <li> <p>Worker Agent Processing:</p> <ul> <li> <p>ReAct Agents proceed through their iterative reasoning and acting cycle (reasoning about tools, acting, observing results, and refining).</p> </li> <li> <p>Multi-Agents may work collaboratively or independently, coordinating their tools and knowledge to resolve the query.</p> </li> </ul> </li> <li> <p>Response Aggregation:</p> <p>The Meta Agent collects and evaluates responses from the worker agent(s).</p> </li> <li> <p>Decision Making &amp; Final Answer:</p> <p>Based on aggregated responses and overall context, the Meta Agent decides:</p> <ul> <li> <p>To invoke additional worker agents if needed, or</p> </li> <li> <p>To conclude and return the final consolidated answer to the user.</p> </li> </ul> </li> </ol> <p>The Meta Agent (Supervisor) acts as a high-level controller that dynamically delegates tasks to the most appropriate internal worker agent(s), whether ReAct, Multi-Agent, or a hybrid. It handles query routing, response coordination, and final decision-making, ensuring a flexible and efficient multi-layered reasoning and action system that leverages specialized tools bound to each agent.</p>"},{"location":"Agents%20Design/React%20Agent%20Design/","title":"React Agent Design","text":""},{"location":"Agents%20Design/React%20Agent%20Design/#react-agent-design","title":"React Agent Design","text":"<p>The <code>ReAct (Reasoning and Acting)</code> agent is designed to combine reasoning traces with action execution. It follows a step-by-step process to determine the appropriate tools to use, execute actions, observe the results, and iteratively refine its decisions until it arrives at a final answer.</p> <p>How It Works:</p> <ol> <li> <p>User Query Input:    The process begins with the user query being passed to the React Agent.</p> </li> <li> <p>Reasoning Phase:    The agent analyzes the query and reasons about which tools are required to address the task. This reasoning is based on the context and the nature of the query.</p> </li> <li> <p>Action Phase:    The agent takes action by invoking the appropriate tool(s). For example, it may call an external API, perform a computation, or retrieve data.</p> </li> <li> <p>Feedback Loop:    The response from the tool is fed back into the React Agent. The agent evaluates the tool's output and determines whether additional tools need to be called or if the process can be concluded.</p> </li> <li> <p>Decision Making:    Based on the user query and the results obtained from the tools, the agent decides:</p> <ul> <li>Whether to call another tool for further processing.</li> <li>Whether to stop and return the final answer to the user.</li> </ul> </li> </ol> <p>This iterative reasoning and acting process ensures that the React Agent can handle complex queries efficiently by dynamically adapting its actions based on the context and intermediate results. It is particularly useful for scenarios requiring logical decision-making and multi-step workflows.</p>"},{"location":"Agents%20Design/overview/","title":"Overview","text":"<p>Agentic Foundry supports three types of agent templates: </p> <ul> <li> <p>React Agent: Designed for single-task operations, React Agents use a step-by-step reasoning process to determine and execute the appropriate actions. They are ideal for scenarios requiring precise and efficient task execution. Learn more in the React Agent Design document.</p> </li> <li> <p>Multi Agent: Multi Agents enable collaboration between specialized agents to achieve complex objectives. They follow the Planner-Executor-Critic paradigm, ensuring tasks are planned, executed, and evaluated effectively. Detailed information is available in the Multi Agent Design document.</p> </li> <li> <p>Meta Agent: Acting as orchestrators, Meta Agents manage and coordinate other agents to achieve high-level goals. They dynamically adapt to context and task requirements, ensuring seamless execution. Explore their design in the Meta Agent Design document.</p> </li> </ul> <p>Each agent template is highly customizable, allowing developers to tailor them to specific use cases, making Agentic Foundry a robust platform for building intelligent systems.</p>"},{"location":"Agents_Design/Hybrid_Agent_Design/","title":"Hybrid Agent Design","text":"<p>The hybrid agent does not use LangChain or LangGraph.</p> <p>It is termed <code>hybrid</code> because, unlike the multi agent (planner-executor-critic) architecture which uses separate agents for planning, executing, and critiquing, this approach employs a single agent instance. In the multi agent setup, one agent is responsible for generating the plan, another for executing each step, and a third for providing feedback or critique. In contrast, the hybrid agent creates the plan and then executes each step itself, all within the same agent instance.</p> <p>Other aspects, such as feedback learning and the use of a canvas, remain consistent with other agent templates.</p>"},{"location":"Agents_Design/Multi%20Agent%20Design/","title":"Multi Agent Design","text":""},{"location":"Agents_Design/Multi%20Agent%20Design/#multi-agent-design","title":"Multi Agent Design","text":"Multi Agent Without Feedback Multi Agent With Feedback <p>The Multi Agent operates on the Planner-Executor-Critic paradigm, which involves three key components:</p> <ol> <li>Planner Agent: Generates a detailed step-by-step plan based on the user query.</li> <li>Executor Agent: Executes each step of the plan sequentially.</li> <li>Critic: Evaluates the outputs of each step by scoring the results.</li> </ol> <p>The framework supports two types of Multi Agents, differentiated by their feedback mechanism: With Feedback (Human-in-the-Loop) and Without Feedback (Fully Automated). Users have the flexibility to enable or disable the Human-in-the-Loop feature based on their requirements.</p> <p>How It Works:</p>"},{"location":"Agents_Design/Multi%20Agent%20Design/#1-multi-agent-without-feedback","title":"1. Multi Agent Without Feedback","text":"<ol> <li>The user provides a query, and the <code>Planner Agent</code> generates a clear, step-by-step plan.</li> <li>The plan is passed to the <code>Executor Agent</code>, which executes each step sequentially.</li> <li>The <code>Response Generator Agent</code> processes the results and sends them to the <code>Critic</code> for evaluation.</li> <li>The <code>Critic</code> scores the response:</li> <li>If the score meets the threshold, the final response is returned to the user.</li> <li>If the score is below the threshold, the response is sent back to the <code>Critic-Based Planner Agent</code> for refinement. The process repeats until an accurate response is generated.</li> </ol> <p>This entire process happens internally and is not visible to the user. The user only sees the final response, ensuring a seamless experience.</p>"},{"location":"Agents_Design/Multi%20Agent%20Design/#2-multi-agent-with-feedback","title":"2. Multi Agent With Feedback","text":"<ol> <li>After the <code>Planner Agent</code> generates a step-by-step plan, the user can review the plan.</li> <li>If the <code>Human-in-the-Loop</code> option is enabled:</li> <li><code>Approval</code>: If the user is satisfied with the plan, they can approve it by clicking a thumbs-up or like button. The approved plan is then passed to the <code>Executor Agent</code>, which executes each step sequentially. The results are evaluated by the <code>Critic</code>, and the process continues as in the <code>Without Feedback</code> mode.</li> <li><code>Rejection</code>: If the user is not satisfied with the plan, they can reject it by clicking a thumbs-down button and providing feedback. This feedback is sent to the <code>Re-Planner Agent</code>, which refines the plan based on the user's input. The process repeats until the user approves the plan.</li> <li>Once the plan is finalized and approved, the system proceeds with execution and evaluation, ultimately delivering the final response to the user.</li> </ol> <p>This mode allows users to actively participate in the planning process, ensuring the generated plan aligns with their expectations. By incorporating user feedback at critical stages, the <code>With Feedback</code> mode enhances accuracy, adaptability, and user satisfaction.</p>"},{"location":"Agents_Design/Orchestration_Meta_Agent_Design/","title":"Orchestration Meta Agent Design","text":""},{"location":"Agents_Design/Orchestration_Meta_Agent_Design/#meta-agent-design","title":"Meta Agent Design","text":"<p>The <code>Meta Agent</code>, also known as the <code>Supervisor Agent</code>, orchestrates a combination of worker agents \u2014 which can be ReAct agents, Multi-Agents, or hybrids of both. It manages and supervises these worker agents by deciding which agent(s) to invoke based on the user's query and coordinates their responses to deliver the final answer.</p> <p>How It Works:</p> <ol> <li> <p>User Query Input:</p> <p>The user query is received by the Meta Agent, which acts as the central supervisor.</p> </li> <li> <p>Worker Agent Selection:</p> <p>The Meta Agent analyzes the query and decides which type of worker agent (ReAct, Multi-Agent, or a combination) is best suited to handle the task based on the query\u2019s complexity, required capabilities, and context.</p> </li> <li> <p>Delegation to Worker Agent(s):</p> <p>The selected worker agent(s) are called upon to process the query. Each worker agent internally uses their bound tools \u2014 small Python functions implementing specific logic or actions \u2014 to perform their tasks.</p> </li> <li> <p>Worker Agent Processing:</p> <ul> <li> <p>ReAct Agents proceed through their iterative reasoning and acting cycle (reasoning about tools, acting, observing results, and refining).</p> </li> <li> <p>Multi-Agents may work collaboratively or independently, coordinating their tools and knowledge to resolve the query.</p> </li> </ul> </li> <li> <p>Response Aggregation:</p> <p>The Meta Agent collects and evaluates responses from the worker agent(s).</p> </li> <li> <p>Decision Making &amp; Final Answer:</p> <p>Based on aggregated responses and overall context, the Meta Agent decides:</p> <ul> <li> <p>To invoke additional worker agents if needed, or</p> </li> <li> <p>To conclude and return the final consolidated answer to the user.</p> </li> </ul> </li> </ol> <p>The Meta Agent (Supervisor) acts as a high-level controller that dynamically delegates tasks to the most appropriate internal worker agent(s), whether ReAct, Multi-Agent, or a hybrid. It handles query routing, response coordination, and final decision-making, ensuring a flexible and efficient multi-layered reasoning and action system that leverages specialized tools bound to each agent.</p>"},{"location":"Agents_Design/Planner_executor_agent_design/","title":"Planner Executor Agent Design","text":""},{"location":"Agents_Design/Planner_executor_agent_design/#planner-executor-agent-design","title":"Planner-Executor Agent Design","text":"<p>The Planner-Executor Agent design is inspired by the multi-agent paradigm, focusing on the clear separation of planning and execution responsibilities to efficiently solve complex tasks. This architecture is particularly effective for scenarios that require structured problem-solving, iterative refinement, and robust error handling.</p> <p>Key Components:</p> <ol> <li> <p>Planner Agent: Responsible for analyzing the user query and generating a detailed, step-by-step plan or workflow. The plan outlines the sequence of actions or tool invocations required to achieve the desired outcome.</p> </li> <li> <p>Executor Agent: Takes the plan generated by the Planner Agent and executes each step in order. The Executor Agent interacts with tools, APIs, or other resources as specified in the plan, collecting results and handling intermediate outputs.</p> </li> </ol> <p>How It Works:</p> <ol> <li>User Query Input: The process begins when the user submits a query or task.</li> <li>Planning Phase: The Planner Agent decomposes the query into actionable steps, creating a structured plan.</li> <li>Execution Phase: The Executor Agent sequentially executes each step of the plan, invoking the necessary tools or actions.</li> <li>Observation and Iteration: After each step, the Executor Agent observes the results. If a step fails or produces unexpected results, the Executor can either retry, adjust the plan, or escalate the issue for further planning.</li> <li>Final Response: Once all steps are successfully executed, the final result is compiled and returned to the user.</li> </ol> <p>Advantages:</p> <ul> <li>Modularity: Clear separation of planning and execution allows for easier debugging, maintenance, and extension.</li> <li>Adaptability: The system can dynamically adjust plans based on intermediate results or errors.</li> <li>Transparency: Each step in the process is traceable, enabling better understanding and auditing of agent behavior.</li> </ul> <p>This design can be further extended by integrating a Critic Agent for evaluation and feedback, or by incorporating Human-in-the-Loop mechanisms for plan approval and refinement, as described in the Multi Agent Design section.</p>"},{"location":"Agents_Design/Planner_meta_agent_design/","title":"Planner Meta Agent Design","text":""},{"location":"Agents_Design/Planner_meta_agent_design/#planner-meta-agent-design","title":"Planner Meta Agent Design","text":"<p>The Planner Meta Agent extends the concept of meta-level orchestration by combining advanced planning capabilities with dynamic agent management. Acting as a high-level controller, the Planner Meta Agent coordinates multiple specialized worker agents\u2014such as Planner-Executor agents, ReAct agents, or Multi-Agents\u2014to solve complex user queries efficiently and flexibly.</p> <p>Key Responsibilities:</p> <ol> <li> <p>User Query Reception: The Planner Meta Agent receives the user's query and serves as the entry point for the orchestration process.</p> </li> <li> <p>Strategic Planning: It formulates a high-level plan or workflow, breaking down the query into manageable sub-tasks. This plan determines which worker agents or agent types are best suited for each sub-task.</p> </li> <li> <p>Agent Selection and Delegation: Based on the plan, the Planner Meta Agent selects appropriate worker agents (e.g., Planner-Executor, ReAct, or Multi-Agent) and delegates sub-tasks to them. Each agent is chosen for its specialized capabilities relevant to the sub-task.</p> </li> <li> <p>Supervision and Coordination: The Planner Meta Agent supervises the execution of sub-tasks, monitors progress, and coordinates communication between agents. It ensures that outputs from one agent can be used as inputs for others when necessary.</p> </li> <li> <p>Aggregation and Evaluation: After all sub-tasks are completed, the Planner Meta Agent aggregates the results, evaluates their quality, and determines if further refinement or additional agent invocation is needed.</p> </li> <li> <p>Final Decision and Response: Once the aggregated results meet the desired criteria, the Planner Meta Agent compiles the final answer and returns it to the user.</p> </li> </ol> <p>Advantages:</p> <ul> <li>Hierarchical Control: Enables multi-layered reasoning and action by combining planning, delegation, and supervision.</li> <li>Flexibility: Dynamically selects and coordinates specialized agents based on task requirements.</li> <li>Scalability: Can handle complex, multi-step queries by decomposing them and leveraging multiple agents in parallel or sequence.</li> </ul> <p>This design ensures robust, adaptive, and transparent orchestration of agent-based workflows, making it suitable for advanced AI systems that require both strategic planning and dynamic execution across diverse agent types.</p>"},{"location":"Agents_Design/React%20Agent%20Design/","title":"React Agent Design","text":""},{"location":"Agents_Design/React%20Agent%20Design/#react-agent-design","title":"React Agent Design","text":"<p>The <code>ReAct (Reasoning and Acting)</code> agent is an intelligent system that combines reasoning traces with action execution. It follows a systematic, iterative process to analyze queries, select appropriate tools, execute actions, and refine decisions until reaching a final answer.</p> <p>How It Works:</p> <p>1. User Query Input:</p> <p>The process begins when a user query is passed to the React Agent for processing.</p> <p>2. Reasoning Phase:  </p> <p>The agent performs context-aware analysis of the query to determine:</p> <ul> <li>The nature and complexity of the task</li> <li>Which tools or resources are required</li> <li>The optimal sequence of actions needed</li> </ul> <p>3. Action Phase: </p> <p>The agent executes the selected action by invoking appropriate tools, which may include:</p> <ul> <li>External API calls</li> <li>Data retrieval operations</li> <li>Computational tasks</li> <li>Database queries</li> </ul> <p>4. Observation and Evaluation: </p> <p>After each action, the agent:</p> <ul> <li>Receives and analyzes the tool's output</li> <li>Evaluates the quality and completeness of the response</li> <li>Determines if the results satisfy the query requirements</li> </ul> <p>5. Decision Making:  </p> <p>Based on the evaluation, the agent decides whether to:</p> <ul> <li>Execute additional tools for further processing</li> <li>Refine the current approach with different parameters</li> <li>Conclude the process and return the final answer</li> </ul> <p>6. Iterative Refinement:</p> <p>If additional processing is needed, the agent returns to the reasoning phase with updated context, creating a feedback loop that continues until the query is fully resolved.</p> <p>Key Benefits:</p> <ul> <li>Dynamic Adaptation: Adjusts strategy based on intermediate results</li> <li>Multi-step Processing: Handles complex queries requiring sequential operations</li> <li>Efficient Resource Usage: Only invokes necessary tools</li> <li>Logical Decision-Making: Makes informed choices at each step</li> </ul> <p>This iterative reasoning and acting process ensures that the React Agent can handle complex queries efficiently by dynamically adapting its actions based on the context and intermediate results. It is particularly useful for scenarios requiring logical decision-making and multi-step workflows.</p>"},{"location":"Agents_Design/ReactCriticAgent_Design/","title":"React Critic Agent Design","text":""},{"location":"Agents_Design/ReactCriticAgent_Design/#react-critic-agent-design","title":"React Critic Agent Design","text":"<p>The <code>React Critic Agent</code> builds upon the core principles of the ReAct (Reasoning and Acting) agent, introducing an additional layer of critical evaluation to enhance decision-making and output quality. While the standard React Agent iteratively reasons and acts to solve a task, the React Critic Agent incorporates a critic module that reviews, critiques, and refines the agent's intermediate reasoning steps and actions before finalizing a response.</p> <p>How It Works:</p> <ol> <li> <p>User Query Input:     The process starts with the user query being passed to the React Critic Agent.</p> </li> <li> <p>Reasoning and Action Phase:     The agent reasons about the query, determines which tools to use, and performs actions as in the standard React Agent.</p> </li> <li> <p>Critic Evaluation Phase:     After each reasoning-action step, the Critic module evaluates the agent's thought process and the results of the actions. It checks for logical consistency, correctness, and relevance to the user query.</p> </li> <li> <p>Feedback and Refinement Loop:     The Critic provides feedback, suggesting improvements or corrections. The agent can revise its reasoning or actions based on this feedback, ensuring higher accuracy and reliability.</p> </li> <li> <p>Decision Making:     The process continues iteratively, with the Critic reviewing each step, until the agent and Critic agree on a satisfactory solution. The final answer is then returned to the user.</p> </li> </ol> <p>This design enables the React Critic Agent to handle complex, multi-step queries with greater robustness by minimizing errors and improving the quality of intermediate and final outputs. It is especially valuable in scenarios where accuracy, transparency, and self-correction are critical.</p>"},{"location":"Agents_Design/overview/","title":"Overview","text":"<p>Agentic Foundry supports a variety of agent templates:</p> <p>React Agent:</p> <p>Designed for single-task operations, React Agents use a step-by-step reasoning process to determine and execute the appropriate actions. They are ideal for scenarios requiring precise and efficient task execution. Learn more in the React Agent Design document.</p> <p>React Critic Agent:</p> <p>An extension of the React Agent, the React Critic Agent introduces a dual system prompt mechanism for self-critique and improved output quality. See React Critic Agent Design for details.</p> <p>Multi Agent:</p> <p>Multi Agents enable collaboration between specialized agents to achieve complex objectives. They follow the Planner-Executor-Critic paradigm, ensuring tasks are planned, executed, and evaluated effectively. Detailed information is available in the Multi Agent Design document.</p> <p>Planner Executor Agent:</p> <p>This agent type implements the Planner-Executor-Critic workflow with enhanced system prompt structure for more granular control. See Planner Executor Agent Design.</p> <p>Meta Agent:</p> <p>Acting as orchestrators, Meta Agents manage and coordinate other agents to achieve high-level goals. They dynamically adapt to context and task requirements, ensuring seamless execution. Explore their design in the Meta Agent Design document.</p> <p>Planner Meta Agent:</p> <p>An advanced orchestrator, the Planner Meta Agent uses multiple system prompts to provide robust, adaptive, and transparent orchestration of agent-based workflows. Learn more in Planner Meta Agent Design.</p> <p>Each agent template is highly customizable, allowing developers to tailor them to specific use cases, making Agentic Foundry a robust platform for building intelligent systems.</p> <p>Hybrid Agent:</p> <p>Hybrid Agents combine features from multiple agent types, enabling flexible workflows that leverage both step-by-step reasoning and collaborative planning. They are suited for scenarios requiring dynamic adaptation between individual and multi-agent strategies. For more details, refer to the Hybrid Agent Design document.</p>"},{"location":"Database%20setup/Database%20Setup/","title":"PostgreSQL Setup Guide","text":""},{"location":"Database%20setup/Database%20Setup/#installation-on-vm","title":"Installation on VM","text":"<ol> <li>Download the PostgreSQL installation wizard and start it up. </li> <li>Choose the default directory or customize as required. </li> <li> <p>All the components will be selected by default and will be useful,  so keep them as it is and Next to continue. </p> <p></p> </li> <li> <p>Choose the default Data directory or change as required. </p> </li> <li> <p>Create a password for postgres (superuser) - This password will be used in the connection string for connecting to the database: <code>postgresql://postgres:password@localhost:port/database</code>.</p> <p></p> </li> <li> <p>Set the port number (default: 5432) or change if required. </p> </li> <li> <p>Use the Locale field to specify the locale that will be used by the new database cluster. The Default locale is the operating system locale. You can leave this as is and click next to continue.</p> <p></p> </li> <li> <p>Click Next to continue.</p> <p></p> </li> <li> <p>Click Next to start the installation. </p> </li> <li>After installation is complete, there will be a checked check box which asks if additional tools should be installed to complement your postgres installation using Stack Builder.</li> <li> <p>You should uncheck this as it is not necessary, and it is also not possible as the url gets blocked by the VM.</p> <p></p> </li> </ol>"},{"location":"Database%20setup/Database%20Setup/#installation-on-local","title":"Installation on local","text":"<ol> <li>Install PostgreSQL from company portal</li> <li> <p>Verify installation:</p> <ul> <li>Open SQL Shell (psql)</li> <li>Default username, password, database: <code>postgres</code></li> <li> <p>Default port: <code>5432</code> </p> <p></p> </li> <li> <p>Default connection string: <code>postgresql://postgres:postgres@localhost:5432/postgres</code></p> </li> <li>Connection String Format</li> </ul> <pre><code>postgresql://username:password@host:port/database\n</code></pre> <p>Example:</p> <pre><code>postgresql://postgres:postgres@localhost:5432/postgres\n</code></pre> </li> <li> <p>Run <code>\\l</code> command to check list of databases, username, and password status</p> <p></p> </li> </ol>"},{"location":"Database%20setup/Database%20Setup/#database-setup","title":"Database Setup","text":"<p>Environment Configuration</p> <p>Create a <code>.env</code> file with the following variables:</p> <pre><code>DATABASE_URL=postgresql://postgres:postgres@localhost:5432/postgres\n\n# PostgreSQL Configuration\nPOSTGRESQL_HOST=localhost\nPOSTGRESQL_USER=postgres\nPOSTGRESQL_PASSWORD=postgres\nDATABASE=your_database_name\nPOSTGRESQL_DATABASE_URL=postgresql://postgres:postgres@localhost:5432/your_database_name?sslmode=disable\n</code></pre> <p>Database Creation</p> <p>1. Define required databases in a list variable in <code>database_manager.py</code> file:</p> <pre><code>REQUIRED_DATABASES = [\n    \"feedback_learning\",\n    \"telemetry_logs\", \n    \"agentic_workflow_as_service_database\",\n    \"login\",\n    \"logs\",\n    \"arize_traces\"\n]\n</code></pre> <p>2. Load environment variables in <code>database_manager.py</code> file:</p> <pre><code>Postgre_string = os.getenv(\"DATABASE_URL\")\nPOSTGRESQL_HOST = os.getenv(\"POSTGRESQL_HOST\", \"\")\nPOSTGRESQL_USER = os.getenv(\"POSTGRESQL_USER\", \"\")\nPOSTGRESQL_PASSWORD = os.getenv(\"POSTGRESQL_PASSWORD\", \"\")\nDATABASE = os.getenv(\"DATABASE\", \"\")\nDATABASE_URL = os.getenv(\"POSTGRESQL_DATABASE_URL\", \"\")\n</code></pre> <p>3. Create function to connect to postgres database in <code>database_manager.py</code> file:</p> <pre><code>def get_postgres_url():\n    url = urlparse(Postgre_string)\n    # Replace path with '/postgres'\n    new_url = url._replace(path=\"/postgres\")\n    return urlunparse(new_url)\n</code></pre> <p>4. Create Databases function</p> <ul> <li>The system will connect to the 'postgres' database under postgres user and create the required databases listed in <code>REQUIRED_DATABASES</code> using following code in <code>database_manager.py</code> file.</li> </ul> <pre><code>async def check_and_create_databases():\n    conn = await asyncpg.connect(get_postgres_url())\n    try:\n        for db_name in REQUIRED_DATABASES:\n            exists = await conn.fetchval(\n                \"SELECT 1 FROM pg_database WHERE datname = $1\", db_name\n            )\n            if not exists:\n                print(f\"Database '{db_name}' not found. Creating...\")\n                await conn.execute(f'CREATE DATABASE \"{db_name}\"')\n            else:\n                print(f\"Database '{db_name}' already exists.\")\n    finally:\n        await conn.close()\n</code></pre>"},{"location":"Definitions/Vocabulary%20-%20Definitions/","title":"Vocabulary   Definitions","text":""},{"location":"Definitions/Vocabulary%20-%20Definitions/#what-is-an-ai-agent","title":"What is an AI Agent?","text":"<p>An AI agent is an autonomous entity that can perceive its environment and take actions to achieve specific goals. </p> <p></p> <p>Tools/Skills External functionalities that an AI agent can leverage to perform specific tasks. These extend the agent's capabilities beyond language understanding, enabling it to execute actions and interact with external systems effectively.</p> <p>Other Agents (Integrations/Interfaces) Interfaces or connections with other AI agents or systems. These enable collaboration, task delegation, and seamless integration within a multi-agent ecosystem or with external applications.</p> <p>Actions (Large Action Models) Predefined or dynamically learned complex operations that the agent can execute to achieve specific objectives or respond to user queries.</p> <p>Environment (Context/Prediction) The external state or data that the agent perceives and reacts to. This allows the agent to adapt its behavior dynamically based on real-time context or changing scenarios.</p> <p>Planning (Reasoning) The cognitive process of determining the next steps based on goals, environmental inputs, and memory. Planning drives the logical sequencing of actions and tool usage to accomplish tasks efficiently and effectively.</p> <p>Goals The objectives or desired outcomes defined for or by the agent. These guide the agent's decision-making and planning processes, ensuring its behavior aligns with its mission or user expectations.</p> <p>User (Feedback) Human users who interact with the agent and provide feedback, corrections, or instructions. This feedback helps refine the agent's responses and ensures alignment with user needs and expectations.</p> <p>Memory (Short and Long) The storage of past interactions or knowledge, categorized as: - Short Term Memory: Temporary context retained during the current conversation or session. - Long Term Memory: Persistent data stored across sessions, enabling the agent to maintain continuity and personalization over time.</p> <p>Agent (Core LLM) The central decision-making entity, typically powered by a large language model (LLM). It performs natural language understanding, reasoning, and orchestrates all other components to complete tasks and provide meaningful responses.</p>"},{"location":"Definitions/Vocabulary%20-%20Definitions/#key-components-of-an-agent","title":"Key Components of an Agent \u200b","text":"<p>LLM The brain of the agent that interprets instructions from humans or other agents, plans actions, or evaluates alternatives.</p> <p>Memory This where instructions, feedback, external stimulus and states are stored.</p> <p>Integrations/ Interface Agents need to communicate with other agents, humans or LLms to pass on messages, prompts, feedback and instructions.</p> <p>Adaptive Agents adapt and learn dynamically from the real time environment and adjust their actions.</p> <p>Skills and Tools This is the device like web-browser, fill forms, use other applications(API's) tools like OCR, image generators, social media plugins etc. to take real life actions.</p> <p>Reasoning Capabilities Different additional frameworks like Chain/Graph of thoughts, RAG Setups, vector DBs are needed to augment performance.</p>"},{"location":"Definitions/Vocabulary%20-%20Definitions/#roles-of-an-agent","title":"Roles of an Agent","text":"<p>AI agents can adopt a wide range of roles based on the task, domain, and context in which they are deployed. These roles showcase the adaptability and functional versatility of intelligent agents, making them valuable across both business and technical environments.</p> <p></p>"},{"location":"Definitions/Vocabulary%20-%20Definitions/#react-agent","title":"React Agent","text":"<p>The ReAct(Reasoning and Acting) agent combines reasoning traces with action execution. It uses a step by step thought process to determine what tool to use, executes it, observe the result, and continues until it can return a final answer.</p>"},{"location":"Definitions/Vocabulary%20-%20Definitions/#multi-agent","title":"Multi Agent","text":"<p>The Multi Agent operates on the Planner-Executor-Critic paradigm. It begins with a Planner Agent that generates a step-by-step plan based on the user query. The Executor Agent then executes each step of the plan. The Critic evaluates the outputs by scoring the results of each step.</p>"},{"location":"Definitions/Vocabulary%20-%20Definitions/#planner-agent","title":"Planner Agent","text":"<p>The Planner Agent is responsible for creating a detailed, step-by-step plan to achieve a given objective or respond to a user query. It interprets the user's instructions, breaks them down into actionable steps, and presents the plan for review or execution. This ensures clarity, structure, and alignment with the desired outcome.</p>"},{"location":"Definitions/Vocabulary%20-%20Definitions/#executor-agent","title":"Executor Agent","text":"<p>The Executor Agent is responsible for carrying out the steps outlined by the Planner Agent. It executes each step in the plan. The Executor Agent ensures that actions are performed accurately and efficiently, adapting to any changes or feedback during execution to achieve the desired outcome.</p>"},{"location":"Definitions/Vocabulary%20-%20Definitions/#critic-agent","title":"Critic Agent","text":"<p>The Critic Agent evaluates the outputs generated during the execution process. It assigns a score to each result based on predefined criteria or thresholds. If the score meets or exceeds the threshold, the Critic Agent finalizes the response and presents it to the user. If the score falls below the threshold, the Critic Agent triggers internal adjustments or refinements to improve the output, ensuring the final response is accurate and aligned with user expectations.</p>"},{"location":"Definitions/Vocabulary%20-%20Definitions/#meta-agent","title":"Meta Agent","text":"<p>Serves as the central decision making entity. Individual agents are coordinated by a central supervisor agent. The supervisor controls all communication flow and task delegation, making decisions about which agent to invoke based on the current context and task requirements.</p>"},{"location":"Definitions/Vocabulary_Definitions/","title":"Vocabulary - Definitions","text":""},{"location":"Definitions/Vocabulary_Definitions/#what-is-an-ai-agent","title":"What is an AI Agent?","text":"<p>An AI agent is an <code>autonomous</code> entity that can <code>perceive</code> its environment and take <code>actions</code> to achieve specific <code>goals</code>. </p> <p></p> <p>Tools/Skills External functionalities that an AI agent can leverage to perform specific tasks. These extend the agent's capabilities beyond language understanding, enabling it to execute actions and interact with external systems effectively.</p> <p>Other Agents (Integrations/Interfaces) Interfaces or connections with other AI agents or systems. These enable collaboration, task delegation, and seamless integration within a multi-agent ecosystem or with external applications.</p> <p>Actions (Large Action Models) Predefined or dynamically learned complex operations that the agent can execute to achieve specific objectives or respond to user queries.</p> <p>Environment (Context/Prediction) The external state or data that the agent perceives and reacts to. This allows the agent to adapt its behavior dynamically based on real-time context or changing scenarios.</p> <p>Planning (Reasoning) The cognitive process of determining the next steps based on goals, environmental inputs, and memory. Planning drives the logical sequencing of actions and tool usage to accomplish tasks efficiently and effectively.</p> <p>Goals The objectives or desired outcomes defined for or by the agent. These guide the agent's decision-making and planning processes, ensuring its behavior aligns with its mission or user expectations.</p> <p>User (Feedback) Human users who interact with the agent and provide feedback, corrections, or instructions. This feedback helps refine the agent's responses and ensures alignment with user needs and expectations.</p> <p>Memory (Short and Long) The storage of past interactions or knowledge, categorized as: - Short Term Memory: Temporary context retained during the current conversation or session. - Long Term Memory: Persistent data stored across sessions, enabling the agent to maintain continuity and personalization over time.</p> <p>Agent (Core LLM) The central decision-making entity, typically powered by a large language model (LLM). It performs natural language understanding, reasoning, and orchestrates all other components to complete tasks and provide meaningful responses.</p>"},{"location":"Definitions/Vocabulary_Definitions/#key-components-of-an-agent","title":"Key Components of an Agent \u200b","text":"<p>LLM The brain of the agent that interprets instructions from humans or other agents, plans actions, or evaluates alternatives.</p> <p>Memory This where instructions, feedback, external stimulus and states are stored.</p> <p>Integrations/ Interface Agents need to communicate with other agents, humans or LLms to pass on messages, prompts, feedback and instructions.</p> <p>Adaptive Agents adapt and learn dynamically from the real time environment and adjust their actions.</p> <p>Skills and Tools This is the device like web-browser, fill forms, use other applications(API's) tools like OCR, image generators, social media plugins etc. to take real life actions.</p> <p>Reasoning Capabilities Different additional frameworks like Chain/Graph of thoughts, RAG Setups, vector DBs are needed to augment performance.</p>"},{"location":"Definitions/Vocabulary_Definitions/#roles-of-an-agent","title":"Roles of an Agent","text":"<p>AI agents can adopt a wide range of roles based on the task, domain, and context in which they are deployed. These roles showcase the adaptability and functional versatility of intelligent agents, making them valuable across both business and technical environments.</p> <p></p>"},{"location":"Definitions/Vocabulary_Definitions/#react-agent","title":"React Agent","text":"<p>The ReAct(Reasoning and Acting) agent combines reasoning traces with action execution. It uses a step by step thought process to determine what tool to use, executes it, observe the result, and continues until it can return a final answer.</p>"},{"location":"Definitions/Vocabulary_Definitions/#multi-agent","title":"Multi Agent","text":"<p>The Multi Agent operates on the <code>Planner-Executor-Critic</code> paradigm. It begins with a <code>Planner Agent</code> that generates a step-by-step plan based on the user query. The <code>Executor Agent</code> then executes each step of the plan. The <code>Critic</code> evaluates the outputs by scoring the results of each step.</p>"},{"location":"Definitions/Vocabulary_Definitions/#planner-agent","title":"Planner Agent","text":"<p>The Planner Agent is responsible for creating a detailed, step-by-step plan to achieve a given objective or respond to a user query. It interprets the user's instructions, breaks them down into actionable steps, and presents the plan for review or execution. This ensures clarity, structure, and alignment with the desired outcome.</p>"},{"location":"Definitions/Vocabulary_Definitions/#executor-agent","title":"Executor Agent","text":"<p>The Executor Agent is responsible for carrying out the steps outlined by the Planner Agent. It executes each step in the plan. The Executor Agent ensures that actions are performed accurately and efficiently, adapting to any changes or feedback during execution to achieve the desired outcome.</p>"},{"location":"Definitions/Vocabulary_Definitions/#critic-agent","title":"Critic Agent","text":"<p>The Critic Agent evaluates the outputs generated during the execution process. It assigns a score to each result based on predefined criteria or thresholds. If the score meets or exceeds the threshold, the Critic Agent finalizes the response and presents it to the user. If the score falls below the threshold, the Critic Agent triggers internal adjustments or refinements to improve the output, ensuring the final response is accurate and aligned with user expectations.</p>"},{"location":"Definitions/Vocabulary_Definitions/#meta-agent","title":"Meta Agent","text":"<p>Serves as the central decision making entity. Individual agents are coordinated by a central supervisor agent. The supervisor controls all communication flow and task delegation, making decisions about which agent to invoke based on the current context and task requirements.</p>"},{"location":"Definitions/hitl/","title":"Human Involvement in Agentic Frameworks","text":"<p>Human-in-the-loop (HITL) is a system design approach where human input, oversight, or decision-making is integrated into an automated or AI-driven process to improve accuracy, safety, or control.</p> <p>In the Agentic Foundry Framework, HITL is included as a core element of the Multi Agent, ensuring that humans can guide or intervene in agent behavior when necessary.</p>"},{"location":"Definitions/hitl/#overview-human-intervention-in-ai-systems","title":"Overview: Human Intervention in AI Systems","text":"<p>Human intervention refers to the active or passive role a human plays during the lifecycle of an AI or agent system. In our framework, this intervention is integrated at multiple stages:</p> <ul> <li>Design phase: Humans select and configure tools that the agent will use, establishing the foundational parameters and capabilities that guide agent behavior.</li> <li>Execution phase: Humans monitor outputs and provide real-time feedback, ensuring quality control and appropriate responses to dynamic situations.</li> <li>Learning phase: Humans review performance metrics and provide training examples to continuously refine agent capabilities and improve future performance.</li> </ul> <p>The goal is to combine the speed and efficiency of automation with the contextual understanding, judgment, and oversight that only humans can provide, creating a collaborative intelligence system.</p>"},{"location":"Definitions/hitl/#human-in-the-loop-hitl","title":"Human-in-the-Loop (HITL)","text":"<p>In this mode, humans are an essential part of the decision-making process. The agent does not proceed without human input at critical steps. This ensures high accuracy and safety, especially in sensitive or high-stakes domains where automated decisions could have significant consequences.</p> <p>Application in the Framework</p> <ul> <li>During inference, the agent generates draft responses that undergo human review before finalization, ensuring quality and appropriateness of outputs.</li> <li>During tool onboarding, users manually select and curate data sources, tools, and prompts, establishing the operational parameters for agent functionality.</li> <li>Continuous monitoring allows for real-time adjustments and intervention when agent behavior deviates from expected parameters.</li> </ul> <p>Typical Use Cases</p> <ul> <li>Domains requiring high precision and regulatory compliance, such as healthcare diagnostics, legal document review, or financial analysis.</li> <li>Tasks where incorrect or biased outputs could have serious consequences, including content generation for public consumption or automated decision-making systems.</li> <li>Content moderation or generation workflows where human approval is mandatory due to organizational policies or legal requirements.</li> </ul> <p>Workflow</p> <ol> <li>The user submits a query or task to the agent system, initiating the collaborative process.</li> <li>The agent processes the input and generates a preliminary output, such as a detailed step-by-step plan or draft response based on the user query and available tools.</li> <li>The human reviewer evaluates the preliminary output and makes a decision:<ul> <li>Approve: If the user approves the plan, the agent finalizes the output, applies any necessary formatting, and returns the completed response.</li> <li>Reject: If the user rejects the plan, they provide specific feedback detailing issues, concerns, or required modifications. The agent utilizes a replanner mechanism to regenerate a revised plan that addresses the feedback.</li> </ul> </li> <li>This review cycle continues until the human approves the output, ensuring the final response meets quality standards and user expectations before delivery.</li> </ol>"},{"location":"Evaluation/Consistency/","title":"Consistency and Robustness Evaluation","text":"<p>This framework provides a comprehensive methodology for measuring the reliability and resilience of AI agents through systematic evaluation of response consistency and robustness against challenging inputs.</p>"},{"location":"Evaluation/Consistency/#evaluation-metrics-overview","title":"Evaluation Metrics Overview","text":"<p>Consistency</p> <p>Measures the stability and repeatability of agent responses when presented with identical queries across multiple time intervals. Consistent performance ensures predictable behavior and builds user confidence in production environments.</p> <p>Robustness</p> <p>Assesses the agent's capability to maintain functional performance when encountering edge cases, malformed inputs, or adversarial scenarios. Robust systems demonstrate graceful degradation and error handling under challenging conditions.</p>"},{"location":"Evaluation/Consistency/#evaluation-methodology","title":"Evaluation Methodology","text":"<p>The framework employs a two-phase evaluation approach designed to comprehensively assess agent performance:</p> <p>Phase 1: Consistency Assessment</p> <ol> <li>Query Dataset Preparation: Compile standardized test queries with established ground truth responses</li> <li>Response Collection: Execute queries through the agent API and capture response data</li> <li>Temporal Analysis: Compare agent outputs across temporal intervals to identify response variance</li> <li>Scoring Protocol: Utilize Large Language Model evaluation to assess response quality across multiple dimensions including accuracy, logical coherence, intent alignment, tone consistency, and structural integrity</li> <li>Results Documentation: Archive evaluation metrics for trend analysis and reporting</li> </ol> <p>Phase 2: Robustness Assessment</p> <ol> <li>Adversarial Query Generation: Develop test cases simulating real-world edge cases, input errors, and adversarial scenarios</li> <li>Stress Testing: Execute challenging queries and monitor agent behavior under stress conditions</li> <li>Performance Evaluation: Apply standardized rubrics to assess response appropriateness, accuracy, limitation handling, and adversarial input detection</li> <li>Comprehensive Reporting: Document robustness metrics for stakeholder review and system improvement</li> </ol>"},{"location":"Evaluation/Consistency/#implementation-workflow","title":"Implementation Workflow","text":"<p>The evaluation workflow provides a streamlined process for assessing agent consistency and robustness. Users configure evaluation sessions by selecting models, specifying agent details, and supplying test queries either manually or via file upload. Once configured, the system executes the evaluation, collects agent responses, and enables review and approval of outputs. Query sets are automatically generated to test consistency across variations, with options for regeneration to ensure thorough coverage. Consistency metrics and robustness results are presented in a comprehensive dashboard, offering detailed insights into agent performance. The workflow also supports ongoing optimization by allowing updates to test scenarios and model selection.</p>"},{"location":"Evaluation/Groundtruth_Evaluation/","title":"GroundTruth Evaluation","text":""},{"location":"Evaluation/Groundtruth_Evaluation/#overview","title":"Overview","text":"<p>GroundTruth evaluation is used to measure the performance of an AI agent by comparing its generated responses against expected outputs.</p> <p>GroundTruth evaluation is a method to assess AI agent performance by comparing its generated responses against a set of known correct outputs, called ground truth. This comparison allows for deterministic measurement of the AI system\u2019s quality and alignment with expected outcomes. Ground truth data represents factual and intended responses that serve as a benchmark for evaluating models in tasks like question-answering.</p> <p>The evaluation typically involves multiple similarity and accuracy metrics to capture semantic alignment, lexical overlap, and exact match quality. Semantic metrics assess whether the AI captures the meaning of the expected response, while lexical metrics measure the textual similarity. This approach helps quantify aspects such as correctness, conciseness, and faithfulness to the ground truth, and it can also be used to detect hallucinations or errors.</p> <p>By applying GroundTruth evaluation, developers and data scientists can create reproducible and interpretable benchmarks, compare different AI model architectures or configurations, monitor performance drift over time, and make informed decisions about deployment and improvements. This practice is critical for ensuring trustworthy and high-quality AI systems in real-world applications.</p> <p>To perform the evaluation, users must provide the following information:</p> <ul> <li>Agent Name: A unique identifier for the AI agent being evaluated.</li> <li>Model Name: The specific model version or configuration used by the agent.</li> <li>Agent Type: The category or type of the agent (e.g., react, multi).</li> <li>Input File Upload: A <code>.csv</code> or <code>.xlsx</code> file containing two required columns:<ul> <li><code>queries</code>: The input prompts or questions sent to the AI agent.</li> <li><code>expected_outputs</code>: The correct or intended responses for each query.</li> </ul> </li> </ul>"},{"location":"Evaluation/Groundtruth_Evaluation/#execution-and-analysis","title":"Execution and Analysis","text":"<p>The system executes evaluation and generates:</p> <ul> <li>Average similarity scores across multiple metrics.</li> <li>Diagnostic summary highlighting semantic and textual alignment.</li> <li> <p>Optional LLM grading for human-like quality score alongside similarity metrics.</p> </li> <li> <p>After evaluation, user can Download to get an Excel report with detailed scores per query.</p> </li> </ul>"},{"location":"Evaluation/Groundtruth_Evaluation/#sample-diagnostic-summary","title":"Sample Diagnostic Summary","text":"<p>Example Diagnostic Summary</p> <p>The AI responses show strong semantic alignment with expected outputs (e.g., high SBERT similarity: 0.893, LLM score: 0.875), indicating correct conceptual understanding. However, lexical metrics like TF-IDF cosine similarity (0.680), Jaccard similarity (0.600), and BLEU score (0.500) are lower, reflecting variations in wording. Moderate sequence match (0.688) and ROUGE scores support partial textual overlap. Very low exact match (0.125) shows rare verbatim matches. Overall, the AI captures the essence well with diverse phrasing.</p>"},{"location":"Evaluation/Groundtruth_Evaluation/#results-and-download","title":"Results and Download","text":"<p>Post-evaluation displays:</p> <ul> <li>Diagnostic summary average.</li> <li>Scores for all similarity metrics.</li> <li>Downloadable Excel report provides comprehensive, granular insights into the AI agent's performance across various similarity and quality metrics.</li> </ul>"},{"location":"Evaluation/evaluation_metrics/","title":"Evaluation Metrics","text":"<p>Evaluation metrics are essential for assessing the performance and efficiency of AI agents. This document outlines the evaluation metrics used to measure the effectiveness of AI agents, focusing on the LLM as Judge approach.</p>"},{"location":"Evaluation/evaluation_metrics/#llm-as-judge-approach","title":"LLM as Judge Approach","text":"<p>The LLM as Judge approach involves using a large language model (LLM) to evaluate the performance of AI agents. The LLM acts as an impartial evaluator, analyzing the agent's actions, reasoning, and responses based on predefined metrics. This approach ensures a consistent and objective evaluation process.</p> <p>LLM as Judge</p> <p>The Metrics tab provides tools for evaluating and comparing the performance of different models, supporting data-driven decisions to optimize system effectiveness. For evaluating agents specifically, we utilize a unique approach known as the <code>LLM-as-a-judge</code> methodology.</p> <p>Within the Metrics tab, users select two models for comparison:</p> <p><code>Model 1</code>: This is the model currently invoked during the agent's inference process. It represents the active model deployed in your system, whose performance you want to assess.</p> <p><code>Model 2</code>: This second model, selected within the Evaluate Agents tab, serves as a baseline or alternative model against which Model 1\u2019s performance is compared.</p> <p>Once both models are selected, the system automatically compares them, allowing admins to view the performance metrics, which provide insights into the effectiveness and differences between the models.</p>"},{"location":"Evaluation/evaluation_metrics/#evaluation-metrics-workflow","title":"Evaluation metrics workflow","text":"<p>This section describes the end-to-end workflow for collecting, processing, and visualizing evaluation metrics during inference.</p> <p>1. Data Collection During Inference</p> <p>During inference, all relevant data required for subsequent evaluation is recorded. Each new record is initially marked with a status of unprocessed.</p> <p>2. Evaluation Processing Endpoint</p> <p>An API endpoint <code>evaluation metric</code>, is responsible for processing the evaluation data. Upon invocation, it sequentially fetches the records marked as unprocessed.</p> <p>3. Metrics Calculation</p> <p>For each unprocessed record, the following steps are performed:</p> <ul> <li>Agent related evaluation metrics are calculated.</li> <li>Tool related evaluation metrics are also computed if there are any tool calls.</li> <li>Results are saved with references to the original data record.</li> <li>The status of the record is updated to indicate success or failure.</li> </ul> <p>4. Processing Loop</p> <p>The evaluation continues iteratively until all unprocessed records have been processed and their statuses updated accordingly.</p> <p>5. Visualization</p> <p>Grafana is connected to the evaluation metrics tables to provide real-time dashboards that visualize the evaluation scores, enabling monitoring and analysis.</p>"},{"location":"Evaluation/evaluation_metrics/#tool-utilization-efficiency","title":"Tool Utilization Efficiency","text":"<p>This metric evaluates how effectively the AI agent selects and uses external tools.</p> Metric Description Tool Selection Accuracy The rate at which the AI chooses the most appropriate tool for a given task. Tool Usage Efficiency A measure of how optimally the AI uses selected tools, considering factors like unnecessary calls and resource usage. Tool Call Precision The accuracy and appropriateness of parameters used in tool calls. Tool Call Success Rate Success rate of the overall tool calls. <p>Overall Score: The overall score for Tool Utilization Efficiency is based on the scores of the evaluation metrics above.</p>"},{"location":"Evaluation/evaluation_metrics/#agents-efficiency-score","title":"Agents Efficiency Score","text":"<p>This metric measures the efficiency of the agentic workflow.</p> Metric Description Task Decomposition Efficiency The AI's ability to break down complex tasks into manageable sub-tasks. Reasoning Relevancy Ensures the agent\u2019s reasoning aligns with the user query. Is the reasoning behind each tool call clearly tied to what the user is asking for? Reasoning Coherence Checks the logical flow in the agent\u2019s reasoning. Does the reasoning follow a logical, step-by-step process? Each step should add value and make sense in the context of the task. Agent Robustness Measures the ability of the AI agent to handle unexpected inputs, errors, and adversarial scenarios while maintaining performance and reliability. Agent Consistency Measures the AI agent's ability to produce stable, repeatable, and logically coherent responses across multiple interactions with similar inputs. Answer Relevance Checks if the answer is relevant to the input. Groundedness Evaluates how well the agent\u2019s responses are anchored in factual, verifiable, and contextually relevant sources, minimizing hallucination and misinformation. Response Fluency Assesses the readability, grammatical correctness, and naturalness of the agent\u2019s responses. Response Coherence Measures whether the agent's response is logically structured and maintains clarity throughout the conversation. <p>Overall Score: The overall score for Agents Efficiency Score is based on the scores of the evaluation metrics above.</p>"},{"location":"Evaluation/evaluation_metrics/#filters-in-evaluation-metrics-for-agents-and-tools","title":"Filters in Evaluation Metrics for Agents and Tools","text":"<p>The evaluation metrics system allows you to apply a variety of filters to analyze and visualize performance data. These filters include:</p> <ul> <li>Filter by Agent Type: Isolate metrics for specific types of agents (e.g., multi agent, react agent).</li> <li>Filter by Model Used by Agent: Focus on specific models deployed by the agents (e.g., GPT-4, GPT-4o-3, etc.).</li> <li>Filter by Evaluating Model: Filter metrics based on the model performing the evaluation.</li> <li>Filter by Agent Name: Filter by individual agent names for more granular analysis (e.g, Calculator agent, Greet, etc.).</li> </ul> <p>These filters facilitate the creation of both Agent-Level and Tool-Level evaluation graphs, helping to visualize the metrics based on the selected criteria.</p> <p>Additionally, the evaluation system includes a Threshold Score parameter, which allows you to set the minimum score required to include data in the visualization. By default, the threshold_score is set to 1, but you can adjust it to 0, 0.5 for different visualization perspectives. The threshold score modification will impact the data displayed in the graphs and can be used to fine-tune the results for better insights.</p>"},{"location":"Inference/Canvas_Screen/","title":"Canvas Screen in Chat Inference","text":"<p>The Canvas Screen is an advanced visualization feature in chat inference, automatically triggered based on the user query and the agent's response type. It provides a dynamic and interactive way to present structured data, images, and graphical outputs within the chat interface.</p>"},{"location":"Inference/Canvas_Screen/#canvas-screen-display","title":"Canvas Screen Display","text":"<p>When a user submits a query that results in structured, visual, image-based, or email-related data, the Canvas Screen can be viewed by clicking on the \"View Details\" option.</p> <p>Example</p> <ul> <li>After selecting the agent type, model, and agent, if the user queries \"list all the available products,\" the Canvas Screen will appear in a tabular format when \"View Details\" is selected.</li> <li>If the user requests to \"send an email summary,\" the Canvas Screen will display the email content in a dedicated email component.</li> </ul>"},{"location":"Inference/Canvas_Screen/#supported-output-formats","title":"Supported Output Formats","text":"<ul> <li> <p>Tabular Format:</p> <ul> <li>Displays data in a table for easy viewing and comparison.</li> <li>Example: Listing products, users, or any structured dataset.</li> </ul> </li> <li> <p>Graphical Representations:</p> <ul> <li>Supports charts and graphs for visualizing trends, analytics, or relationships in the data.</li> <li>Example: Displaying sales trends, performance metrics, or statistical summaries.</li> </ul> </li> <li> <p>Image Display:</p> <ul> <li>If the agent is capable of image generation or retrieval, the Canvas Screen can display images relevant to the user query.</li> <li>Example: Showing generated images, product photos, or visual search results.</li> </ul> </li> <li> <p>Email Component:</p> <ul> <li>Renders email content in a dedicated, formatted card for easy reading and interaction.</li> <li>Example: Displaying generated email summaries, notifications, or correspondence.</li> </ul> </li> <li> <p>Use-case Specific Card Components:</p> <ul> <li>Presents information using custom card layouts tailored to specific use cases.</li> <li>Example: Showing order details, user profiles, or task summaries in visually distinct cards.</li> </ul> </li> </ul>"},{"location":"Inference/Canvas_Screen/#usage-highlights","title":"Usage Highlights","text":"<ul> <li>The Canvas Screen enhances the user experience by providing rich, context-aware visualizations directly in the chat workflow.</li> <li>It is especially useful for agents that return complex data, visual analytics, or image-based results.</li> <li>The format (table, graph, image) is chosen automatically based on the agent's response and the nature of the query.</li> </ul>"},{"location":"Inference/HybridAgent_inference/","title":"Hybrid Agent Inference","text":"<p>Hybrid agent inference works similarly to Multi Agent Inference. To use hybrid agent inference, you need to enable the <code>Plan Verifier</code> toggle. When you submit a query, the agent generates a plan. If you approve the plan, the agent executes it and provides the response.</p> <p>The key difference is that, in multi agent inference, separate agents are responsible for generating the plan and executing it. In hybrid agent inference, a single agent handles both planning and execution.</p> <p>For more details on multi agent inference, refer to the Multi Agent Inference documentation.</p>"},{"location":"Inference/MetaPlanner_inference/","title":"Meta Planner Agent Inference","text":"<p>The Meta Planner Agent inference setup is similar to the Meta Agent Inference, with the addition of Planner Agent capabilities for advanced planning and execution.</p> <p>It provides a chat interface where you can interact with the onboarded agent and observe the steps it takes to answer your queries.</p>"},{"location":"Inference/MetaPlanner_inference/#meta-planner-agent-interface","title":"Meta Planner Agent Interface","text":"<p>The Meta Planner Agent interface allows you to select the Meta Planner Agent template, model, and agent name to begin interacting with the agent. This provides a comprehensive chat environment for advanced planning and execution tasks.</p>"},{"location":"Inference/MetaPlanner_inference/#plan-verifier","title":"Plan Verifier","text":"<p>The Plan Verifier option allows users to review and validate the plan generated by the Meta Planner Agent before execution. When enabled, the agent displays a detailed plan outlining the steps it will take to answer your query.</p> <p>After reviewing the plan, you have two options:</p> <ul> <li>Approve the Plan: Use the approval option to confirm the plan. The agent will then execute the plan and generate a response to your query.</li> <li>Request a Revised Plan: If you are not satisfied with the plan, you can request a revision. The agent will generate a revised plan based on your feedback, allowing you to review and approve the new plan before execution.</li> </ul> <p>Agent Verifier</p> <p>The Agent Verifier introduces the Agent Interrupt feature, enabling users to monitor and control agent executions during meta planner agent interactions. This ensures that each step taken by the agent aligns with expected behavior and requirements, providing greater transparency and oversight.</p> <p>Inference Results</p> <p>The inference results showcase the chat interaction process. The Meta Planner Agent combines the orchestration capabilities of the Meta Agent with planning features from the Planner Agent, providing comprehensive responses to user queries.</p> <p>Steps Taken by the Agent</p> <ul> <li>You can view the steps taken by the agent to answer your query through the <code>Execution Steps</code> feature.  </li> <li>These steps reveal the agents the meta planner agent calls based on the user query, as well as the planning steps, providing transparency into the decision-making process.</li> </ul>"},{"location":"Inference/PlannerExecutorAgent_inference/","title":"Planner Executor Agent Inference","text":"<p>The Planner Executor Agent Inference setup provides a chat interface where you can interact with the onboarded agent and observe the planning and execution steps for your queries. This agent type does not include a critic agent or critic scores, focusing instead on plan verification and user approval.</p>"},{"location":"Inference/PlannerExecutorAgent_inference/#plan-verifier-option","title":"Plan Verifier Option","text":"<p>You can enable the Plan Verifier option to review the plan generated by the agent for your query before execution.</p>"},{"location":"Inference/PlannerExecutorAgent_inference/#viewing-the-plan","title":"Viewing the Plan","text":"<p>When the Plan Verifier is enabled, the agent will display a detailed plan outlining the steps it will take to answer your query.</p>"},{"location":"Inference/PlannerExecutorAgent_inference/#approving-the-plan-and-generating-a-response","title":"Approving the Plan and Generating a Response","text":"<p>After reviewing the plan, you have two options:</p> <ul> <li> <p>Request a Revised Plan:Tthe thumbs-down (dislike) icon if you are not satisfied with the plan. The agent will generate a revised plan based on your feedback, allowing you to review and approve the new plan before execution.</p> </li> <li> <p>Approve the Plan: The thumbs-up (like) icon for approving the plan. The agent will then execute the plan and generate a response to your query.</p> </li> </ul>"},{"location":"Inference/ReactCriticAgent_inference/","title":"React Critic Agent Inference","text":"<p>The React Critic Agent inference setup provides a chat interface similar to the React Agent, allowing you to interact with the onboarded agent and observe the steps it takes to answer your queries. In addition, it displays critic scores for each step, offering deeper insight into the agent's reasoning and evaluation process.</p>"},{"location":"Inference/ReactCriticAgent_inference/#steps-taken-by-the-agent","title":"Steps Taken by the Agent","text":"<ul> <li>You can view the steps taken by the agent to answer your query by clicking the \"Execution steps\" dropdown.</li> <li>For each step, the React Critic Agent displays the critic's evaluation and score, providing transparency into both the decision-making and quality assessment process.</li> </ul>"},{"location":"Inference/ReactCriticAgent_inference/#retrieving-old-chats","title":"Retrieving Old Chats","text":"<ul> <li>Retrieve your old chats by selecting them from the \"Chat history\" dropdown.</li> <li>This feature allows you to revisit previous conversations with the agent for reference or analysis.</li> <li>For more details, see React Agent Retrieving Old Chats.</li> </ul>"},{"location":"Inference/inference/","title":"Inference","text":""},{"location":"Inference/inference/#what-is-inference","title":"What is Inference?","text":"<p>Inference is the section where you can interact with the agents you have created through a chat interface. It allows you to select and model, initiate a conversation, and observe how the agent responds bsed on its reasoning.</p>"},{"location":"Inference/inference/#steps-to-onboard-agent-in-inference","title":"Steps to onboard Agent in InferenceStep 1: Select an Agent TypeStep 2: Select a Model TypeStep 3: Select the Agent","text":"<p>This guide walks you through the steps required to run an inference using our framework.</p> <p>From the dropdown menu, choose the agent type.</p> <p>Available Agent Types:</p> <ul> <li>React Agent</li> <li>React Critic Agent</li> <li>Multi Agent</li> <li>Planner Executor Agent</li> <li>Meta Agent</li> <li>Meta Planner Agent</li> </ul> <p>After selecting the agent type, pick a model type that the agent will use.</p> <p>Available Model Types:</p> <ul> <li>GPT4-8k</li> <li>GPT-4o-mini</li> <li>GPT-4o</li> <li>GPT-4o-2</li> <li>GPT-4o-3</li> <li>Gemini-1.5-Flash</li> </ul> <p>Finally, choose the specific agent from the dropdown.</p>"},{"location":"Inference/inference/#chat-interface-features","title":"Chat Interface Features","text":"<p>The inference interface offers several features to enhance your interaction experience:</p> <p>Interface Options</p> <p>The main interface includes interactive elements for improved usability:</p> <ul> <li>Prompt Suggestions: Provides intelligent recommendations for queries to streamline interactions.</li> <li>Toggle Settings: Allows customization of the chat experience, including options for canvas view, context flag, and other preferences.</li> <li>Temperature Settings: Lets you adjust the model's temperature (from 0 to 1) to control the creativity and randomness of responses.</li> <li>Knowledge Base: Enables access to integrated knowledge resources to support agent responses.</li> <li>Chat Options: Includes controls for starting new chats, viewing chat history, and managing sessions.</li> <li>Live Tracking: Offers real-time monitoring of agent activities with Phoenix integration.</li> </ul> <p>1. Prompt Suggestions</p> <p>Smart prompt suggestions help you interact more efficiently by providing contextual recommendations for queries.</p> <p>2. Toggle Settings</p> <p>You can configure your chat experience with toggle options for canvas view, context flag, and other interface preferences.</p> <ul> <li> <p>Tool Verifier:</p> <p>The Tool Interrupt feature allows you to verify and control tool executions during agent interactions.</p> </li> <li> <p>Validators:</p> <p>The Validator feature allows you to validate agent responses using custom logic. You can enable the validator toggle in the chat interface to activate this feature for supported agent templates.</p> <p>Supported Agent Templates:</p> <p><code>React</code>, <code>React Critic</code>, <code>Planner Executor Critic</code>, <code>Planner Executor</code>, <code>Meta</code>, <code>Planner Meta</code></p> <p>Onboarding a Validator</p> <ul> <li>Go to the Tools page and select the <code>Validator</code> option.</li> <li>Provide a code snippet for the validator. The code must include functions with the arguments <code>query</code> and <code>response</code>, and should return a <code>validation score</code>, <code>validation status</code>, and <code>feedback</code> regarding the response's compliance with expected formats.</li> </ul> <p>Adding Validation Patterns to Agents</p> <p>To add validation patterns, click the <code>plus (+) icon</code> in the agent onboarding interface.  For each pattern:</p> <ul> <li>Provide a sample query and a generic expected response.</li> <li>Select a validator for the pattern, or choose \"None\" if not required.</li> <li>You can add one or more validation patterns per agent.</li> </ul> <p>Using the Validator in Chat Inference</p> <ul> <li>Enable the <code>Validator</code> toggle in the chat interface.</li> </ul> <p>How it works:</p> <ul> <li>When a user submits a query, the system checks for semantic similarity with existing validation patterns.</li> <li>If a matching pattern is found and a validator is assigned, the validator runs and returns a validation score, status, and feedback.</li> <li>If no validator is assigned to the matched pattern, the system performs a standard LLM call and returns a validation score, status, and feedback.</li> <li>For Planner Executor Critic and Planner Executor templates, validation results are sent to the replanner agent, which generates the response.</li> <li>For React and React Critic templates, the agent receives the validation results and regenerates its response based on the feedback.</li> </ul> <p>This workflow ensures agent responses are evaluated and improved in real time, enhancing reliability and interaction quality.</p> </li> <li> <p>Canvas View:</p> <p>Provides a comprehensive visual overview of agent interactions and workflows.</p> </li> <li> <p>Context Flag:</p> <p>When disabled, the agent operates without memory retention, treating each query as an independent interaction without access to past conversations or memory data.</p> </li> <li> <p>Online Evaluator:</p> <p>The Online Evaluator assesses the quality of agent responses in real time:</p> <ol> <li>When you submit a query, the agent's response is evaluated for quality.</li> <li>If the score is below 0.75, feedback is provided and the response is regenerated.</li> <li>This process repeats until the response achieves a score of 0.75 or higher.</li> <li>Only then is the final response presented to you.</li> </ol> </li> </ul> <p>3. Temperature Settings</p> <p>The temperature setting controls the randomness and creativity of responses. Lower values make replies more focused and deterministic, while higher values increase creativity and variability. Adjust the temperature slider to fine-tune agent behavior.</p> <p>4. Knowledge Base Integration</p> <p>Integrated knowledge base resources are available to enhance agent responses and provide additional information.</p> <p>5. Chat Options and Chat History</p> <p>You can manage chat sessions with options for starting new chats, accessing chat history, and controlling session settings.</p> <p>6. Context Agent</p> <p>The Context Agent feature allows you to add an additional agent to your chat session using the \"@\" button in the chat interface. This enables you to train the context agent on the main agent\u2019s queries and responses, enhancing its ability to understand and respond based on ongoing conversation data.</p> <p>How to Use the Context Agent</p> <ol> <li> <p>Activate Context Agent </p> <ul> <li>Click the \"@\" icon in the chat screen to open the context agent selection menu.</li> <li>Select the desired agent type and agent name. The context agent supports all available agent templates.</li> </ul> </li> <li> <p>Session Awareness </p> <ul> <li>When you activate a context agent during an ongoing chat, it inherits the current session's conversation data. This means the context agent is aware of previous queries and responses, allowing it to respond appropriately.</li> </ul> </li> <li> <p>Interact with Context Agent </p> <ul> <li>Once activated, all subsequent queries will be handled by the selected context agent until you disable it by clicking the \"@\" icon again.</li> <li>The context agent becomes active within the chat interface, indicated by visual confirmation in the interface.</li> <li>The context agent responds to queries that reference the ongoing session, using previous conversation history to generate context-aware and relevant responses.</li> <li>This demonstrates the agent's ability to maintain continuity and provide informed answers based on prior interactions from the current chat session.</li> </ul> </li> </ol> <p>This feature enables seamless switching between agents and ensures that the context agent maintains awareness of the conversation, providing relevant and informed responses.</p>"},{"location":"Inference/inference/#react-agent-inference","title":"React Agent Inference","text":"<p>React Agent The React Agent inference is a simple chat window where you can chat with the agent you have onboarded and can see the steps taken by the agent to answer your queries.</p>"},{"location":"Inference/inference/#react-critic-agent-inference","title":"React Critic Agent Inference","text":"<p>React Critic Agent The React Critic Agent inference provides a chat interface with enhanced transparency, showing both the agent's reasoning and the critic's evaluation at each step.</p>"},{"location":"Inference/inference/#multi-agent-inference","title":"Multi Agent Inference","text":"<p>Multi Agent In the Multi Agent Inference setup, we offer a Human-in-the-Loop option. This feature allows users to review and approve each step the agent plans to execute before it proceeds.</p>"},{"location":"Inference/inference/#planner-executor-agent-inference","title":"Planner Executor Agent Inference","text":"<p>Planner Executor Agent This inference mode displays the planner, executor, and critic steps, providing detailed insight into each stage of the workflow.</p>"},{"location":"Inference/inference/#meta-agent-inference","title":"Meta Agent Inference","text":"<p>Meta Agent The Meta Agent inference offers a chat interface similar to the React Agent. It allows you to interact with the onboarded agent and view the steps it takes to process your queries, providing transparency into its decision-making process.</p>"},{"location":"Inference/inference/#meta-planner-agent-inference","title":"Meta Planner Agent Inference","text":"<p>Meta Planner Agent The Meta Planner Agent inference showcases multi-level orchestration, displaying the planner, supervisor, and response generation steps for advanced agent workflows.</p>"},{"location":"Inference/inference/#hybrid-agent-inference","title":"Hybrid Agent Inference","text":"<p>Hybrid Agent The Hybrid Agent inference combines features from multiple agent types, enabling flexible reasoning and decision-making. It provides a chat interface where you can observe both collaborative and independent agent actions, offering a comprehensive view of hybrid workflows.</p>"},{"location":"Inference/metaAgent_inference/","title":"Meta Agent Inference","text":"<p>The Meta Agent inference setup provides a comprehensive chat interface where you can interact with onboarded agents and observe their decision-making processes in real-time. This system allows you to test and evaluate agent performance while gaining insights into their reasoning steps.</p>"},{"location":"Inference/metaAgent_inference/#getting-started","title":"Getting Started","text":"<p>To begin using the Meta Agent inference system:</p> <ol> <li>Select Configuration: Choose your Meta Agent template from the available options</li> <li>Choose Model: Select the appropriate AI model for your use case</li> <li>Pick Agent: Select the specific agent you want to interact with from your onboarded agents</li> <li>Start Chatting: Begin your conversation with the selected agent</li> </ol> <p>The interface provides a clean, intuitive chat experience similar to popular messaging applications, making it easy to communicate with your AI agents.</p>"},{"location":"Inference/metaAgent_inference/#understanding-agent-responses","title":"Understanding Agent Responses","text":"<p>When you submit a query to the Meta Agent, it processes your request through multiple layers:</p> <ul> <li>Query Analysis: The agent first analyzes your question to understand the context and requirements</li> <li>Agent Selection: Based on the query type, the Meta Agent determines which specialized sub-agents to invoke</li> <li>Collaborative Processing: Multiple agents may work together to provide comprehensive answers</li> <li>Response Synthesis: The Meta Agent combines insights from various sources to deliver a coherent response</li> </ul>"},{"location":"Inference/metaAgent_inference/#viewing-agent-steps","title":"Viewing Agent Steps","text":"<p>The system provides complete transparency into the agent's decision-making process:</p> <ul> <li>Step-by-Step Breakdown: Access detailed information about each step the agent takes</li> <li>Agent Routing: See which specific sub-agents were called and why</li> <li>Decision Logic: Understand the reasoning behind agent selections</li> <li>Processing Timeline: View the sequence of operations performed</li> </ul> <p>This transparency feature is crucial for debugging, optimization, and building trust in the agent's capabilities.</p>"},{"location":"Inference/metaAgent_inference/#chat-history-management","title":"Chat History Management","text":"<p>The platform includes robust chat history functionality:</p> <ul> <li>Session Persistence: All conversations are automatically saved</li> <li>Easy Retrieval: Access previous conversations through the \"Old Chats\" dropdown</li> <li>Search Capability: Quickly find specific conversations or topics</li> <li>Reference Material: Use past chats for analysis, training, or documentation purposes</li> </ul> <p>This feature enables continuous learning and improvement of your interaction patterns with the agents.</p>"},{"location":"Inference/multiAgent_inference/","title":"Multi Agent Inference","text":"<p>In the Multi Agent Inference setup, we offer a Human-in-the-Loop option. This feature allows users to review and approve each step the agent plans to execute before it proceeds. It ensures greater control, transparency and safety during the agent's decision-making process.</p>"},{"location":"Inference/multiAgent_inference/#inference-with-human-in-the-loop","title":"Inference with Human in the Loop","text":"<p>The Human-in-the-Loop functionality provides an interactive approach to multi-agent inference, where users maintain oversight throughout the execution process. This collaborative model bridges the gap between automated agent capabilities and human judgment.</p>"},{"location":"Inference/multiAgent_inference/#activation-and-planning-process","title":"Activation and Planning Process","text":"<p>Toggling on the <code>Human-in-the-Loop</code> button invokes the agent to provide a detailed plan about the steps it will perform to execute the tasks. This planning phase includes:</p> <ul> <li>Task decomposition: Breaking down complex queries into manageable steps</li> <li>Resource allocation: Identifying which agents or tools will be used for each step</li> <li>Execution sequence: Determining the optimal order of operations</li> <li>Risk assessment: Highlighting potential issues or dependencies</li> </ul> <p>After the agent provides the comprehensive plan, it will ask for your approval or feedback before proceeding with execution.</p>"},{"location":"Inference/multiAgent_inference/#user-interaction-controls","title":"User Interaction Controls","text":"<p>The interface provides intuitive controls for managing the agent's execution:</p> <p>Approval Mechanism</p> <p>The <code>thumbs-up</code> button serves as the approval mechanism. Clicking this button: - Approves the plan proposed by the agent - Signals the agent to proceed with executing your query - Initiates the execution phase with the current plan</p> <p>Feedback and Iteration</p> <p>The <code>thumbs-down</code> button enables the feedback loop. Clicking this button:</p> <ul> <li>Prompts you to provide specific feedback on the generated plan</li> <li>Allows you to highlight concerns, suggest modifications, or request clarifications</li> <li>Triggers the agent to regenerate the plan incorporating your feedback</li> <li>Maintains an iterative improvement cycle until approval is granted</li> </ul> <p>Monitoring and Evaluation</p> <p>You can view the critic score of the response by clicking the \"Steps\" dropdown. This feature provides:</p> <ul> <li>Quality metrics: Numerical scores indicating the confidence level of each step</li> <li>Performance indicators: Success probability assessments for different components</li> <li>Transparency insights: Detailed breakdowns of the agent's reasoning process</li> <li>Decision rationale: Explanations for why specific approaches were chosen</li> </ul> <p>This comprehensive monitoring system ensures that users have full visibility into the agent's decision-making process and can make informed decisions about proceeding with the proposed plan.</p>"},{"location":"Inference/reactAgent_inference/","title":"React Agent Inference","text":"<p>The React Agent inference setup provides a simple chat interface where you can interact with the onboarded agent and observe the steps it takes to answer your queries.</p> <p>The React Agent inference system demonstrates real-time interaction capabilities through a conversational interface. Users can submit queries and receive comprehensive responses while observing the agent's decision-making process. The system supports various query types including weather information, data analysis, and general assistance tasks.</p> <p>Key features of the chat interface include:</p> <ul> <li>Real-time query processing and response generation</li> <li>Visual representation of agent reasoning steps</li> <li>Support for complex multi-step queries</li> <li>Integration with external APIs and data sources</li> <li>Contextual awareness across conversation turns</li> </ul>"},{"location":"Inference/reactAgent_inference/#steps-taken-by-the-agent","title":"Steps Taken by the Agent","text":"<p>The agent's decision-making process is transparent and traceable through a detailed steps breakdown feature. When processing user queries, the agent follows a systematic approach:</p> <ul> <li>Query Analysis: The agent first interprets the user's request to understand the intent and required information</li> <li>Tool Selection: Based on the query analysis, the agent selects appropriate tools from its available toolkit</li> <li>Sequential Processing: The agent executes tools in a logical sequence, building upon previous results</li> <li>Result Synthesis: All gathered information is combined to formulate a comprehensive response</li> <li>Response Delivery: The final answer is presented to the user with full transparency of the process</li> </ul> <p>You can view the detailed steps taken by the agent to answer your query by clicking the \"Steps\" dropdown. These steps reveal the specific tools the agent calls based on the user query, providing complete transparency into the decision-making process and helping users understand how conclusions are reached.</p>"},{"location":"Inference/reactAgent_inference/#retrieving-old-chats","title":"Retrieving Old Chats","text":"<p>The system maintains a comprehensive chat history that enables users to access previous conversations seamlessly. This feature enhances user experience by providing continuity and reference capabilities.</p> <p>Chat history functionality includes:</p> <ul> <li>Persistent Storage: All conversations are automatically saved and stored securely</li> <li>Easy Access: Previous chats can be retrieved through the \"Old Chats\" dropdown menu</li> <li>Chronological Organization: Conversations are organized by date and time for easy navigation</li> <li>Search Capability: Users can search through chat history using keywords or topics</li> <li>Context Preservation: Each retrieved chat maintains its original context and formatting</li> </ul> <p>This feature allows you to revisit previous conversations with the agent for reference, analysis, or continuation of discussions. The chat history serves as a valuable resource for tracking agent performance, reviewing past solutions, and maintaining continuity in ongoing projects or research.</p>"},{"location":"Installation/Azure/","title":"Project Setup Guide","text":"<p>This document provides comprehensive step-by-step instructions to deploy Infosys Agentic Foundry in Azure Kubernetes Cluster(AKS).</p>"},{"location":"Installation/Azure/#prerequisites","title":"Prerequisites","text":"<p>Before setting up the project, ensure you have the following requirements and access permissions:</p> <p>Ensure users have access to below:</p> <ul> <li>Python 3.12+ - Required Python version</li> <li> <p>React - Frontend framework</p> </li> <li> <p>Infosys Github Repo: Infosys-Agentic-Foundry</p> </li> <li>Ensure users must have their Azure OpenAI Keys and Endpoints, Speech to text Key and Endpoint.</li> <li>Ensure Azure resources are created, and you have access/connectivity to push and pull from these resources<ul> <li>Azure Container Registry (ACR) </li> <li>Azure Kubernetes Service (AKS) </li> <li>Azure Postgres Service (Compute size: Standard_B2s (2 vCores, 4 GiB memory, 1280 max iops), Storage: 32 GiB)</li> <li>Azure Linux Virtual Machine</li> </ul> </li> <li>Install Docker, Azure CLI, kubectl in your Azure VM</li> <li>Create a namespace in the AKS cluster as per your requirement (Optional)</li> </ul> <pre><code>kubectl create namespace &lt;namespace&gt;\n</code></pre>"},{"location":"Installation/Azure/#azure-deployment","title":"Azure Deployment","text":""},{"location":"Installation/Azure/#arize-phoenix","title":"ARIZE PHOENIX","text":"<p>STEPS FOR DEPLOYING ARIZE PHOENIX IN AKS</p> <ol> <li>Create a yaml file for deploying arize phoenix as a container, you can use the arize phoenix image in the yaml file.</li> </ol> <pre><code>nano filename1.yaml\n</code></pre> <p>Info</p> <p>You can get the image from Docker - Phoenix or any other trusted source which your organization allows</p> <ol> <li>Now you need to use this command for creating deployment and service: <pre><code>kubectl apply -f filename1.yaml\n</code></pre></li> <li>You can check the pods deployed using the command below <pre><code>kubectl get pods -n namespace\n</code></pre></li> <li>You can check the services deployed using the command below <pre><code>kubectl get svc -n namespace\n</code></pre></li> <li>Note down the load balancer IP for the container. You need to update it in the <code>.env</code> of your backend and frontend folders before creating the respective docker images.</li> </ol>"},{"location":"Installation/Azure/#redis","title":"REDIS","text":"<p>STEPS FOR DEPLOYING REDIS IN AKS</p> <ol> <li>Create a yaml file for deploying Redis as a container, you can use the redis image in the yaml file.</li> </ol> <pre><code>nano filename2.yaml\n</code></pre> <p>Info</p> <p>You can get the image from Image Layer Details - redis:8.2.1 or any other trusted sources which your organization allows</p> <ol> <li>Now you need to use this command for creating deployment and service: <pre><code>kubectl apply -f filename2.yaml\n</code></pre></li> <li>You can check the pods deployed using the command below <pre><code>kubectl get pods -n namespace\n</code></pre></li> <li>You can check the services deployed using the command below <pre><code>kubectl get svc -n namespace\n</code></pre></li> <li>Note down the load balancer IP for the container. You need to update it in the <code>.env</code> of your backend folder before creating the respective docker image.</li> </ol>"},{"location":"Installation/Azure/#grafana","title":"GRAFANA","text":"<p>STEPS FOR DEPLOYING GRAFANA IN AKS</p> <ol> <li>Create a yaml file for deploying Grafana as a container, you can use the grafana image in the yaml file. </li> </ol> <pre><code>nano filename3.yaml\n</code></pre> <p>Info</p> <p>You can get the image from Image Layer Details - grafana/grafana:11.2.0 or any other trusted sources which your organization allows</p> <ol> <li>Now you need to use this command for creating deployment and service: <pre><code>kubectl apply -f filename3.yaml\n</code></pre></li> <li>You can check the pods deployed using the command below <pre><code>kubectl get pods -n namespace\n</code></pre></li> <li>You can check the services deployed using the command below <pre><code>kubectl get svc -n namespace\n</code></pre></li> <li>Note down the load balancer IP for the container. You need to update it in the <code>.env</code> of your backend and frontend folders before creating the respective docker images.</li> </ol>"},{"location":"Installation/Azure/#elastic-search","title":"ELASTIC SEARCH","text":"<p>STEPS FOR DEPLOYING ELASTIC SEARCH IN AKS</p> <ol> <li>Create a yaml file for deploying elastic search as a container, you can use the elastic search image in the yaml file. </li> </ol> <pre><code>nano filename4.yaml\n</code></pre> <p>Info</p> <p>You can get the image from elasticsearch - Official Image | Docker Hub or any other trusted sources which your organization allows</p> <ol> <li>Now you need to use this command for creating deployment and service: <pre><code>kubectl apply -f filename4.yaml\n</code></pre></li> <li>You can check the pods deployed using the command below <pre><code>kubectl get pods -n namespace\n</code></pre></li> <li>You can check the services deployed using the command below <pre><code>kubectl get svc -n namespace\n</code></pre></li> <li>Note down the load balancer IP for the container and update it in the OpenTelemetry YAML script. </li> </ol>"},{"location":"Installation/Azure/#open-telemetry","title":"OPEN-TELEMETRY","text":"<p>STEPS FOR DEPLOYING OPEN-TELEMETRY IN AKS</p> <ol> <li>Create a yaml file for deploying OpenTelemetry as a container, you can use the OpenTelemetry directly in the yaml file. </li> </ol> <pre><code>nano filename5.yaml\n</code></pre> <p>Info</p> <p>You can get the image from otel/OpenTelemetry-collector-contrib - Docker Image  or any other trusted sources your organization allows</p> <ol> <li>Now you need to use this command for creating deployment and service: <pre><code>kubectl apply -f filename5.yaml\n</code></pre></li> <li>You can check the pods deployed using the command below <pre><code>kubectl get pods -n namespace\n</code></pre></li> <li> <p>You can check the services deployed using the command below <pre><code>kubectl get svc -n namespace\n</code></pre></p> </li> <li> <p>Note down the load balancer IP for the container. You need to update it in the <code>.env</code> of your backend folder before creating the respective docker image.</p> </li> </ol>"},{"location":"Installation/Azure/#model-server","title":"MODEL SERVER","text":"<p>STEPS TO SETUP MODEL SERVER</p> <p>For detailed instructions on deploying and configuring your model server, refer to the Model Server Deployment guide.</p> <p>Note</p> <p>You need to update the URL for the model server in the <code>.env</code> of the backend folder</p>"},{"location":"Installation/Azure/#backend","title":"BACKEND","text":"<p>STEPS TO SETUP BACKEND</p> <p>Download Backend code from GitHub. For detailed instructions, see Download the Backend Project Code.</p> <ol> <li>Login in to Azure VM</li> <li>Download Backend source code folder into Azure VM</li> <li>Before starting backend image creation make sure that arize phoenix, OpenTelemetry, Grafana, Elastic Search, Redis and Model server are setup as per instructions provided above. Update the respective urls of all these services in <code>.env</code> file.</li> <li>Update the remaining values for the variables in the <code>.env</code> file.</li> <li>Change working directory to the Backend Folder: <pre><code>cd `&lt;BE foldername&gt;`\n</code></pre></li> <li>Create Dockerfile inside Backend folder to create docker image for Backend code.</li> <li>Create backend image: <pre><code>    docker build -f `&lt;dockerfile-name&gt;` -t `&lt;tag-name&gt;`\n</code></pre></li> <li>Retag the created image to ACR name:     <pre><code>docker tag localhost/&lt;imagename&gt;:&lt;tag&gt; &lt;acr login server&gt;/&lt;imagename&gt;:&lt;tag&gt;\n</code></pre></li> <li>Login to az and then login to acr:     <pre><code>docker login &lt;acr login servername&gt;\n</code></pre></li> <li>Push the retagged image to ACR:     <pre><code>docker push &lt;acr login servername&gt;/&lt;imagename&gt;:&lt;tag&gt;\n</code></pre></li> <li>Create backend deployment file:     <pre><code>nano &lt;deployment filename.yaml&gt;\n</code></pre></li> <li>Login to Azure Kubernetes</li> <li>Execute the deployment file:     <pre><code>kubectl apply -f &lt;deployment filename.yaml&gt;\n</code></pre></li> <li>Check if the pods is deployed successfully:     <pre><code>kubectl get pods -n &lt;namespace&gt;\n</code></pre></li> <li>Check if the service is up &amp; running successfully:     <pre><code>kubectl get svc -n &lt;namespace&gt;\n</code></pre></li> </ol>"},{"location":"Installation/Azure/#frontend","title":"FRONTEND","text":"<p>STEPS TO SETUP FRONTEND</p> <p>Download Frontend code from GitHub. For detailed instructions, see Download the Frontend Project Code.</p> <ol> <li>Login in to Azure VM</li> <li>Download Frontend source code folder into Azure VM</li> <li>Before starting frontend image creation make sure that arize phoenix, Grafana are setup as per instructions provided above. Update the respective urls of all these services in <code>.env</code> file.</li> <li>Change working directory to the Frontend Folder: <pre><code>    cd &lt;Frontend foldername&gt;\n</code></pre></li> <li>Create Dockerfile inside Frontend folder to create docker image from Frontend code.</li> <li> <p>In <code>.env</code> file of Frontend, update below service loadbalancer urls which were generated after deployment of backend server in Azure Kubernetes.</p> <ul> <li><code>REACT_APP_BASE_URL</code> (use Backend url),</li> <li><code>REACT_APP_MKDOCS_BASE_URL</code> (use mkdocs url),</li> <li><code>REACT_APP_LIVE_TRACKING_URL</code> (use Arize phoenix url),</li> <li><code>REACT_APP_GRAFANA_DASHBOARD_URL</code> (use Grafana url)</li> </ul> </li> <li> <p>Create Frontend image: <pre><code>    docker build -f &lt;dockerfile-name&gt; -t &lt;tag-name&gt;\n</code></pre></p> </li> <li>Retag the created image to ACR name:     <pre><code>docker tag localhost/&lt;imagename&gt;:&lt;tag&gt; &lt;acr login server&gt;/&lt;imagename&gt;:&lt;tag&gt;\n</code></pre></li> <li>Login to az and then login to acr:     <pre><code>docker login &lt;acr login servername&gt;\n</code></pre></li> <li>Push the retagged image to ACR:     <pre><code>docker push &lt;acr login servername&gt;/&lt;imagename&gt;:&lt;tag&gt;\n</code></pre></li> <li>Create Frontend deployment file:     <pre><code>nano &lt;deployment filename.yaml&gt;\n</code></pre></li> <li>Login to Azure Kubernetes</li> <li>Execute the deployment file:     <pre><code>kubectl apply -f &lt;deployment filename.yaml&gt;\n</code></pre></li> <li>Check if the pods is deployed successfully:     <pre><code>kubectl get pods -n &lt;namespace&gt;\n</code></pre></li> <li>Check if the service is up &amp; running successfully:     <pre><code>kubectl get svc -n &lt;namespace&gt;\n</code></pre></li> </ol>"},{"location":"Installation/Azure/#troubleshooting","title":"Troubleshooting","text":"<p>Virtual Environment Activation Fails</p> <ul> <li>Permissions Error: Try running command prompt as administrator</li> </ul> <p>Dependency Installation Errors</p> <p>Update pip to the latest version: <pre><code>python -m pip install --upgrade pip\n</code></pre></p> <p>Server or UI Not Starting</p> <ol> <li>Verify the virtual environment is active</li> <li>Check for typos in commands or file names</li> <li>Ensure all dependencies are properly installed</li> <li>To troubleshoot check pod logs in Kubernetes, By using below commands:  <pre><code>kubectl describe pods &lt;pod_name&gt; -n &lt;namespace&gt;\nkubectl logs &lt;pod_name&gt; -n &lt;namespace&gt;\n</code></pre></li> </ol>"},{"location":"Installation/Database_Setup/","title":"Database Setup Guide","text":""},{"location":"Installation/Database_Setup/#postgresql-installation-on-windows-vm","title":"PostgreSQL Installation on Windows VM","text":"<ol> <li>Download the PostgreSQL installation wizard and start it up. </li> <li>Choose the default directory or customize as required. </li> <li>All the components will be selected by default; keep them as is and click \"Next\" to continue. </li> <li>Choose the default Data directory or change as required. </li> <li>Create a password for postgres (superuser) \u2013 This password will be used in the connection string for connecting to the database: <code>postgresql://postgres:password@localhost:port/database</code> </li> <li>Set the port number (default: 5432) or change if required. </li> <li>Use the Locale field as desired (default is OS locale). Leave this as is and click next to continue. </li> <li>Click Next to continue. </li> <li>Click Next to start the installation. </li> <li>After installation, a checkbox will ask if you wish to install additional tools with Stack Builder.</li> <li>Uncheck this as it is not necessary and likely not supported inside a VM. </li> </ol>"},{"location":"Installation/Database_Setup/#postgresql-installation-on-linux","title":"PostgreSQL Installation on Linux","text":"<p>Installation using PostgreSQL Official Packages</p> <ol> <li>Go to the PostgreSQL Linux download page.</li> <li>Select your OS distribution and follow the instructions to get the appropriate installation script.</li> <li>Choose PostgreSQL version 17.</li> <li>Example for RHEL9/CentOS9: <pre><code>sudo dnf install -y https://download.postgresql.org/pub/repos/yum/reporpms/EL-9-x86_64/pgdg-redhat-repo-latest.noarch.rpm\nsudo dnf install -y postgresql17-server\nsudo /usr/pgsql-17/bin/postgresql-17-setup initdb\nsudo systemctl enable postgresql-17\nsudo systemctl start postgresql-17\n</code></pre></li> </ol> <p>Installation from Source</p> <ol> <li>Download and extract source: <pre><code>wget https://ftp.postgresql.org/pub/source/v17.3/postgresql-17.3.tar.gz\ntar -xzf postgresql-17.3.tar.gz\ncd postgresql-17.3\n</code></pre></li> <li>Install required build dependencies: <pre><code>sudo dnf install libicu-devel readline-devel perl-FindBin\n</code></pre></li> <li>Compile and install: <pre><code>./configure\nmake\nsudo make install\n</code></pre></li> <li>Initialize data directory and configure permissions: <pre><code>sudo mkdir /usr/local/pgsql/data\nsudo useradd -r -s /bin/bash postgres\nsudo chown -R postgres:postgres /usr/local/pgsql/\n\nsudo mkdir -p /home/postgres\nsudo chown postgres:postgres /home/postgres\nsudo -u postgres /usr/local/pgsql/bin/initdb -D /usr/local/pgsql/data\n</code></pre></li> <li>Start PostgreSQL server and verify installation: <pre><code>sudo -u postgres /usr/local/pgsql/bin/pg_ctl -D /usr/local/pgsql/data start\npsql --version\n</code></pre></li> </ol> <p>Recommended: Set Up PostgreSQL as a systemd Service</p> <ol> <li>Create the systemd service file: <pre><code>sudo nano /etc/systemd/system/postgresql.service\n</code></pre>     Paste the following:     <pre><code>[Unit]\nDescription=PostgreSQL database server\nAfter=network.target\n\n[Service]\nType=forking\n\nUser=postgres\nGroup=postgres\n\nExecStart=/usr/local/pgsql/bin/pg_ctl start -D /usr/local/pgsql/data -s -l /usr/local/pgsql/data/serverlog -o \"-p 5432\"\nExecStop=/usr/local/pgsql/bin/pg_ctl stop -D /usr/local/pgsql/data -s -m fast\nExecReload=/usr/local/pgsql/bin/pg_ctl reload -D /usr/local/pgsql/data -s\nEnvironment=PGDATA=/usr/local/pgsql/data\n\n[Install]\nWantedBy=multi-user.target\n</code></pre></li> <li>Enable and start the service: <pre><code>sudo systemctl enable postgresql.service\nsudo systemctl restart postgresql.service\njournalctl -u postgresql.service -f  # (to view logs)\n</code></pre></li> </ol> <p>Additional Configuration (Remote Access, Password Setup)</p> <ol> <li>Allow connections from other hosts:<ul> <li>Edit <code>postgresql.conf</code>:     <pre><code>sudo nano /usr/local/pgsql/data/postgresql.conf\n</code></pre>     Set the following values:     <pre><code>listen_addresses = '*'\nmax_connections = 500\n</code></pre></li> </ul> </li> <li>Set postgres user password: <pre><code>sudo -u postgres /usr/local/pgsql/bin/psql\n</code></pre>     In psql prompt:     <pre><code>alter user postgres password '&lt;yourpassword&gt;';\n</code></pre></li> <li>Enable password authentication for remote access:<ul> <li>Edit <code>pg_hba.conf</code>:     <pre><code>sudo nano /usr/local/pgsql/data/pg_hba.conf\n</code></pre>     Add this line:     <pre><code>host  all  all  0.0.0.0/0  md5\n</code></pre></li> </ul> </li> <li>Restart PostgreSQL to apply changes: <pre><code>sudo systemctl restart postgresql.service\njournalctl -u postgresql.service -f\n</code></pre></li> <li>If firewall is active, open port 5432: <pre><code>sudo firewall-cmd --permanent --add-port=5432/tcp #Example for RHEL\nsudo firewall-cmd --reload\n</code></pre></li> </ol> <p>Note: Adjust paths and version numbers as needed for your environment.</p>"},{"location":"Installation/Database_Setup/#postgresql-database-setup","title":"PostgreSQL Database Setup","text":"<p>Environment Configuration</p> <p>Create a <code>.env</code> file with the following variables:</p> <pre><code>DATABASE_URL=postgresql://postgres:postgres@localhost:5432/postgres\n\n# PostgreSQL Configuration\nPOSTGRESQL_HOST=localhost\nPOSTGRESQL_USER=postgres\nPOSTGRESQL_PASSWORD=postgres\nDATABASE=your_database_name\nPOSTGRESQL_DATABASE_URL=postgresql://postgres:postgres@localhost:5432/your_database_name?sslmode=disable\n</code></pre> <p>Database Creation</p> <ol> <li>Define required databases in a list variable in <code>database_manager.py</code>:</li> </ol> <pre><code>REQUIRED_DATABASES = [\n    \"feedback_learning\",\n    \"telemetry_logs\", \n    \"agentic_workflow_as_service_database\",\n    \"login\",\n    \"logs\",\n    \"arize_traces\"\n]\n</code></pre> <ol> <li>Load environment variables in <code>database_manager.py</code>:</li> </ol> <pre><code>Postgre_string = os.getenv(\"DATABASE_URL\")\nPOSTGRESQL_HOST = os.getenv(\"POSTGRESQL_HOST\", \"\")\nPOSTGRESQL_USER = os.getenv(\"POSTGRESQL_USER\", \"\")\nPOSTGRESQL_PASSWORD = os.getenv(\"POSTGRESQL_PASSWORD\", \"\")\nDATABASE = os.getenv(\"DATABASE\", \"\")\nDATABASE_URL = os.getenv(\"POSTGRESQL_DATABASE_URL\", \"\")\n</code></pre> <ol> <li>Function to connect to postgres database in <code>database_manager.py</code>:</li> </ol> <pre><code>def get_postgres_url():\n    url = urlparse(Postgre_string)\n    # Replace path with '/postgres'\n    new_url = url._replace(path=\"/postgres\")\n    return urlunparse(new_url)\n</code></pre> <ol> <li> <p>Create Databases function</p> </li> <li> <p>The system will connect to the 'postgres' database under postgres user and create the required databases listed in <code>REQUIRED_DATABASES</code>:</p> </li> </ol> <pre><code>async def check_and_create_databases():\n    conn = await asyncpg.connect(get_postgres_url())\n    try:\n        for db_name in REQUIRED_DATABASES:\n            exists = await conn.fetchval(\n                \"SELECT 1 FROM pg_database WHERE datname = $1\", db_name\n            )\n            if not exists:\n                print(f\"Database '{db_name}' not found. Creating...\")\n                await conn.execute(f'CREATE DATABASE \"{db_name}\"')\n            else:\n                print(f\"Database '{db_name}' already exists.\")\n    finally:\n        await conn.close()\n</code></pre>"},{"location":"Installation/Database_Setup/#redis-installation-on-windows","title":"Redis Installation on Windows","text":"<ol> <li>Go to the redis-windows GitHub releases page.</li> <li>Download the ZIP build for Redis 8.2.1: <code>Redis-8.2.1-Windows-x64-msys2.zip</code></li> <li>Extract the ZIP file to a folder of your choice.</li> <li>Open <code>redis.conf</code> in the same folder and set the following parameters:     <pre><code>bind 0.0.0.0\nrequirepass &lt;password&gt;\n</code></pre></li> <li>Open PowerShell or CMD inside that folder.</li> <li>Start Redis server:     <pre><code>redis-server.exe redis.conf\n</code></pre></li> </ol>"},{"location":"Installation/Database_Setup/#redis-installation-on-linux","title":"Redis Installation on Linux","text":"<p>Install dependencies (per OS)</p> <p>Use the command for your distribution:</p> <ul> <li>Red Hat/CentOS/Fedora: <pre><code>sudo dnf install gcc make openssl-devel tcl libtool autoconf automake -y\n</code></pre></li> <li>Debian/Ubuntu: <pre><code>sudo apt install build-essential libssl-dev tcl-dev libtool autoconf automake -y\n</code></pre></li> <li>SUSE/OpenSUSE: <pre><code>sudo zypper install gcc make libopenssl-devel tcl libtool autoconf automake -y\n</code></pre></li> </ul> <p>Build and install from source</p> <ol> <li>Download Redis 8.2.1: <pre><code>wget https://github.com/redis/redis/archive/refs/tags/8.2.1.tar.gz\n</code></pre></li> <li>Extract and build: <pre><code>tar xvf 8.2.1.tar.gz\ncd redis-8.2.1\nmake\nsudo make install\n</code></pre></li> </ol> <p>Configuration (redis.conf)</p> <ol> <li>Edit <code>redis.conf</code> to set the following values:     <pre><code>bind 0.0.0.0\nrequirepass &lt;password&gt;\npidfile /var/run/redis/\n</code></pre></li> <li>Copy the config and set permissions:     <pre><code>sudo mkdir -p /etc/redis\nsudo cp /path/to/redis-8.2.1/redis.conf /etc/redis/redis.conf\nsudo chown -R projadmin:projadmin /etc/redis/\n\nsudo mkdir -p /var/run/redis\nsudo chown -R projadmin:projadmin /var/run/redis\n</code></pre></li> </ol> <p>Optional: Set Up Redis as a systemd Service</p> <ol> <li> <p>Create a systemd service file:     <pre><code>sudo nano /etc/systemd/system/redis.service\n</code></pre>     Paste in the following:     <pre><code>[Unit]\nDescription=Redis In-Memory Data Store\nAfter=network.target\n\n[Service]\nUser=projadmin\nExecStart=/usr/local/bin/redis-server /etc/redis/redis.conf\nExecStop=/usr/local/bin/redis-cli -a &lt;password&gt; shutdown\nRestart=always\n\n[Install]\nWantedBy=multi-user.target\n</code></pre></p> </li> <li> <p>Start and enable Redis service:     <pre><code>sudo systemctl start redis.service\nsudo systemctl enable redis.service\n</code></pre></p> </li> <li>If firewall is active, open port 6379: <pre><code>sudo firewall-cmd --permanent --add-port=6379/tcp #Example for RHEL\nsudo firewall-cmd --reload\n</code></pre> <p>Note: Update <code>User</code> and <code>&lt;password&gt;</code> in your redis configuration and service file to match your security and environment needs.</p> </li> </ol>"},{"location":"Installation/GCP/","title":"Project Setup Guide","text":"<p>This document provides comprehensive step-by-step instructions to deploy Infosys Agentic Foundry in Google Kubernetes Engine(GKE).</p>"},{"location":"Installation/GCP/#prerequisites","title":"Prerequisites","text":"<p>Before setting up the project, ensure you have the following requirements and access permissions:</p> <p>Ensure users have access to below:</p> <ul> <li>Python 3.12+ - Required Python version</li> <li> <p>React - Frontend framework</p> </li> <li> <p>Infosys Github Repo: Infosys-Agentic-Foundry</p> </li> <li>Ensure users have their Google Cloud Vertex AI / Generative AI Studio API Keys and Speech-to-Text Keys and Endpoints.</li> <li>Ensure GCP resources are created, and you have access/connectivity to push and pull from these resources:<ul> <li>Google Cloud Artifact Registry</li> <li>Google Kubernetes Engine (GKE)</li> <li>Cloud SQL for PostgreSQL (Instance equivalent: db-custom-2-4096 \u2013 2 vCPUs, 4 GiB memory, 32 GiB SSD storage)</li> </ul> </li> <li>Google Compute Engine (GCE) Linux Virtual Machine</li> <li>Install Docker, gcloud CLI, and kubectl in your GCE VM</li> <li>Create a namespace in the GKE cluster as per your requirement (Optional)</li> </ul> <pre><code>kubectl create namespace &lt;namespace&gt;\n</code></pre>"},{"location":"Installation/GCP/#gcp-deployment","title":"GCP Deployment","text":""},{"location":"Installation/GCP/#arize-phoenix","title":"ARIZE PHOENIX","text":"<p>STEPS FOR DEPLOYING ARIZE PHOENIX IN GKE</p> <ol> <li>Create a yaml file for deploying arize phoenix as a container, you can use the arize phoenix image in the yaml file.</li> </ol> <pre><code>nano filename1.yaml\n</code></pre> <p>Info</p> <p>You can get the image from Docker - Phoenix or any other trusted source which your organization allows</p> <ol> <li>Now you need to use this command for creating deployment and service: <pre><code>kubectl apply -f filename1.yaml\n</code></pre></li> <li>You can check the pods deployed using the command below <pre><code>kubectl get pods -n namespace\n</code></pre></li> <li>You can check the services deployed using the command below <pre><code>kubectl get svc -n namespace\n</code></pre></li> <li>Note down the load balancer IP for the container. You need to update it in the <code>.env</code> of your backend and frontend folders before creating the respective docker images.</li> </ol>"},{"location":"Installation/GCP/#redis","title":"REDIS","text":"<p>STEPS FOR DEPLOYING REDIS IN GKE</p> <ol> <li>Create a yaml file for deploying Redis as a container, you can use the redis image in the yaml file.</li> </ol> <pre><code>nano filename2.yaml\n</code></pre> <p>Info</p> <p>You can get the image from Image Layer Details - redis:8.2.1 or any other trusted sources which your organization allows</p> <ol> <li>Now you need to use this command for creating deployment and service: <pre><code>kubectl apply -f filename2.yaml\n</code></pre></li> <li>You can check the pods deployed using the command below <pre><code>kubectl get pods -n namespace\n</code></pre></li> <li>You can check the services deployed using the command below <pre><code>kubectl get svc -n namespace\n</code></pre></li> <li>Note down the load balancer IP for the container. You need to update it in the <code>.env</code> of your backend folder before creating the respective docker image.</li> </ol>"},{"location":"Installation/GCP/#grafana","title":"GRAFANA","text":"<p>STEPS FOR DEPLOYING GRAFANA IN GKE</p> <ol> <li>Create a yaml file for deploying Grafana as a container, you can use the grafana image in the yaml file. </li> </ol> <pre><code>nano filename3.yaml\n</code></pre> <p>Info</p> <p>You can get the image from Image Layer Details - grafana/grafana:11.2.0 or any other trusted sources which your organization allows</p> <ol> <li>Now you need to use this command for creating deployment and service: <pre><code>kubectl apply -f filename3.yaml\n</code></pre></li> <li>You can check the pods deployed using the command below <pre><code>kubectl get pods -n namespace\n</code></pre></li> <li>You can check the services deployed using the command below <pre><code>kubectl get svc -n namespace\n</code></pre></li> <li>Note down the load balancer IP for the container. You need to update it in the <code>.env</code> of your backend and frontend folders before creating the respective docker images.</li> </ol>"},{"location":"Installation/GCP/#elastic-search","title":"ELASTIC SEARCH","text":"<p>STEPS FOR DEPLOYING ELASTIC SEARCH IN GKE</p> <ol> <li>Create a yaml file for deploying elastic search as a container, you can use the elastic search image in the yaml file. </li> </ol> <pre><code>nano filename4.yaml\n</code></pre> <p>Info</p> <p>You can get the image from elasticsearch - Official Image | Docker Hub or any other trusted sources which your organization allows</p> <ol> <li>Now you need to use this command for creating deployment and service: <pre><code>kubectl apply -f filename4.yaml\n</code></pre></li> <li>You can check the pods deployed using the command below <pre><code>kubectl get pods -n namespace\n</code></pre></li> <li>You can check the services deployed using the command below <pre><code>kubectl get svc -n namespace\n</code></pre></li> <li>Note down the load balancer IP for the container and update it in the OpenTelemetry YAML script. </li> </ol>"},{"location":"Installation/GCP/#open-telemetry","title":"OPEN-TELEMETRY","text":"<p>STEPS FOR DEPLOYING OPEN-TELEMETRY IN GKE</p> <ol> <li>Create a yaml file for deploying OpenTelemetry as a container, you can use the OpenTelemetry directly in the yaml file. </li> </ol> <pre><code>nano filename5.yaml\n</code></pre> <p>Info</p> <p>You can get the image from otel/OpenTelemetry-collector-contrib - Docker Image  or any other trusted sources your organization allows</p> <ol> <li>Now you need to use this command for creating deployment and service: <pre><code>kubectl apply -f filename5.yaml\n</code></pre></li> <li>You can check the pods deployed using the command below <pre><code>kubectl get pods -n namespace\n</code></pre></li> <li> <p>You can check the services deployed using the command below <pre><code>kubectl get svc -n namespace\n</code></pre></p> </li> <li> <p>Note down the load balancer IP for the container. You need to update it in the <code>.env</code> of your backend folder before creating the respective docker image.</p> </li> </ol>"},{"location":"Installation/GCP/#model-server","title":"MODEL SERVER","text":"<p>STEPS TO SETUP MODEL SERVER</p> <p>For detailed instructions on deploying and configuring your model server, refer to the Model Server Deployment guide.</p> <p>Note</p> <p>You need to update the URL for the model server in the <code>.env</code> of the backend folder</p>"},{"location":"Installation/GCP/#backend","title":"BACKEND","text":"<p>STEPS TO SETUP BACKEND</p> <p>Download Backend code from GitHub. For detailed instructions, see Download the Backend Project Code.</p> <ol> <li>Login in to Login into Google Compute Engine (GCE) VM.</li> <li>Download Backend source code folder into GCE VM</li> <li>Before starting backend image creation make sure that arize phoenix, OpenTelemetry, Grafana, Elastic Search, Redis and Model server are setup as per instructions provided above. Update the respective urls of all these services in <code>.env</code> file.</li> <li>Update the remaining values for the variables in the <code>.env</code> file.</li> <li>Change working directory to the Backend Folder: <pre><code>cd `&lt;BE foldername&gt;`\n</code></pre></li> <li>Create Dockerfile inside Backend folder to create docker image for Backend code.</li> <li>Create backend image: <pre><code>    docker build -f `&lt;dockerfile-name&gt;` -t `&lt;tag-name&gt;`\n</code></pre></li> <li>Retag the created image to Artifact Registry name:     <pre><code>docker tag localhost/&lt;imagename&gt;:&lt;tag&gt; &lt;region&gt;-docker.pkg.dev/&lt;project-id&gt;/&lt;repository-name&gt;/&lt;imagename&gt;:&lt;tag&gt;\n</code></pre></li> <li>Configure Docker to use GCR credentials &amp; Login to Artifact Registry:     <pre><code>gcloud auth configure-docker &lt;region&gt;-docker.pkg.dev\n</code></pre></li> <li>Push the retagged image to Artifact Registry:     <pre><code>docker push &lt;region&gt;-docker.pkg.dev/&lt;project-id&gt;/&lt;repository&gt;/&lt;imagename&gt;:&lt;tag&gt;\n</code></pre></li> <li>Create backend deployment file:     <pre><code>nano &lt;deployment filename.yaml&gt;\n</code></pre></li> <li>Login to Google Kubernetes Engine (GKE)</li> <li>Execute the deployment file:     <pre><code>kubectl apply -f &lt;deployment filename.yaml&gt;\n</code></pre></li> <li>Check if the pods is deployed successfully:     <pre><code>kubectl get pods -n &lt;namespace&gt;\n</code></pre></li> <li>Check if the service is up &amp; running successfully:     <pre><code>kubectl get svc -n &lt;namespace&gt;\n</code></pre></li> </ol>"},{"location":"Installation/GCP/#frontend","title":"FRONTEND","text":"<p>STEPS TO SETUP FRONTEND</p> <p>Download Frontend code from GitHub. For detailed instructions, see Download the Frontend Project Code.</p> <ol> <li>Login in to Google Cloud Platform(GCP VM)</li> <li>Download Frontend source code folder into Google Cloud Platform (GCP VM)</li> <li>Before starting frontend image creation make sure that arize phoenix, Grafana are setup as per instructions provided above. Update the respective urls of all these services in <code>.env</code> file.</li> <li>Change working directory to the Frontend Folder: <pre><code>    cd &lt;Frontend foldername&gt;\n</code></pre></li> <li>Create Dockerfile inside Frontend folder to create docker image from Frontend code.</li> <li>Change the name of <code>.env-example</code> file of Frontend to <code>.env</code>.</li> <li>Create Frontend image: <pre><code>    docker build -f &lt;dockerfile-name&gt; -t &lt;tag-name&gt;\n</code></pre></li> <li>Retag the created image to Artifact Registry:     <pre><code>docker tag localhost/&lt;imagename&gt;:&lt;tag&gt; &lt;region&gt;-docker.pkg.dev/&lt;project-id&gt;/&lt;repository&gt;/&lt;imagename&gt;:&lt;tag&gt;\n</code></pre></li> <li>Login to GCP VM and then login to Artifact Registry:     <pre><code>gcloud auth configure-docker &lt;region&gt;-docker.pkg.dev\n</code></pre></li> <li>Push the retagged image to Artifact Registry:     <pre><code>docker push &lt;region&gt;-docker.pkg.dev/&lt;project-id&gt;/&lt;repository&gt;/&lt;imagename&gt;:&lt;tag&gt;\n</code></pre></li> <li> <p>Create Frontend deployment file:     <pre><code>nano &lt;deployment filename.yaml&gt;\n</code></pre>    Pass/override the load balancer URLs which were generated after deployment of Backend server in Google Kubernetes Engine (GKE) via <code>env</code> section of deployment file for below listed services</p> <ul> <li><code>REACT_APP_BASE_URL</code> (use Backend URL),</li> <li><code>REACT_APP_MKDOCS_BASE_URL</code> (use mkdocs public URL),</li> <li><code>REACT_APP_LIVE_TRACKING_URL</code> (use Arize phoenix URL),</li> <li><code>REACT_APP_GRAFANA_DASHBOARD_URL</code> (use Grafana URL)</li> </ul> </li> <li> <p>Login to Google Kubernetes Engine</p> </li> <li>Execute the deployment file:     <pre><code>kubectl apply -f &lt;deployment filename.yaml&gt;\n</code></pre></li> <li>Check if the pods is deployed successfully:     <pre><code>kubectl get pods -n &lt;namespace&gt;\n</code></pre></li> <li>Check if the service is up &amp; running successfully:     <pre><code>kubectl get svc -n &lt;namespace&gt;\n</code></pre></li> </ol>"},{"location":"Installation/GCP/#troubleshooting","title":"Troubleshooting","text":"<p>Virtual Environment Activation Fails</p> <ul> <li>Permissions Error: Try running command prompt as administrator</li> </ul> <p>Dependency Installation Errors</p> <p>Update pip to the latest version: <pre><code>python -m pip install --upgrade pip\n</code></pre></p> <p>Server or UI Not Starting</p> <ol> <li>Verify the virtual environment is active</li> <li>Check for typos in commands or file names</li> <li>Ensure all dependencies are properly installed</li> <li>To troubleshoot check pod logs in Kubernetes, By using below commands:  <pre><code>kubectl describe pods &lt;pod_name&gt; -n &lt;namespace&gt;\nkubectl logs &lt;pod_name&gt; -n &lt;namespace&gt;\n</code></pre></li> </ol>"},{"location":"Installation/linux/","title":"Linux VM Deployment Guide","text":"<p>This guide provides detailed instructions for setting up and running the FastAPI backend and React frontend project on Linux Virtual Machines.</p>"},{"location":"Installation/linux/#project-overview","title":"Project Overview","text":"<p>This project consists of a backend server built with FastAPI and a frontend interface using React, designed for deployment on Linux VMs.</p>"},{"location":"Installation/linux/#prerequisites","title":"Prerequisites","text":"<p>Ensure you have the following installed on your Linux system:</p> <p>System Requirements</p> <ul> <li>sudo privileges (for installing system packages)</li> <li>Python 3.11 or higher (for backend)</li> <li>NodeJS version 22 or higher </li> <li>NPM version 10.9.2 or higher ( comes bundled with NodeJs) </li> <li>Git (optional, for cloning the repository)</li> <li>Redis 8.2.1</li> <li>Postgres 17</li> </ul>"},{"location":"Installation/linux/#python-version-setup","title":"Python Version Setup","text":"<p>Check Your Python Version</p> <p>First, verify your current Python version:</p> <pre><code>python --version\n# or\npython3 --version\n</code></pre> <p>Make sure it is 3.11 or higher. If it's not, you'll need to update your Python version.</p> <p>Python Installation Options</p> <p>There are 2 ways to install the required Python version:</p> <ol> <li>Install - Install a required greenlisted Python version</li> <li>Use pyenv - Update the Python version using pyenv (recommended)</li> </ol>"},{"location":"Installation/linux/#installing-python-with-pyenv-on-rhel","title":"Installing Python with pyenv on RHEL","text":"<p>To install pyenv on Red Hat Enterprise Linux (RHEL), follow these steps. pyenv lets you easily install and switch between multiple Python versions.</p> <p>Step 1: Install Required Dependencies</p> <p>You'll need development tools and libraries for building Python:</p> <pre><code>sudo dnf groupinstall \"Development Tools\" -y\nsudo dnf install -y \\\n    gcc zlib-devel bzip2 bzip2-devel readline-devel sqlite sqlite-devel \\\n    openssl-devel libffi-devel wget make xz-devel \\\n    git curl patch\n</code></pre> <p>Step 2: Install pyenv</p> <p>Clone the pyenv repository into your home directory:</p> <pre><code>git clone https://github.com/pyenv/pyenv.git ~/.pyenv\n</code></pre> <p>Step 3: Set Up Shell Environment</p> <p>Add the following to your shell config file (e.g., <code>~/.bashrc</code>, <code>~/.bash_profile</code>, or <code>~/.zshrc</code>):</p> <pre><code>export PYENV_ROOT=\"$HOME/.pyenv\"\nexport PATH=\"$PYENV_ROOT/bin:$PATH\"\neval \"$(pyenv init --path)\"\neval \"$(pyenv init -)\"\n</code></pre> <p>Then apply the changes:</p> <pre><code>source ~/.bashrc\n# or\nexec \"$SHELL\"\n</code></pre> <p>Step 4: Verify Installation</p> <p>Run the following command to verify pyenv is installed correctly:</p> <pre><code>pyenv --version\n</code></pre> <p>Step 5: Install Python Version</p> <p>Install the required Python version (example with Python &gt;=3.11.8 or &lt;3.13):</p> <pre><code>pyenv install 3.11.8\n</code></pre> <p>You can also install other versions as needed:</p> <pre><code># List available Python versions\npyenv install --list\n\n# Install specific version\npyenv install 3.12.0\n</code></pre> <p>Step 6: Set Global Python Version</p> <p>Set the installed Python version as your global default:</p> <pre><code>pyenv global 3.11.8\n</code></pre> <p>Verify the installation:</p> <pre><code>python --version\n</code></pre> <p>Verify Node.js and npm</p> <p>To verify your Node.js and npm installations, open Terminal and run:</p> <pre><code>node -v\nnpm -v\n</code></pre> <ul> <li>If Node.js is installed, running the version command will display the installed version.</li> <li>If Node.js is not installed, you will see an error in the terminal such as <code>\"node\" is not recognized as an internal or external command</code>.</li> </ul> <p>Installation Guidance</p> <ul> <li> <p>For Local Linux Setup:    Install Node.js (version 22 or higher). Npm comes bundled with Node.js. For downloading Node dependencies\u2014see below for details.</p> </li> <li> <p>Linux RHEL VM Setup:    On RHEL VMs, you can install Node.js and npm using the following commands:</p> </li> </ul> <pre><code>sudo dnf module enable nodejs:22\nsudo dnf install nodejs npm\nnode --version\n</code></pre> <p>If you are behind a proxy, configure npm as follows:</p> <pre><code>npm config set proxy &lt;your_proxy&gt;\nnpm config set https-proxy &lt;your_proxy&gt;\n</code></pre>"},{"location":"Installation/linux/#setting-up-proxy-in-linux-environment-if-required","title":"Setting Up Proxy in Linux Environment (If Required)","text":"<p>If your network requires a proxy to access the internet, follow these steps to set proxy values as environment variables:</p> <p>Steps:</p> <p>Set proxy environment variables temporarily:</p> <pre><code># Replace with your actual proxy server and port\nexport http_proxy=http://your-proxy-server:your-proxy-port\nexport https_proxy=http://your-proxy-server:your-proxy-port\n</code></pre> <p>To make proxy settings permanent, add to your shell profile:</p> <pre><code># Replace with your actual proxy server and port\necho 'export http_proxy=http://your-proxy-server:your-proxy-port' &gt;&gt; ~/.bashrc\necho 'export https_proxy=http://your-proxy-server:your-proxy-port' &gt;&gt; ~/.bashrc\nsource ~/.bashrc\n</code></pre> <p>Note</p> <p>\ud83d\udca1 Always verify proxy details with your administrator and replace the example values with your actual proxy configuration.</p>"},{"location":"Installation/linux/#download-the-backend-project-code","title":"Download the Backend Project Code","text":"<p>You can obtain the project files using one of the following methods:</p> <p>Option 1: Clone Using Git</p> <pre><code>git clone https://github.com/Infosys/Infosys-Agentic-Foundry\ncd Infosys-Agentic-Foundry\n</code></pre> <p>Option 2: Download Zip from GitHub</p> <ol> <li>Navigate to: https://github.com/Infosys/Infosys-Agentic-Foundry</li> <li>Click \"Code\" \u2192 \"Download Zip\"</li> <li>Extract to your preferred location</li> </ol> <p>Transferring Files to Linux VM (if needed)</p> <p>If transferring from another machine, use SCP: <pre><code># Replace with your actual username, VM IP address, and file paths\nscp -r /path/to/your/local/project your-username@your-vm-ip-address:/home/your-username/\n</code></pre></p>"},{"location":"Installation/linux/#backend-setup","title":"Backend Setup","text":"<p>Setting Up the Backend Environment</p> <ol> <li>Navigate to Backend Directory:</li> </ol> <pre><code>cd Infosys-Agentic-Foundry-Backend\n</code></pre> <ol> <li>Create a Virtual Environment:</li> </ol> <pre><code>python3 -m venv .venv\n</code></pre> <ol> <li>Activate the Virtual Environment:</li> </ol> <pre><code>source ./.venv/bin/activate\n</code></pre> <ol> <li>Install Backend Dependencies:</li> </ol> <pre><code>pip install -r requirements.txt\n</code></pre> <p>If you face any SSL error issue, use the below command:</p> <pre><code>pip install -r requirements.txt --trusted-host pypi.org --trusted-host files.pythonhosted.org\n</code></pre>"},{"location":"Installation/linux/#frontend-setup","title":"Frontend Setup","text":"<p>Setup on Linux VM</p> <ol> <li>Navigate to Frontend Directory:</li> </ol> <pre><code>cd Infosys-Agentic-Foundry-Frontend\n</code></pre> <ol> <li>Remove Existing Lock File (if present):</li> </ol> <pre><code>rm -f package-lock.json\n</code></pre> <ol> <li>Install Node Modules:</li> </ol> <pre><code>npm install\n</code></pre> <p>If you encounter proxy issues during npm install, configure npm proxy:</p> <pre><code># Replace with your actual proxy server and port\nnpm config set proxy http://your-proxy-server:your-proxy-port\nnpm config set https-proxy http://your-proxy-server:your-proxy-port\n\n# Then run npm install\nnpm install\n</code></pre> <ol> <li>Open Firewall Ports (RHEL):</li> </ol> <pre><code># Replace with your actual frontend and backend port numbers\nsudo firewall-cmd --permanent --add-port=your-frontend-port/tcp\nsudo firewall-cmd --permanent --add-port=your-backend-port/tcp\n\n\n# Reload firewall\nsudo firewall-cmd --reload\n\n# Verify ports are open\nsudo firewall-cmd --list-ports\n</code></pre>"},{"location":"Installation/linux/#configuration-setup","title":"Configuration Setup","text":"<p>Frontend-Backend Connection Configuration: Configure Frontend to Connect to Backend</p> <p>1. Edit Constants File:</p> <pre><code>nano Infosys-Agentic-Foundry-Frontend/.env\n</code></pre> <p>2. Update Base URL for the API server:</p> <pre><code>// Replace with your actual backend server IP address and port\nREACT_APP_BASE_URL = \"http://your-backend-server-ip:your-backend-port\";\n</code></pre> <p>Configure Backend CORS Settings</p> <p>In the backend <code>.env</code> file (<code>Infosys-Agentic-Foundry-Backend/.env</code>), set the allowed frontend origins:</p> <pre><code># Add your frontend IP address\nUI_CORS_IP=\"&lt;your-frontend-server-ip&gt;\"\n\n# Add your frontend IP with port number\nUI_CORS_IP_WITH_PORT=\"&lt;your-frontend-server-ip:your-frontend-port&gt;\"\n</code></pre> <p>If you want to let other UI connect to the backend, Update the backend server file (typically <code>run_server.py</code> or <code>main.py</code>):</p> <pre><code>origins = [\n    # Replace with your actual frontend server IP and port\n    \"http://your-frontend-server-ip:your-frontend-port\",\n    \"http://localhost:3000\",\n    \"http://localhost:6002\",\n    # Add additional origins as needed\n]\n</code></pre>"},{"location":"Installation/linux/#environment-configuration","title":"Environment Configuration","text":"<p>Backend Environment Variables</p> <p>Create <code>.env</code> file in backend directory:</p> <pre><code>nano Infosys-Agentic-Foundry-Backend/.env\n</code></pre> <p>Add the following content (replace with your actual values):</p> <pre><code>DEBUG=False\n# Replace with your actual API keys and configuration\nAPI_KEY=your_actual_api_key_here\nDATABASE_URL=your_database_url_here\nSECRET_KEY=your_secret_key_here\n</code></pre> <p>Frontend Environment Variables</p> <p>Create <code>.env</code> file in frontend directory:</p> <pre><code>nano Infosys-Agentic-Foundry-Frontend/.env\n</code></pre> <p>Add the following content:</p> <pre><code># Replace with your actual backend IP address and port\nREACT_APP_API_URL=http://your-backend-ip:your-backend-port\n</code></pre>"},{"location":"Installation/linux/#model-server-setup","title":"Model Server Setup","text":"<p>For detailed instructions on deploying and configuring your model server, refer to the Model Server Deployment guide.</p>"},{"location":"Installation/linux/#running-the-applications","title":"Running the Applications","text":"<p>Start the Backend Server</p> <p>With the virtual environment activated:</p> <pre><code>cd Infosys-Agentic-Foundry-Backend\n\npython run_server.py --host 0.0.0.0 --port your-backend-port `or`\npython main.py --host 0.0.0.0 --port your-backend-port\n</code></pre> <p>Start the Frontend</p> <pre><code>cd Infosys-Agentic-Foundry-Frontend\nnpm start\n</code></pre> <p>The React UI will be accessible at:</p> <ul> <li><code>http://your-vm-ip:your-frontend-port</code></li> </ul>"},{"location":"Installation/linux/#accessing-the-applications","title":"Accessing the Applications","text":"<p>Frontend Access URLs:</p> <ul> <li>Local access: <code>http://localhost:your-frontend-port</code></li> <li>Network access: <code>http://your-vm-ip:your-frontend-port</code></li> </ul> <p>Backend API Access URLs:</p> <ul> <li>Local access: <code>http://localhost:your-backend-port</code></li> <li>Network access: <code>http://your-vm-ip:your-backend-port</code></li> <li>API Documentation: <code>http://your-vm-ip:your-backend-port/docs</code></li> </ul>"},{"location":"Installation/linux/#service-deployment","title":"Service Deployment","text":"<ul> <li>Running as System Services</li> </ul> <p>Backend Service</p> <p>Create a systemd service file:</p> <pre><code>sudo nano /etc/systemd/system/infyagent-backend.service\n</code></pre> <p>Add the following content (replace placeholders with your actual values):</p> <pre><code>[Unit]\nDescription=FastAPI Application\nAfter=network.target\n\n[Service]\nWorkingDirectory=/home/your-username/your-project-directory/Infosys-Agentic-Foundry-Backend\nEnvironment=\"NO_PROXY=localhost,127.0.0.1,::1,model_server_ip,ip_of_this_VM\"\nEnvironment=\"HTTP_PROXY=&lt;your_proxy&gt;\"\nEnvironment=\"HTTPS_PROXY=&lt;your_proxy&gt;\"\nEnvironment=\"PYTHONUNBUFFERED=1\"\nEnvironment=VIRTUAL_ENV=/home/your-username/your-project-directory/Infosys-Agentic-Foundry-Backend/venv\nEnvironment=PATH=/home/your-username/your-project-directory/Infosys-Agentic-Foundry-Backend/venv/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin/:/sbin:/bin\nExecStart=/home/your-username/your-project-directory/Infosys-Agentic-Foundry-Backend/venv/bin/python main.py --host 0.0.0.0 --port your-backend-port\nRestart=always\nUser=your-username\nStandardOutput=journal\nStandardError=journal\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>Things You Need to Customize:</p> <ol> <li>Project Path: Replace <code>/home/your-username/your-project-directory/</code> with your actual project path.</li> <li>Username: Replace <code>your-username</code> with your actual Linux username.</li> <li>Backend Port: Replace <code>your-backend-port</code> with your chosen backend port (e.g., 8000).</li> <li>NO_PROXY: Update with your model server IP and VM IP as needed.</li> <li>ExecStart: Ensure the Python path and script name (<code>main.py</code> or <code>run_server.py</code>) match your project.</li> <li>Proxy Settings: Adjust or remove proxy environment variables if not required.</li> </ol> <p>Enable and start the service:</p> <pre><code>sudo systemctl enable infyagent-backend.service\nsudo systemctl start infyagent-backend.service\nsudo systemctl status infyagent-backend.service\n</code></pre> <p>Frontend Service</p> <p>Create a systemd service file for the frontend:</p> <pre><code>sudo nano /etc/systemd/system/infyagent-frontend.service\n</code></pre> <p>Add the following content:</p> <pre><code>[Unit]\nDescription=My Node.js Application\nAfter=network.target\n\n[Service]\nWorkingDirectory=/home/your-username/your-project-directory/Infosys-Agentic-Foundry-Frontend\nExecStart=/usr/bin/npm start\nRestart=always\nUser=your-username\nEnvironment=NODE_ENV=production\nEnvironment=PORT=&lt;your_port&gt;\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>Customize the following:</p> <ul> <li>WorkingDirectory: Set to your actual frontend project path.</li> <li>User: Set to your Linux username.</li> <li>PORT: Set to your desired frontend port.</li> </ul> <p>Enable and start the frontend service:</p> <p><pre><code>sudo systemctl enable infyagent-frontend.service\nsudo systemctl start infyagent-frontend.service\nsudo systemctl status infyagent-frontend.service\n</code></pre> Start Phoenix server using systemctl</p> <pre><code>[Unit]\nDescription=Phoenix Logging Server\nAfter=network.target\n\n[Service]\nUser=your-username\nWorkingDirectory=/home/your-username/your-project-directory/Infosys-Agentic-Foundry-Backend\nExecStart=/home/your-username/your-project-directory/Infosys-Agentic-Foundry-Backend/venv/bin/python -m phoenix.server.main serve\nRestart=always\nRestartSec=5\nEnvironment=HTTP_PROXY=\nEnvironment=NO_PROXY=localhost,127.0.0.1\nEnvironment=PHOENIX_GRPC_PORT=50051\nEnvironment=PHOENIX_SQL_DATABASE_URL=postgresql://postgres:&lt;your-password&gt;@localhost:5432/arize_traces\n\n[Install]\nWantedBy=multi-user.target\n</code></pre>"},{"location":"Installation/linux/#network-testing","title":"Network Testing","text":"<p>Test connectivity between frontend and backend:</p> <pre><code># Test backend API from frontend server\ncurl http://your-backend-server-ip:your-backend-port/health\n\n# Test frontend access\ncurl http://your-frontend-server-ip:your-frontend-port\n</code></pre>"},{"location":"Installation/linux/#git-installation","title":"Git Installation","text":"<p>Install Git using your OS package manager:</p> <ul> <li>RHEL/CentOS/Fedora: <pre><code>sudo dnf install git -y\n</code></pre></li> <li>Debian/Ubuntu: <pre><code>sudo apt update\nsudo apt install git -y\n</code></pre></li> <li>SUSE/OpenSUSE: <pre><code>sudo zypper install git -y\n</code></pre></li> </ul>"},{"location":"Installation/linux/#grafana-installation","title":"Grafana Installation","text":"<ol> <li> <p>Download and install Grafana Enterprise:     <pre><code>wget https://dl.grafana.com/enterprise/release/grafana-enterprise-12.0.2-1.x86_64.rpm\nsudo dnf install grafana-enterprise-12.0.2-1.x86_64.rpm -y\n</code></pre></p> </li> <li> <p>Start and enable Grafana service:     <pre><code>sudo systemctl daemon-reload\nsudo systemctl start grafana-server\nsudo systemctl enable grafana-server\n</code></pre></p> </li> </ol>"},{"location":"Installation/linux/#opentelemetry-collector-installation","title":"OpenTelemetry Collector Installation","text":"<ol> <li> <p>Download and extract the OpenTelemetry Collector Contrib binary:     <pre><code>wget https://github.com/open-telemetry/opentelemetry-collector-releases/releases/download/v0.127.0/otelcol-contrib_0.127.0_linux_amd64.tar.gz\ntar -xvzf otelcol-contrib_0.127.0_linux_amd64.tar.gz\nsudo mv otelcol-contrib /usr/local/bin/\nsudo chmod +x /usr/local/bin/otelcol-contrib\n</code></pre></p> </li> <li> <p>Create the configuration file:     <pre><code>sudo nano /usr/local/bin/otelcol-contrib.yaml\n</code></pre>     Sample config:     <pre><code>receivers:\n  otlp:\n    protocols:\n      grpc:\n        endpoint: 0.0.0.0:4319\n      http:\n        endpoint: 0.0.0.0:4320\n\nexporters:\n  debug:\n    verbosity: detailed\n  elasticsearch:\n    endpoints: [\"http://localhost:9200\"]\n    logs_index: \"agentic-foundry-tool-logs\"\n    sending_queue:\n      enabled: true\n\nprocessors:\n  batch:\n\nservice:\n  pipelines:\n    traces:\n      receivers: [otlp]\n      processors: [batch]\n      exporters: [debug]\n    metrics:\n      receivers: [otlp]\n      processors: [batch]\n      exporters: [debug]\n    logs:\n      receivers: [otlp]\n      processors: [batch]\n      exporters: [debug, elasticsearch]\n\n  telemetry:\n    logs:\n      level: info\n    metrics:\n      level: basic\n      address: localhost:8889\n</code></pre></p> </li> </ol> <p>Systemd Service Setup for OpenTelemetry Collector</p> <ol> <li>Create a systemd unit file:     <pre><code>sudo nano /etc/systemd/system/otelcol-contrib.service\n</code></pre>     Content:     <pre><code>[Unit]\nDescription=OpenTelemetry Collector Contrib\nAfter=network.target\n\n[Service]\nType=simple\nExecStart=/usr/local/bin/otelcol-contrib --config /usr/local/bin/otelcol-contrib.yaml\nRestart=on-failure\n\n[Install]\nWantedBy=multi-user.target\n</code></pre></li> <li>Enable and start the service:     <pre><code>sudo systemctl enable otelcol-contrib.service\nsudo systemctl start otelcol-contrib.service\nsudo systemctl status otelcol-contrib.service\n</code></pre></li> </ol> <p>Firewall Example for RHEL9</p> <p>To allow external access to OpenTelemetry ports (replace as needed): <pre><code># Example: RHEL9 firewall commands for OpenTelemetry Collector (if using firewalld)\nsudo firewall-cmd --permanent --add-port=4318/tcp\nsudo firewall-cmd --permanent --add-port=4319/tcp\nsudo firewall-cmd --permanent --add-port=4320/tcp\nsudo firewall-cmd --reload\n</code></pre></p>"},{"location":"Installation/linux/#elasticsearch-installation","title":"Elasticsearch Installation","text":"<ol> <li>Download and extract Elasticsearch:     <pre><code>wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-8.17.3-linux-x86_64.tar.gz\ntar -xzf elasticsearch-8.17.3-linux-x86_64.tar.gz\nsudo mv elasticsearch-8.17.3 /opt/\ncd /opt/elasticsearch-8.17.3/config/\n</code></pre></li> </ol> <p>Elasticsearch Configuration (elasticsearch.yml)</p> <ol> <li> <p>Edit or create <code>elasticsearch.yml</code> (location: <code>/opt/elasticsearch-8.17.3/config/elasticsearch.yml</code>):     <pre><code>sudo nano elasticsearch.yml\n</code></pre>     Sample local development configuration:     <pre><code># ======================== Elasticsearch Configuration =========================\n\ncluster.name: my-local-dev-cluster\nnode.name: node-1\n\n# Security &amp; network settings for local dev ONLY:\nnetwork.host: 0.0.0.0\nhttp.port: 9200\ndiscovery.type: single-node\n\nxpack.security.enabled: false\nxpack.security.enrollment.enabled: false\nxpack.security.http.ssl.enabled: false\nxpack.security.transport.ssl.enabled: false\n</code></pre></p> <p>Leave other default settings as-is or commented. For true single-node development, set <code>discovery.type: single-node</code>, disable all security as above, and bind only to <code>127.0.0.1</code>.</p> </li> </ol> <p>Systemd Service Setup for Elasticsearch</p> <ol> <li> <p>Create a systemd unit file:     <pre><code>sudo nano /etc/systemd/system/elasticsearch.service\n</code></pre>     Content:     <pre><code>[Unit]\nDescription=Elasticsearch\nDocumentation=https://www.elastic.co\nAfter=network.target\n\n[Service]\nType=simple\nUser=projadmin\nGroup=projadmin\nExecStart=/opt/elasticsearch-8.17.3/bin/elasticsearch\nEnvironment=\"ES_JAVA_OPTS=-Xms4g -Xmx4g\"\nWorkingDirectory=/opt/elasticsearch-8.17.3\nRestart=on-failure\nLimitNOFILE=65535\n\n[Install]\nWantedBy=multi-user.target\n</code></pre></p> </li> <li> <p>Enable and start Elasticsearch:     <pre><code>sudo systemctl enable elasticsearch.service\nsudo systemctl start elasticsearch.service\nsudo systemctl status elasticsearch.service\n</code></pre></p> </li> </ol> <p>Elasticsearch Firewall Example for RHEL9</p> <p>To allow HTTP access (default port 9200, for local development only):</p> <pre><code># Example: RHEL9 firewall commands for Elasticsearch (if using firewalld)\nsudo firewall-cmd --permanent --add-port=9200/tcp\nsudo firewall-cmd --reload\n</code></pre>"},{"location":"Installation/linux/#troubleshooting","title":"Troubleshooting","text":"<p>Connection Issues</p> <p>1. Frontend cannot connect to Backend:</p> <ul> <li>Verify <code>REACT_APP_BASE_URL</code> in <code>Infosys-Agentic-Foundry-Frontend/.env</code></li> <li>Check CORS settings in backend</li> <li>Ensure backend server is running and accessible</li> <li>Test: <code>curl http://your-backend-ip:your-backend-port/health</code></li> </ul> <p>2. Cannot access applications from external network:</p> <ul> <li>Check firewall rules: <code>sudo firewall-cmd --list-ports</code></li> <li>Verify HOST is set to <code>0.0.0.0</code> (not <code>localhost</code>)</li> <li>Check VM security group settings</li> </ul> <p>Service Issues</p> <p>3. Service fails to start:</p> <ul> <li>Check service logs: <code>sudo journalctl -u infyagent-backend.service -f</code></li> <li>Verify file paths in service configuration</li> <li>Check user permissions</li> <li>Ensure virtual environment is properly configured</li> </ul> <p>Proxy Issues</p> <p>4. Cannot install packages or clone repositories:</p> <ul> <li>Verify proxy settings: <code>echo $http_proxy</code></li> <li>Check proxy configuration with your administrator</li> <li>Test proxy: <code>curl -I http://google.com</code></li> </ul> <p>Log Locations</p> <ul> <li>Backend service logs: <code>sudo journalctl -u infyagent-backend.service</code></li> <li>Frontend service logs: <code>sudo journalctl -u infyagent-frontend.service</code></li> <li>System logs: <code>/var/log/messages</code></li> </ul> <p>Maintenance</p> <p>Keep your deployment updated:</p> <pre><code># Update project (if using Git)\ngit pull origin main\n\n# Update backend dependencies\ncd Infosys-Agentic-Foundry-Backend\nsource ./.venv/bin/activate\npip install -r requirements.txt\n\n# Update frontend dependencies\ncd Infosys-Agentic-Foundry-Frontend\nnpm install\n\n# Restart services after updates\nsudo systemctl restart infyagent-backend.service\nsudo systemctl restart infyagent-frontend.service\n</code></pre>"},{"location":"Installation/linux/#project-structure","title":"Project Structure","text":"<p>The structure shown below is a sample. The full project includes additional files and directories not listed here.</p> <p>Backend project structure:</p> <pre><code>Infosys-Agentic-Foundry-Backend/\n\u251c\u2500\u2500 src/                  # Source code\n\u2502   \u251c\u2500\u2500 agent_templates/  # Agent onboarding templates and configurations   \n\u2502   \u251c\u2500\u2500 api/              # REST API endpoints and route handlers\n\u2502   \u251c\u2500\u2500 auth/             # Authentication and authorization services\n\u2502   \u251c\u2500\u2500 config/           # Database connectivity and cache configurations\n\u2502   \u251c\u2500\u2500 database/         # Database models, repositories, and services\n\u2502   \u251c\u2500\u2500 file_templates/   # Template files for various operations\n\u2502   \u251c\u2500\u2500 inference/        # AI model inference and agent execution logic\n\u2502   \u251c\u2500\u2500 models/           # Data models and business logic\n\u2502   \u251c\u2500\u2500 prompts/          # Prompt templates for AI interactions\n\u2502   \u251c\u2500\u2500 schemas/          # Pydantic schemas for data validation\n\u2502   \u251c\u2500\u2500 tools/            # Utility tools and helper functions\n\u2502   \u2514\u2500\u2500 utils/            # Common utilities and shared functions\n\u251c\u2500\u2500 .venv/                # Python virtual environment (auto-generated)\n\u251c\u2500\u2500 requirements.txt      # Python dependencies specification\n\u251c\u2500\u2500 main.py               # Application entry point\n\u251c\u2500\u2500 run_server.py         # Development server runner with additional options\n\u251c\u2500\u2500 .env                  # Environment variables (create from .env.example)\n\u251c\u2500\u2500 .env.example          # Template for environment configuration\n\u2514\u2500\u2500 README.md             # Project documentation\n</code></pre> <p>Frontend project structure:</p> <pre><code>Infosys-Agentic-Foundry-Frontend/  # React frontend application\n\u251c\u2500\u2500 .github/                       # GitHub configuration\n\u251c\u2500\u2500 node_modules/                  # Node.js dependencies (generated)\n\u251c\u2500\u2500 public/                        # Static assets\n\u251c\u2500\u2500 src/                           # React source code\n\u2502   \u251c\u2500\u2500 Assets/                    # Image and media assets\n\u2502   \u251c\u2500\u2500 components/                # React components\n\u2502   \u251c\u2500\u2500 context/                   # React context providers\n\u2502   \u251c\u2500\u2500 css_modules/               # CSS module files\n\u2502   \u251c\u2500\u2500 Hooks/                     # Custom React hooks\n\u2502   \u251c\u2500\u2500 Icons/                     # SVG icons and graphics\n\u2502   \u251c\u2500\u2500 services/                  # API service functions\n\u2502   \u251c\u2500\u2500 App.js                     # Main App component with routing\n\u2502   \u251c\u2500\u2500 constant.js                # Configuration constants (BASE_URL, APIs)\n\u2502   \u251c\u2500\u2500 index.js                   # Entry point\n\u2502   \u251c\u2500\u2500 index.css                  # Global styles\n\u2502   \u2514\u2500\u2500 ProtectedRoute.js          # Route protection component\n\u251c\u2500\u2500 package.json                   # Node.js dependencies and scripts\n\u251c\u2500\u2500 package-lock.json             # Lock file for dependencies\n\u251c\u2500\u2500 README.md                      # Project documentation\n\u2514\u2500\u2500 .env.example                          # Environment variables (not shown but referenced)\n</code></pre> <p>Default Commands with Placeholders:</p> <pre><code># Backend startup\npython run_server.py --host 0.0.0.0 --port your-backend-port `or`\npython main.py --host 0.0.0.0 --port your-backend-port\n\n# Frontend access\nhttp://your-vm-ip:your-frontend-port\n\n# Backend API access\nhttp://your-vm-ip:your-backend-port/docs\n</code></pre> <p>Remember to replace all placeholder values with your actual IP addresses, ports, usernames, and paths before deployment!</p>"},{"location":"Installation/windows/","title":"Windows Setup Guide","text":"<p>This guide provides step-by-step instructions for setting up and running the project with React UI on a Windows VM (Virtual Machine).</p>"},{"location":"Installation/windows/#project-overview","title":"Project Overview","text":"<p>This project consists of a backend server built with FastAPI and a frontend interface using React. Follow the instructions below to get it up and running on your VM.</p>"},{"location":"Installation/windows/#prerequisites","title":"Prerequisites","text":"<p>Before proceeding, ensure you have the following installed:</p>"},{"location":"Installation/windows/#required-software","title":"Required Software","text":"<ul> <li>Python 3.11 or higher</li> <li>pip (Python package manager)</li> <li>Git (optional, for cloning the repository)</li> <li>A code editor (e.g., VS Code is recommended, but any editor of your choice will work)</li> <li>NodeJS version 22 or above  </li> <li>NPM version 10.9.2 or above ( comes bundled with NodeJs) </li> <li>Redis 8.2.1</li> <li>Postgres 17</li> </ul> <p>To verify your Node.js and npm installations, open Terminal and run:</p> <pre><code>node -v\nnpm -v\n</code></pre> <ul> <li>If Node.js is installed, running the version command will display the installed version.</li> <li>If Node.js is not installed, you will see an error in the terminal such as <code>\"node\" is not recognized as an internal or external command</code>.</li> </ul> <p>For Local Windows Setup</p> <ul> <li>To install Node.js: <ul> <li>Search for NodeJs  </li> <li>Choose version 22 or any higher stable version.</li> <li>Install Node.js (NPM comes bundled with Node.js).</li> </ul> </li> <li>After installation, open your command prompt or terminal.</li> <li>Run <code>node -v</code> to confirm that it shows version 22 or higher.</li> </ul>"},{"location":"Installation/windows/#setting-up-proxy-in-system-environment-variables-if-required","title":"Setting Up Proxy in System Environment Variables (If Required)","text":"<p>If your network requires a proxy to access the internet, follow these steps to set proxy values as system environment variables:</p> <p>Steps:</p> <ol> <li>Open the Start Menu and search for Environment Variables.</li> <li>Click on \"Edit the system environment variables\".</li> <li>In the System Properties window, click on the \"Environment Variables\" button.</li> <li>Under the System variables section, click New.</li> <li>Create the following two variables (ask your VM creator):</li> </ol> <pre><code>   Variable name: http_proxy\n   Variable value: `&lt;your_proxy&gt;`\n\n   Variable name: https_proxy\n   Variable value: `&lt;your_proxy&gt;`\n</code></pre> <ol> <li>Click OK to save and close all windows.</li> <li>Restart your Command Prompt or system (if needed) for the changes to take effect.</li> </ol>"},{"location":"Installation/windows/#download-the-backend-project-code","title":"Download the Backend Project Code","text":"<p>You can obtain the project files using one of the following methods:</p> <p>Option 1: Clone Using Git</p> <p>If you have Git installed, open Terminal and run:</p> <pre><code>git clone https://github.com/Infosys/Infosys-Agentic-Foundry\ncd Infosys-Agentic-Foundry\n</code></pre> <ul> <li>The git clone command will create a new folder named \"Infosys-Agentic-Foundry\" in your current directory and download all repository files into it.</li> <li>The cd command navigates into the newly created folder.</li> </ul> <p>Option 2: Download Zip from GitHub</p> <ol> <li>Navigate to the repository in your web browser: https://github.com/Infosys/Infosys-Agentic-Foundry</li> <li>Click the green \"Code\" button</li> <li>Select \"Download Zip\"</li> <li>Extract the Zip file to your preferred location on your machine</li> </ol>"},{"location":"Installation/windows/#download-the-frontend-project-code","title":"Download the Frontend Project Code","text":"<p>You can obtain the project files using one of the following methods:</p> <p>Option 1: Clone Using Git</p> <p>If you have Git installed, open Terminal and run:</p> <pre><code>git clone https://github.com/Infosys/Infosys-Agentic-Foundry\ncd Infosys-Agentic-Foundry-Frontend\n</code></pre> <ul> <li>The git clone command will create a new folder named \"Infosys-Agentic-Foundry-Frontend\" in your current directory and download all repository files into it.</li> <li>The cd command navigates into the newly created folder.</li> </ul> <p>Option 2: Download Zip from GitHub</p> <ol> <li>Navigate to the repository in your web browser: https://github.com/Infosys//Infosys-Agentic-Foundry-Frontend.git</li> <li>Click the green \"Code\" button</li> <li>Select \"Download Zip\"</li> <li>Extract the Zip file to your preferred location on your machine</li> </ol>"},{"location":"Installation/windows/#setup-and-installation","title":"Setup and Installation","text":"<p>Open in Code Editor</p> <ol> <li>Open Visual Studio Code or your preferred code editor</li> <li>Select File &gt; Open Folder</li> <li>Navigate to and select the project directory</li> </ol>"},{"location":"Installation/windows/#setting-up-the-backend-environment","title":"Setting Up the Backend Environment","text":"<p>Follow these steps in Terminal (opened in the project directory):</p> <p>1. Create a Virtual Environment for the Backend:</p> <pre><code>cd Infosys-Agentic-Foundry-Backend\npython -m venv .venv\n</code></pre> <p>This creates a virtual environment named <code>.venv</code> in the Infosys-Agentic-Foundry-Backend directory.</p> <p>2. Activate the Virtual Environment:</p> <pre><code>.\\.venv\\Scripts\\activate\n</code></pre> <p>When activated successfully, you'll see <code>(.venv)</code> at the beginning of your command prompt.</p> <p>3. Install Backend Dependencies:</p> <p>With the virtual environment activated:</p> <pre><code>pip install uv\nuv pip install -r requirements.txt\n</code></pre> <p>This will install all the necessary Python packages listed in the <code>requirements.txt</code> file.</p> <p>If you face any SSL error issue, use the below command:</p> <pre><code>pip install -r requirements.txt --trusted-host pypi.org --trusted-host files.pythonhosted.org\n</code></pre>"},{"location":"Installation/windows/#setting-up-the-frontend-environment","title":"Setting Up the Frontend Environment","text":"<p>In a new Terminal window, navigate to the project's frontend directory:</p> <pre><code>cd Infosys-Agentic-Foundry-Frontend\n</code></pre> <p>Install Frontend Dependencies</p> <p>After cloning or pulling the UI code from GitHub, delete the <code>package-lock.json</code> file (if present) before installing dependencies. This helps avoid potential conflicts.</p> <ul> <li>Open a terminal in the <code>Infosys-Agentic-Foundry-Frontend</code> directory.</li> <li>Delete the <code>package-lock.json</code> file if it exists:     <pre><code>del package-lock.json\n</code></pre></li> <li>Run the following command to install all required packages:     <pre><code>npm install\n</code></pre></li> <li>You can also use the shorthand:     <pre><code>npm i\n</code></pre></li> <li>To see detailed progress during installation, use:     <pre><code>npm install --verbose\n</code></pre></li> <li>Wait for the installation to complete. This may take a few minutes.</li> </ul>"},{"location":"Installation/windows/#configuration-setup","title":"Configuration Setup","text":"<p>Frontend Configuration (.env file)</p> <p>Open the <code>.env</code> file in your frontend project (<code>Infosys-Agentic-Foundry-Frontend/.env</code>) and set the base URL for the API server:</p> <pre><code># Base URL for the API server\nREACT_APP_BASE_URL=`&lt;your_backend_api_url&gt;`\n</code></pre> <p>Backend Configuration (.env file)</p> <p>Open the <code>.env</code> file in your backend project (<code>Infosys-Agentic-Foundry-Backend/.env</code>) and set the allowed frontend origins:</p> <pre><code># Add your frontend IP address\nUI_CORS_IP=`&lt;your_ui_url&gt;`\n\n# Add your frontend IP with port number\nUI_CORS_IP_WITH_PORT=`&lt;your_ui_url:port&gt;`\n</code></pre> <p>Model Server Setup</p> <p>For detailed instructions on deploying and configuring your model server, refer to the Model Server Deployment guide.</p> <p>CORS Origins List If you want to let other UI connect to the backend, In <code>run_server.py</code> (or <code>main.py</code>), you will find an <code>origins</code> list:</p> <pre><code>origins = [\n    \"http://localhost\",\n    \"http://localhost:3000\",\n    \"null\",\n    # Add other origins as needed, such as your deployed frontend URL\n]\n</code></pre> <ul> <li>If you want to allow connections to the backend from other hosts, add their IP and port numbers to this <code>origins</code> list.</li> </ul> <p>Make sure to update the <code>.env</code> files with your API keys and other required configuration values.</p>"},{"location":"Installation/windows/#running-the-project","title":"Running the Project","text":"<p>1. Start the Backend Server</p> <p>In the Terminal with the active virtual environment:</p> <pre><code>cd Infosys-Agentic-Foundry-Backend  # If not already in the backend directory\npython run_server.py --host 0.0.0.0 --port `&lt;your_port_number&gt;` `or`\npython main.py --host 0.0.0.0 --port `&lt;your_port_number&gt;`\n</code></pre> <p>Backend Server Details</p> <ul> <li>Server: FastAPI (run via Python)</li> <li>Module: <code>run_server.py</code> or <code>main.py</code></li> <li>Host: 0.0.0.0 (accessible from any network interface)</li> <li>Port: 5001</li> </ul> <p>Once running, you can access the backend API at http://localhost:5001</p> <p>2. Start the React Frontend</p> <p>In a new Terminal window:</p> <pre><code>cd Infosys-Agentic-Foundry-Frontend  # If not already in the frontend directory\nnpm start\n</code></pre> <p>Frontend Server Details</p> <ul> <li>Development Server: The React development server is used for local development and testing.<ul> <li>Default Port: Runs on port <code>3000</code> by default.</li> <li>Hot Reloading: Automatically reloads the page when you make changes to the source code.</li> <li>Custom Port: To run the frontend on a different port, open PowerShell and run:   <pre><code>$env:PORT=3003; npm start\n</code></pre></li> <li>This command starts the React development server on port <code>3003</code> instead of the default <code>3000</code>.</li> </ul> </li> </ul> <p>The React development server will start and automatically open http://localhost:3000 in your default web browser.</p>"},{"location":"Installation/windows/#how-to-make-the-server-run-247-on-windows-using-nssm","title":"How to Make the Server Run 24/7 on Windows Using NSSM","text":"<p>Step 1: Create a Batch File</p> <p>Create a new file named <code>servers.bat</code> (you can choose any name).</p> <p>Paste the following content into the file:</p> <pre><code>@echo off\nset PYTHONIOENCODING=utf-8\n\nREM Activate Python virtual environment\ncd /d \"C:\\Infosys-Agentic-Foundry\\Infosys-Agentic-Foundry-Backend\\.venv\\Scripts\"\ncall activate.bat\n\nREM Change to backend project directory\ncd /d \"C:\\Infosys-Agentic-Foundry\\Infosys-Agentic-Foundry-Backend\"\n\nREM Start Phoenix Server\nstart \"\" cmd /k \"python -m phoenix.server.main serve\"\n\nREM Ensure logs directory exists\nif not exist logs mkdir logs\n\nREM Get today's date in YYYY-MM-DD format\nfor /f %%i in ('powershell -Command \"Get-Date -Format yyyy-MM-dd\"') do set datetime=%%i\n\nREM Start FastAPI backend and log to logs\\server_YYYY-MM-DD.log\nstart cmd /k \"python run_server.py --host 0.0.0.0 --port &lt;your_port_number&gt; &gt;&gt; logs\\server_%datetime%.log 2&gt;&amp;1\"\n\nREM Start Node.js frontend \ncd /d \"C:\\Infosys-Agentic-Foundry\\Infosys-Agentic-Foundry-Frontend\"\nstart cmd /k \"npm start\"\n\nREM Or if you want specific port number run the below command:\nstart cmd /k \"set PORT=&lt;your_port_number&gt; &amp;&amp; npm start\"\n\npause\n</code></pre> <p>Important</p> <p>Make sure the file paths match your system's folder structure.</p> <p>Tip</p> <p>Save the file somewhere accessible, like your desktop or project root.</p> <p>Step 2: Install and Configure NSSM </p> <ol> <li>Download NSSM if not already installed: https://nssm.cc/download</li> <li> <p>Open Command Prompt as Administrator. </p> </li> <li> <p>Run this command to open the NSSM setup:</p> </li> </ol> <pre><code>nssm install infy_agent.service\n</code></pre> <p>Custom Service Name</p> <p>You can replace <code>infy_agent.service</code> with any name you prefer, such as <code>my_server_service</code>, <code>webstack_service</code>, or <code>custom_backend</code>. Just make sure to use the same name consistently in all subsequent commands.</p> <ol> <li>In the NSSM GUI:</li> <li>For Application path, browse and select your <code>servers.bat</code> file.</li> <li>Click Install Service.</li> </ol> <p>Step 3: Manage the Service</p> <p>Use these commands from the terminal (as Administrator):</p> <p>Start the service: <pre><code>nssm start infy_agent.service\n</code></pre></p> <p>Stop the service: <pre><code>nssm stop infy_agent.service\n</code></pre></p> <p>Edit the service: <pre><code>nssm edit infy_agent.service\n</code></pre></p> <p>You can also go to Windows Services (press <code>Win + R</code>, type <code>services.msc</code>) to start, stop, or set the service to run automatically on startup.</p>"},{"location":"Installation/windows/#project-structure","title":"Project Structure","text":"<p>The structure shown below is a sample. The full project includes additional files and directories not listed here.</p> <p>Backend project structure:</p> <pre><code>Infosys-Agentic-Foundry-Backend/\n\u251c\u2500\u2500 src/                  # Source code\n\u2502   \u251c\u2500\u2500 agent_templates/  # Agent onboarding templates and configurations   \n\u2502   \u251c\u2500\u2500 api/              # REST API endpoints and route handlers\n\u2502   \u251c\u2500\u2500 auth/             # Authentication and authorization services\n\u2502   \u251c\u2500\u2500 config/           # Database connectivity and cache configurations\n\u2502   \u251c\u2500\u2500 database/         # Database models, repositories, and services\n\u2502   \u251c\u2500\u2500 file_templates/   # Template files for various operations\n\u2502   \u251c\u2500\u2500 inference/        # AI model inference and agent execution logic\n\u2502   \u251c\u2500\u2500 models/           # Data models and business logic\n\u2502   \u251c\u2500\u2500 prompts/          # Prompt templates for AI interactions\n\u2502   \u251c\u2500\u2500 schemas/          # Pydantic schemas for data validation\n\u2502   \u251c\u2500\u2500 tools/            # Utility tools and helper functions\n\u2502   \u2514\u2500\u2500 utils/            # Common utilities and shared functions\n\u251c\u2500\u2500 .venv/                # Python virtual environment (auto-generated)\n\u251c\u2500\u2500 requirements.txt      # Python dependencies specification\n\u251c\u2500\u2500 main.py               # Application entry point\n\u251c\u2500\u2500 run_server.py         # Development server runner with additional options\n\u251c\u2500\u2500 .env                  # Environment variables (create from .env.example)\n\u251c\u2500\u2500 .env.example          # Template for environment configuration\n\u2514\u2500\u2500 README.md             # Project documentation\n</code></pre> <p>Frontend project structure:</p> <pre><code>Infosys-Agentic-Foundry-Frontend/                    # React frontend application\n\u251c\u2500\u2500 .github/                       # GitHub configuration\n\u251c\u2500\u2500 node_modules/                  # Node.js dependencies (generated)\n\u251c\u2500\u2500 public/                        # Static assets\n\u251c\u2500\u2500 src/                           # React source code\n\u2502   \u251c\u2500\u2500 Assets/                    # Image and media assets\n\u2502   \u251c\u2500\u2500 components/                # React components\n\u2502   \u251c\u2500\u2500 context/                   # React context providers\n\u2502   \u251c\u2500\u2500 css_modules/               # CSS module files\n\u2502   \u251c\u2500\u2500 Hooks/                     # Custom React hooks\n\u2502   \u251c\u2500\u2500 Icons/                     # SVG icons and graphics\n\u2502   \u251c\u2500\u2500 services/                  # API service functions\n\u2502   \u251c\u2500\u2500 App.js                     # Main App component with routing\n\u2502   \u251c\u2500\u2500 constant.js                # Configuration constants (BASE_URL, APIs)\n\u2502   \u251c\u2500\u2500 index.js                   # Entry point\n\u2502   \u251c\u2500\u2500 index.css                  # Global styles\n\u2502   \u2514\u2500\u2500 ProtectedRoute.js          # Route protection component\n\u251c\u2500\u2500 package.json                   # Node.js dependencies and scripts\n\u251c\u2500\u2500 package-lock.json              # Lock file for dependencies\n\u251c\u2500\u2500 README.md                      # Project documentation\n\u2514\u2500\u2500 .env.example                   # Environment variables (not shown but referenced)\n</code></pre>"},{"location":"Installation/windows/#troubleshooting","title":"Troubleshooting","text":"<p>Connection Issues Between Frontend and Backend</p> <p>If your React UI cannot connect to the backend:</p> <p>1. Verify BASE_URL Configuration: </p> <ul> <li>Check that <code>Infosys-Agentic-Foundry-Frontend\\.env</code> has the correct backend URL</li> <li>Ensure the IP address and port match your backend server</li> </ul> <p>2. Check Backend CORS Settings: </p> <ul> <li>Confirm your frontend URL is included in the origins list</li> <li>Restart the backend server after making CORS changes</li> </ul> <p>3. Network Connectivity: </p> <ul> <li>Test if you can access the backend URL directly in your browser</li> <li>Verify both services are running on the expected ports</li> </ul> <p>4. Browser Console Errors:</p> <ul> <li>Open browser developer tools and check for CORS or network errors</li> <li>Look for specific error messages that can guide troubleshooting</li> </ul> <p>Common Issues</p> <ul> <li>Port Already in Use: If you get a port error, either stop the conflicting service or use a different port</li> <li>CORS Errors: Ensure the frontend URL is properly added to the backend's origins list</li> <li>Module Not Found: Verify all dependencies are installed and virtual environments are activated</li> </ul>"},{"location":"Telemetry/Arize_Phoenix/","title":"Arize Phoenix Overview","text":""},{"location":"Telemetry/Arize_Phoenix/#what-is-arize-phoenix","title":"What is Arize Phoenix?","text":"<p>Arize Phoenix is a comprehensive observability and tracing platform designed for AI applications, particularly those built with LangChain. It provides real-time monitoring, trace visualization, and performance analysis capabilities for machine learning and AI systems.</p>"},{"location":"Telemetry/Arize_Phoenix/#why-use-arize-phoenix","title":"Why Use Arize Phoenix?","text":"<p>Arize Phoenix addresses critical challenges in AI application development and deployment by providing comprehensive observability solutions. Modern AI systems, especially those involving large language models and complex agent workflows, require sophisticated monitoring to ensure optimal performance, reliability, and cost efficiency.</p> <p>Enhanced Debugging and Troubleshooting</p> <p>Phoenix enables developers to trace execution paths through complex AI workflows, identifying bottlenecks, errors, and unexpected behaviors that would be difficult to detect through traditional logging methods.</p> <p>Performance Optimization</p> <p>By providing detailed insights into resource usage, token consumption, and execution times, Phoenix helps teams optimize their AI applications for better performance and reduced operational costs.</p> <p>Production Readiness</p> <p>The platform ensures AI applications are production-ready by offering real-time monitoring, alerting capabilities, and comprehensive system health tracking that's essential for maintaining reliable AI services.</p> <p>Model Evaluation and Comparison</p> <p>Phoenix facilitates systematic comparison of different models, configurations, and implementations, enabling data-driven decisions about which approaches work best for specific use cases.</p>"},{"location":"Telemetry/Arize_Phoenix/#core-components","title":"Core Components","text":"<p>Phoenix Library</p> <p>The core Phoenix library serves as the foundation for observability, enabling automatic instrumentation and trace collection across your AI applications. It integrates seamlessly with popular frameworks and provides comprehensive monitoring capabilities.</p> <p>OpenInference Instrumentation</p> <p>Phoenix includes specialized instrumentation for LangChain applications, automatically capturing detailed trace information without requiring manual intervention. This allows developers to gain insights into their AI workflows with minimal code changes.</p>"},{"location":"Telemetry/Arize_Phoenix/#trace-recording-architecture","title":"Trace Recording Architecture","text":"<p>Phoenix offers flexible trace recording methods to accommodate different application architectures and deployment scenarios. The platform supports both direct import patterns for simple implementations and project context managers for more complex, multi-project environments.</p> <p>Direct Registration</p> <p>This method provides straightforward trace collection by directly registering the Phoenix instrumentation within your application code. It's ideal for single-service applications or when you need immediate trace collection.</p> <p>Project Context Management</p> <p>For more sophisticated applications, Phoenix supports project-based trace organization through context managers. This approach allows you to group related traces under specific project identifiers, making it easier to analyze complex systems with multiple components.</p>"},{"location":"Telemetry/Arize_Phoenix/#storage-and-configuration","title":"Storage and Configuration","text":"<p>Database Support</p> <p>Phoenix provides flexible storage options ranging from lightweight SQLite databases for development environments to robust PostgreSQL configurations for production deployments. The platform automatically handles trace persistence and retrieval.</p> <p>Network Configuration</p> <p>Phoenix operates through configurable network ports, with GRPC endpoints for trace collection and HTTP endpoints for web interface access. The system integrates with OpenTelemetry standards for seamless trace data exchange.</p>"},{"location":"Telemetry/Arize_Phoenix/#project-organization","title":"Project Organization","text":"<p>Multi-Project Support</p> <p>Phoenix excels at managing multiple projects simultaneously, allowing organizations to monitor different services, applications, or environments from a single dashboard. Each project maintains its own trace history and configuration settings.</p> <p>Service Isolation</p> <p>The platform provides clear separation between different services and applications, enabling teams to focus on specific components while maintaining visibility into the broader system architecture.</p>"},{"location":"Telemetry/Arize_Phoenix/#monitoring-and-analysis-features","title":"Monitoring and Analysis Features","text":"<p>Real-Time Trace Visualization</p> <p>Phoenix offers comprehensive trace visualization capabilities, displaying complete request flows, execution paths, and performance metrics in real-time. This enables rapid identification of bottlenecks and optimization opportunities.</p> <p>Agent Performance Tracking</p> <p>The platform specifically supports AI agent monitoring, tracking individual agent behaviors, decision-making processes, and performance characteristics. This is particularly valuable for evaluating different model configurations or comparing agent implementations.</p> <p>Resource Usage Analysis</p> <p>Phoenix provides detailed insights into resource consumption, including token usage tracking, cost analysis, and performance metrics. This information is crucial for optimizing AI applications and managing operational expenses.</p> <p>Input/Output Analysis</p> <p>The system captures and presents detailed input and output data for each trace, facilitating debugging, quality assurance, and system optimization. This comprehensive data collection enables thorough analysis of AI system behavior.</p>"},{"location":"Telemetry/Arize_Phoenix/#web-interface-capabilities","title":"Web Interface Capabilities","text":"<p>Centralized Dashboard</p> <p>Phoenix provides a web-based interface that serves as a central hub for all monitoring activities. The dashboard offers project overview, system health indicators, and quick access to detailed trace information.</p> <p>Performance Metrics</p> <p>The interface displays comprehensive performance metrics including latency measurements, throughput analysis, and error rate tracking. These metrics help identify trends and potential issues before they impact production systems.</p> <p>Comparative Analysis</p> <p>Phoenix enables side-by-side comparison of different models, configurations, or time periods, making it easier to evaluate system improvements and identify optimal configurations for specific use cases.</p>"},{"location":"Telemetry/Connecting%20to%20Grafana/","title":"Connecting to Grafana and Creating Visualizations from Elasticsearch","text":"<p>This guide will walk you through the process of connecting Grafana to your Elasticsearch data source, configuring it, and creating visualizations and dashboards. Each step is broken down to help you set up the integration seamlessly.</p>"},{"location":"Telemetry/Connecting%20to%20Grafana/#1-provide-elasticsearch-endpoint-url-for-connection","title":"1. Provide Elasticsearch Endpoint URL for Connection","text":"<p> To begin, you must specify the Elasticsearch Endpoint URL. This URL is where Grafana will look for your Elasticsearch instance. Grafana needs this URL to establish a connection and retrieve data from Elasticsearch.</p> <p>Steps:</p> <ul> <li>Open Grafana and navigate to the Data Sources section.</li> <li>Select Add Data Source, then choose Elasticsearch from the list of available sources.</li> <li>In the HTTP URL field, enter the endpoint URL of your Elasticsearch instance.</li> <li>Click Save &amp; Test to verify the connection.</li> </ul> <p>Once the connection is successful, Grafana will confirm that the endpoint is accessible and can pull data from your Elasticsearch server.</p>"},{"location":"Telemetry/Connecting%20to%20Grafana/#2-provide-elasticsearch-details","title":"2. Provide Elasticsearch Details","text":"<p> After establishing the connection, you need to configure the details specific to your Elasticsearch data. These include the Index Name, Message Field Name, and Level Field Name.</p> <ul> <li> <p>Index Name: The index name should exactly match the one defined in your Elasticsearch configuration. It tells Grafana which index to pull data from within your Elasticsearch cluster. If you have multiple indices or time-based indices (e.g., <code>agentic-foundry-tool-log</code>), specify the exact name or pattern here.</p> </li> <li> <p>Message Field Name: This refers to the field in the Elasticsearch index that contains the actual log message or data content that you want to visualize in Grafana. For example, this might be <code>message</code> or <code>log</code> depending on your Elasticsearch mappings.</p> </li> <li> <p>Level Field Name: The level field represents the severity level of logs or events, such as <code>INFO</code>, <code>WARN</code>, <code>ERROR</code>, or any custom field you use for classification. Ensure the field you provide matches the one used in your Elasticsearch index for filtering and aggregation.</p> </li> </ul> <p>Steps:</p> <ul> <li>In Grafana's Data Source configuration page, you\u2019ll see fields for Index Name, Message Field, and Level Field.</li> <li>Fill in the respective names that match your Elasticsearch configuration.</li> <li>After entering the information, click Save &amp; Test again to ensure everything is set up correctly.</li> </ul> <p>Once saved, Grafana will store these details and use them to pull the correct data during visualizations and dashboard creation.</p>"},{"location":"Telemetry/Connecting%20to%20Grafana/#3-adding-visualizations","title":"3. Adding Visualizations","text":"<p> With the connection established and data source configured, you are now ready to add visualizations to your Grafana dashboard. Visualizations are charts, graphs, or tables that will display your data in meaningful ways.</p> <p></p> <p>Steps:</p> <ul> <li>From the Grafana homepage, click on the Plus (+) symbol located on the left-hand sidebar.</li> <li>Select Dashboard from the options that appear.</li> <li>A new, empty dashboard will open. Click on Add new panel.</li> <li>In the panel configuration page, select the Data Source you created in Step 2.</li> <li>Choose a visualization type (e.g., Graph, Time series, Table, etc.).</li> <li>Grafana will automatically query Elasticsearch using the configured Index Name and fields and display the results in the chosen visualization format.</li> </ul> <p>Once you\u2019ve configured your panel, click Save to add it to your dashboard. You can repeat this process to add multiple visualizations to your dashboard, each representing different aspects of your data.</p> <p>After adding a data source, the dashboard will be updated to display the newly integrated data. From here, you can create a wide range of visualizations based on the available data.</p> <p></p> <p>Additional Configurations:</p> <ul> <li>Filters: Grafana allows you to use Filters to narrow down the data displayed. For example, you can filter logs by date, severity level, or specific keywords.</li> <li>Variables: You can define Variables (e.g., <code>session_id</code>, <code>action_id</code>, or <code>action_on</code>) that can be used as dynamic filters across multiple panels. This allows for interactive and flexible dashboards where users can change the value of a variable and see the dashboard update accordingly.</li> </ul>"},{"location":"Telemetry/Connecting%20to%20Grafana/#4-filters-and-variables","title":"4. Filters and Variables","text":"<p>To make your dashboards more dynamic, Grafana supports the use of Filters and Variables. These allow you to control what data is displayed in real-time, making it easy to customize and adjust visualizations on the fly.</p> <p></p> <p>Filters: Filters are conditions that can be applied to specific fields in the Elasticsearch index. They are especially useful for narrowing down data, such as:</p> <ul> <li>Displaying only logs from a particular time range.</li> <li>Filtering logs based on severity level (e.g., showing only <code>ERROR</code> logs).</li> <li>Displaying data related to specific services or hosts.</li> </ul> <p>Variables: Variables in Grafana let you create dynamic dashboards that can adapt based on user input. Once a variable is defined, it can be used in the queries that power your visualizations. This makes it easy for users to filter data on the fly without modifying the queries directly.</p> <p>Steps:</p> <ul> <li>Navigate to Dashboard Settings (the gear icon on the top-right corner of your dashboard).</li> <li>Select Variables from the menu.</li> <li>Create a new variable (e.g., <code>Host</code>, <code>Service</code>, <code>Log Level</code>) and define its options (e.g., available hosts or log levels).</li> <li>Use this variable in your queries to make your dashboards more interactive.</li> </ul> <p>For example, you could create a variable <code>Host</code> that lets users select the host from a dropdown, and the graphs on the dashboard will automatically update to show data for that selected host.</p>"},{"location":"Telemetry/Connecting%20to%20Grafana/#5-sample-final-dashboard","title":"5. Sample Final Dashboard","text":"<p>Once all the visualizations are configured, your Grafana dashboard should look something like the following example, where you can interact with various filters, variables, and different types of graphs.</p> <p>Final Output: </p> <p>By using the Filters and Variables, users can interactively explore the data, focusing on different aspects like time periods, severity levels, and specific hosts or services.</p> <p>Dashboard Severity Filter Options</p> <p>In the final dashboard, we have additional filtering capabilities under the Severity section. Instead of selecting all severity levels at once, users can filter and view the dashboard data separately based on the following four options:</p> <ul> <li>Error</li> <li>Debug</li> <li>Info</li> <li>Warn</li> </ul> <p></p>"},{"location":"Telemetry/Connecting_to_Grafana/","title":"Grafana and Elasticsearch Integration","text":"<p>This document provides a comprehensive technical overview of integrating Grafana with Elasticsearch for data visualization and monitoring purposes.</p>"},{"location":"Telemetry/Connecting_to_Grafana/#about-grafana","title":"About Grafana","text":"<p>Grafana is an open-source analytics and interactive visualization web application that provides charts, graphs, and alerts for monitoring and observability. It connects to various data sources including Elasticsearch, allowing users to create rich, interactive dashboards for data analysis and real-time monitoring.</p>"},{"location":"Telemetry/Connecting_to_Grafana/#elasticsearch-data-source-configuration","title":"Elasticsearch Data Source Configuration","text":"<p>Elasticsearch Endpoint URL: </p> <p>The primary connection point that defines where Grafana communicates with the Elasticsearch cluster. This URL serves as the gateway for all data retrieval operations and must be accessible from the Grafana instance.</p> <p>Index Configuration: </p> <p>The index name specification is critical as it determines which dataset Grafana will query. For time-based logging systems, indices often follow patterns like <code>agentic-foundry-tool-log</code> or use time-based naming conventions. The index name must exactly match the Elasticsearch configuration to ensure proper data retrieval.</p> <p>Field Mapping Configuration:</p> <ul> <li>Message Field Name: Specifies the field containing the primary log content or data payload within the Elasticsearch documents. This field typically contains the actual log messages, event descriptions, or data content that will be displayed in visualizations.</li> <li>Level Field Name: Defines the field used for log severity classification, containing values such as <code>INFO</code>, <code>WARN</code>, <code>ERROR</code>, or <code>DEBUG</code>. This field enables filtering and aggregation based on event severity levels.</li> </ul>"},{"location":"Telemetry/Connecting_to_Grafana/#visualization-capabilities","title":"Visualization Capabilities","text":"<p>Panel Types and Data Representation</p> <p>Grafana supports multiple visualization types when working with Elasticsearch data:</p> <p>Time Series Visualizations: </p> <p>Ideal for displaying log volumes, error rates, and performance metrics over time. These visualizations automatically handle time-based data from Elasticsearch indices.</p> <p>Table Visualizations: </p> <p>Provide tabular representation of log data, allowing users to view individual records, search through messages, and analyze detailed event information.</p> <p>Graph Visualizations: </p> <p>Enable trend analysis, comparative views, and statistical representations of data patterns extracted from Elasticsearch queries.</p> <p>Query Processing and Data Retrieval</p> <p>Grafana automatically constructs Elasticsearch queries based on the configured data source parameters. The system handles:</p> <ul> <li>Lucene Query Syntax: Grafana translates user inputs into proper Elasticsearch query syntax</li> <li>Aggregation Operations: Supports various aggregation types including terms, date histograms, and metric aggregations</li> <li>Time Range Filtering: Automatically applies time-based filters to queries based on dashboard time selections</li> </ul>"},{"location":"Telemetry/Connecting_to_Grafana/#advanced-dashboard-features","title":"Advanced Dashboard Features","text":"<p>Dynamic Filtering and Variables</p> <p>Template Variables: Enable dynamic dashboard behavior by creating parameterized queries. Common variable types include:</p> <ul> <li><code>session_id</code>: For tracking specific user sessions</li> <li><code>action_id</code>: For filtering based on specific actions or events</li> <li><code>action_on</code>: For categorizing actions by target objects or systems</li> </ul> <p>Interactive Filtering: Dashboards support real-time filtering capabilities allowing users to:</p> <ul> <li>Apply date range filters to narrow down time periods</li> <li>Filter by severity levels (ERROR, DEBUG, INFO, WARN)</li> <li>Search within specific fields or message content</li> <li>Apply multiple filter conditions simultaneously</li> </ul> <p>Severity-Based Data Segmentation</p> <p>The dashboard architecture supports granular severity-based filtering with four primary classification levels:</p> <p>Error Level: Captures critical system failures, exceptions, and error conditions that require immediate attention.</p> <p>Debug Level: Contains detailed diagnostic information useful for troubleshooting and development purposes.</p> <p>Info Level: Provides general informational messages about system operations and normal processing activities.</p> <p>Warn Level: Indicates potential issues or unusual conditions that don't constitute errors but warrant monitoring.</p>"},{"location":"Telemetry/Connecting_to_Grafana/#technical-architecture-benefits","title":"Technical Architecture Benefits","text":"<p>Real-Time Data Processing</p> <p>The Grafana-Elasticsearch integration provides near real-time data visualization capabilities, automatically refreshing dashboards as new data arrives in Elasticsearch indices. This enables continuous monitoring and immediate visibility into system behavior.</p> <p>Scalability and Performance</p> <p>The integration leverages Elasticsearch's distributed architecture and query optimization capabilities, allowing dashboards to handle large volumes of log data efficiently. Grafana's query caching and optimization features further enhance performance for frequently accessed visualizations.</p> <p>Flexibility and Customization</p> <p>The system supports extensive customization options including custom query builders, variable-driven dashboards, and conditional formatting. Users can create complex analytical views tailored to specific monitoring requirements and operational needs.</p>"},{"location":"Telemetry/configurations/","title":"Configurations","text":""},{"location":"Telemetry/configurations/#configurations-for-connecting-with-opentelemetry-collector-and-elastic-search","title":"configurations for connecting with  opentelemetry collector and elastic search.","text":"<pre><code># ======================== Elasticsearch Configuration =========================\n# ... (all the commented out default settings can remain commented) ...\n\n# ---------------------------------- Cluster -----------------------------------\ncluster.name: my-local-dev-cluster # Give it a simple name\n\n# ------------------------------------ Node ------------------------------------\nnode.name: node-1 # Simple node name\n\n# ----------------------------------- Paths ------------------------------------\n# It's good practice to define these, even if using defaults,\n# especially if you run multiple ES instances later.\n# Default paths are usually within the ES installation directory.\n# path.data: data\n# path.logs: logs\n\n# ---------------------------------- Network -----------------------------------\n# Bind to localhost only for local development for better security\nnetwork.host: 127.0.0.1\nhttp.port: 9200\n\n# --------------------------------- Discovery ----------------------------------\n# Critical for a single-node development setup\ndiscovery.type: single-node\n\n# --- VITAL: Disable Security for Local Development ---\n# --- Delete or comment out the entire auto-generated security block ---\n# --- and add these lines instead: ---\n\nxpack.security.enabled: false\nxpack.security.enrollment.enabled: false # Not relevant if security is off\nxpack.security.http.ssl.enabled: false   # Disable SSL for HTTP\nxpack.security.transport.ssl.enabled: false # Disable SSL for inter-node communication\n\n# --- END VITAL SECURITY MODIFICATION ---\n\n# You can leave the rest of the file as is (mostly commented out defaults).\n# The auto-generated cluster.initial_master_nodes is not needed if discovery.type=single-node\n# and security is off.\n# The http.host: 0.0.0.0 can be changed to 127.0.0.1 for better local security.\n</code></pre> <p><code>config.yaml</code> file <pre><code>receivers:\n  otlp:\n    protocols:\n      grpc:\n        endpoint: 0.0.0.0:4319 # Your existing gRPC endpoint\n      http:\n        endpoint: 0.0.0.0:4320 # Your existing HTTP endpoint\n\nexporters:\n  debug:\n    verbosity: detailed\n  elasticsearch:\n    endpoints: [\"http://localhost:9200\"]\n    logs_index: \"agentic-foundry-tool-logs\"\n    sending_queue:\n      enabled: true # Just enable the queue, rely on its defaults for retry\n\nprocessors:\n  batch:\n    # send_batch_size: 8192\n    # timeout: 1s\n\nservice:\n  pipelines:\n    traces:\n      receivers: [otlp]\n      processors: [batch]\n      exporters: [debug] # Add 'elasticsearch' if you also want to send traces to ES\n\n    metrics:\n      receivers: [otlp]\n      processors: [batch]\n      exporters: [debug] # Add 'elasticsearch' if you also want to send metrics to ES\n\n    logs:\n      receivers: [otlp]\n      processors: [batch]\n      exporters: [debug, elasticsearch] # Send logs to both debug and Elasticsearch\n\n  telemetry:\n    logs:\n      level: info\n    metrics:\n      level: basic\n      address: localhost:8889\n</code></pre></p>"},{"location":"Telemetry/configurations/#starting-the-opentelemetry-collector","title":"Starting the OpenTelemetry Collector","text":"<p>To start the OpenTelemetry Collector, follow these steps:</p> <p>1. Navigate to the OpenTelemetry Collector directory: <pre><code>cd &lt;path_to_otel_collector_directory&gt;\n</code></pre> 2. Run the Collector with your configuration file: <pre><code>&lt;otel_collector_executable&gt; --config \"&lt;path_to_config_file&gt;\"\n#example\notelcol-contrib.exe --config \"C:\\Users\\user\\Downloads\\config.yaml\"\n</code></pre></p>"},{"location":"Telemetry/configurations/#starting-elasticsearch-and-loading-modules","title":"Starting Elasticsearch and Loading Modules","text":"<p>1. Navigate to the bin Folder:</p> <p>Open your command prompt or terminal and go to the Elasticsearch installation directory. Then, proceed to the bin folder.</p> <p>2. Run the Batch File:</p> <p>Inside the bin folder, execute the appropriate batch file. This will start Elasticsearch and automatically load all necessary modules.</p>"},{"location":"Telemetry/telemetry/","title":"Open Telemetry","text":"<p>OpenTelemetry is a comprehensive observability framework designed to provide deep insights into application behavior, performance, and health. It serves as a unified standard for collecting, processing, and exporting telemetry data across distributed systems, enabling organizations to maintain visibility into complex software architectures.</p>"},{"location":"Telemetry/telemetry/#key-components-of-telemetry","title":"Key Components of Telemetry:","text":"<ol> <li> <p>Logs: Captures detailed information including LLM inputs, outputs, prompts, responses, errors, and debugging information. These logs provide a comprehensive record of application events, user interactions, and system state changes, enabling thorough analysis of system behavior and troubleshooting.</p> </li> <li> <p>Traces: Tracks the complete chain of thought, agent decisions, and agent state transitions to observe how individual LLM tasks are executed. Traces provide end-to-end visibility into request flows, allowing developers to understand the sequence of operations, identify bottlenecks, and optimize performance across distributed components.</p> </li> </ol>"},{"location":"Telemetry/telemetry/#opentelemetry-workflow","title":"OpenTelemetry Workflow","text":"<p>OpenTelemetry is an open-source, vendor-neutral framework for collecting, processing, and exporting telemetry data (logs, traces, and metrics) from applications. This framework provides a standardized approach to observability that works across different programming languages, platforms, and cloud environments. Below is the comprehensive flowchart illustrating the telemetry pipeline:</p> OpenTelemetryLogging Statements \u2193 OpenTelemetry CollectorTransfer Logs \u2193 ElasticsearchFor Longer Format \u2193 GrafanaConnection to Grafana \u2193 Final Dashboard <p>Below is a detailed explanation of each stage in the telemetry workflow:</p> <ol> <li> <p>OpenTelemetry - Logging Statements (Instrumentation Layer): </p> <p>Application code is instrumented with OpenTelemetry libraries to generate structured logs, traces, and metrics. This instrumentation can be automatic (using pre-built libraries) or manual (custom implementations), capturing critical application events, performance metrics, and contextual information at runtime.</p> </li> <li> <p>OpenTelemetry Collector - Data Processing and Transfer: </p> <p>The OpenTelemetry Collector serves as a centralized agent that receives telemetry data from multiple sources. It performs data processing, filtering, batching, and enrichment before forwarding the data to appropriate backends. The collector supports various protocols and can transform data formats to ensure compatibility with different storage systems.</p> </li> <li> <p>Elasticsearch - Structured Storage and Indexing: </p> <p>Telemetry data is stored in Elasticsearch, a distributed search and analytics engine that provides powerful indexing capabilities. Elasticsearch enables efficient storage, searching, and aggregation of large volumes of telemetry data, supporting complex queries and real-time analysis across historical and current data sets.</p> </li> <li> <p>Elasticsearch - Grafana Integration: </p> <p>Elasticsearch serves as the data source for Grafana, providing a robust connection that enables real-time data retrieval and visualization. This integration supports advanced querying capabilities, allowing users to create sophisticated dashboards with dynamic filtering, alerting, and correlation analysis.</p> </li> <li> <p>Final Dashboard - Comprehensive Observability: </p> <p>Grafana presents telemetry data through interactive dashboards featuring charts, graphs, alerts, and custom visualizations. These dashboards provide stakeholders with actionable insights, enabling proactive monitoring, performance optimization, and rapid incident response.</p> </li> </ol>"},{"location":"Telemetry/telemetry/#data-collection-monitoring","title":"Data Collection &amp; Monitoring","text":"<p>The telemetry pipeline encompasses several critical phases that ensure comprehensive observability:</p> <ul> <li> <p>Data Collection - Comprehensive Instrumentation: </p> <p>The OpenTelemetry SDK provides robust instrumentation capabilities for collecting traces, logs, and metrics from agent frameworks and applications. This includes automatic instrumentation for popular libraries and frameworks, as well as APIs for custom instrumentation, ensuring complete visibility into application behavior and performance characteristics.</p> </li> <li> <p>Data Export - Reliable Transfer Mechanisms:</p> <p>The OpenTelemetry Collector implements reliable data transfer protocols to move collected telemetry data from source applications to external storage and analysis systems. This includes support for retry mechanisms, batching, compression, and multiple export formats to ensure data integrity and optimal performance.</p> </li> <li> <p>Centralized Storage &amp; Analysis - Scalable Data Management:</p> <p>All telemetry data is consolidated in Elasticsearch, providing a centralized repository that supports structured storage, efficient indexing, and powerful querying capabilities. This centralized approach enables cross-system correlation, historical analysis, and scalable data management for growing telemetry volumes.</p> </li> <li> <p>Visualization - Interactive Analytics Interface:</p> <p>Grafana connects to centralized storage systems to create dynamic dashboards that provide real-time insights into application performance, user interactions, system health, and agent workflows. These visualizations support drill-down capabilities, custom alerting, and collaborative analysis for enhanced operational awareness.</p> </li> </ul>"},{"location":"Telemetry/telemetry/#benefits-of-telemetry","title":"Benefits of Telemetry","text":"<p>Implementing comprehensive telemetry provides numerous advantages for modern software systems:</p> <ul> <li> <p>Improved Monitoring - Proactive System Oversight: </p> <p>Real-time insights into system performance, resource utilization, and application health enable proactive monitoring and early detection of potential issues before they impact users or business operations.</p> </li> <li> <p>Faster Debugging - Accelerated Issue Resolution: </p> <p>Detailed logs and distributed traces provide comprehensive context for troubleshooting, significantly reducing mean time to resolution (MTTR) by enabling developers to quickly identify root causes and understand system behavior during incidents.</p> </li> <li> <p>Enhanced Optimization - Data-Driven Performance Improvements: </p> <p>Telemetry data enables evidence-based decisions for system optimization, capacity planning, and resource allocation, leading to improved performance, reduced costs, and better user experiences.</p> </li> <li> <p>Scalability - Distributed System Support: </p> <p>The framework is specifically designed for distributed systems and microservices architectures, providing visibility across complex service interactions and supporting horizontal scaling as system complexity grows.</p> </li> <li> <p>Standardization - Vendor-Neutral Approach: </p> <p>OpenTelemetry provides a standardized approach to observability that reduces vendor lock-in and enables consistent telemetry practices across different technologies and platforms.</p> </li> <li> <p>Compliance and Governance - Audit Trail Capabilities: </p> <p>Comprehensive logging and tracing support regulatory compliance requirements and provide detailed audit trails for security, performance, and operational governance.</p> </li> </ul> <p>By leveraging the powerful combination of OpenTelemetry, Elasticsearch, and Grafana, organizations can build robust, scalable observability pipelines that provide deep insights into system behavior, enable proactive monitoring, and support data-driven optimization decisions for maintaining high-performing, reliable systems.</p>"},{"location":"agent_config/HybridAgent/","title":"Hybrid Agent Configuration","text":"<p>Hybrid agent onboarding follows a similar process to planner executor critic agent onboarding. For detailed steps, refer to the planner executor critic agent onboarding guide.</p> <p>However, unlike planner executor critic agent onboarding where multiple system prompts are generated for each agent, the hybrid agent onboarding process generates a single system prompt that governs the behavior of the entire hybrid agent. This simplifies configuration and ensures unified agent responses.</p>"},{"location":"agent_config/Overview/","title":"Agent Configuration","text":"<p>The Agent Configuration is a core functionality in agentic framework that enables users to create, update, and delete agents.</p>"},{"location":"agent_config/Overview/#what-is-an-agent","title":"What is an Agent?","text":"<p>An agent is composed of three core components:</p> <ol> <li> <p>Large Language Model (LLM)     The core reasoning engine that drives the agent's behavior.</p> </li> <li> <p>Tools     A set of Python functions (onboarded as tools) that the agent can call to perform actions such as querying data, generating content, or interacting with systems.</p> </li> <li> <p>Prompt (Workflow Description)     A detailed set of instructions that guides the agent's decisions and actions.</p> </li> </ol>"},{"location":"agent_config/Overview/#agent-configuration-overview","title":"Agent Configuration Overview","text":"<p>Agent configuration involves using a reusable, template-driven setup that allows creation of agents with specific roles, goals, and personas.</p>"},{"location":"agent_config/Overview/#templates-overview","title":"Templates Overview","text":"<p>The system supports multiple types of agent templates, each offering different capabilities suited for specific use cases:</p> <ol> <li> <p>React Agent: The ReAct (Reasoning and Acting) agent combines reasoning traces with action execution.</p> </li> <li> <p>React Critic Agent: An enhanced React Agent with dual system prompts for self-critique and improved output quality.</p> </li> <li> <p>Multi Agent: The multi agent follows the Plan and Execute paradigm, enabling collaboration between specialized agents.</p> </li> <li> <p>Planner Executor Agent: Implements the Planner-Executor-Critic workflow with a more granular system prompt structure.</p> </li> <li> <p>Meta Agent: An agent supervisor responsible for routing to individual agents and managing high-level orchestration.</p> </li> <li> <p>Meta Planner Agent: An advanced orchestrator using multiple system prompts for robust and adaptive agent-based workflows.</p> </li> <li> <p>Hybrid Agent: Combines features from multiple agent types, enabling flexible workflows that leverage both reasoning and execution capabilities for complex tasks.</p> </li> </ol>"},{"location":"agent_config/PlannerExecutor/","title":"Planner-Executor Agent Configuration","text":"<p>The Planner-Executor Agent uses the same onboarding and configuration steps as the multi agent, based on the <code>Planner-Executor-Critic</code> paradigm. The main difference is in the system prompt configuration.</p>"},{"location":"agent_config/PlannerExecutor/#planner-executor-agent-onboarding","title":"Planner-Executor Agent Onboarding","text":"<p>Follow the onboarding steps in Multi Agent Onboarding:</p> <ol> <li>Select Template: Choose the Planner-Executor Agent template.</li> <li>Select Tools: Select tools for the agent's tasks.</li> <li>Agent Name: Provide a name for your agent.</li> <li>Agent Goal: Define the agent's main objective.</li> <li>Workflow Description: Give instructions and guidelines for task execution.</li> <li>Model Name: Select the model for generating system prompts.</li> </ol> <p>System Prompts</p> <p>The Planner-Executor Agent generates five system prompts for the planner, executor, and critic components, offering detailed guidance for each workflow stage.</p>"},{"location":"agent_config/PlannerExecutor/#agent-updation","title":"Agent Updation","text":"<p>Agent updation is similar to React Agent Updation.</p>"},{"location":"agent_config/PlannerExecutor/#agent-deletion","title":"Agent Deletion","text":"<p>Agent deletion is similar to React Agent Deletion.</p> <p>Important</p> <p>Only the original creator of the agent can update or delete it. Other users cannot modify or remove these resources.</p>"},{"location":"agent_config/PlannerMeta/","title":"Planner Meta Agent Configuration","text":"<p>Meta Planner Agent acts as the central orchestrator, coordinating multiple specialized agents (like ReAct, Multi-Agent, or hybrids) to solve complex queries. It extends the Meta Agent by using a multi-system-prompt approach for finer control and supervision.</p>"},{"location":"agent_config/PlannerMeta/#meta-planner-agent-onboarding","title":"Meta Planner Agent Onboarding","text":"<p>The onboarding steps are the same as the Meta Agent. See Meta Agent Onboarding:</p> <ol> <li>Select Template: Choose the Meta Planner Agent template.</li> <li>Select Agents: From the available list, select the worker agents to be managed by the Meta Planner Agent. The Meta Planner Agent supports binding of the following worker agent templates:<ul> <li>React Agent</li> <li>React Critic Agent</li> <li>Multi Agent</li> <li>Planner Executor Agent</li> <li>Planner Executor Critic Agent</li> </ul> </li> <li>Agent Name: Provide a name for your meta planner agent.</li> <li>Agent Goal: Define the main objective.</li> <li>Workflow Description: Give instructions and guidelines for orchestration and task delegation.</li> <li>Model Name: Select the model for generating system prompts.</li> </ol> <p>System Prompts</p> <p>The Meta Planner Agent uses three system prompts:</p> <ul> <li>Meta Planner System Prompt: Guides high-level planning and query decomposition.</li> <li>Meta Responser System Prompt: Directs aggregation and synthesis of responses from worker agents.</li> <li>Meta Supervisor System Prompt: Oversees execution, coordination, and quality control.</li> </ul>"},{"location":"agent_config/PlannerMeta/#agent-updation","title":"Agent Updation","text":"<p>To update the agent (add/remove worker agents, update workflow), follow the steps in Meta Agent Updation.</p>"},{"location":"agent_config/PlannerMeta/#agent-deletion","title":"Agent Deletion","text":"<p>See Meta Agent Deletion for deletion steps and permission notes.</p> <p>Important</p> <p>Only the original creator of the agent can update or delete it. Other users cannot modify or remove these resources.</p>"},{"location":"agent_config/metaAgent/","title":"Meta Agent Configuration","text":"<p>Meta Agent Serves as the central decision making entity. Individual agents are coordinated by a central supervisor agent. The supervisor controls all communication flow and task delegation, making decisions about which agent to invoke based on the current context and task requirements.</p>"},{"location":"agent_config/metaAgent/#meta-agent-onboarding","title":"Meta Agent Onboarding","text":"<p>To onboard a Meta Agent, you'll begin by selecting the META AGENT template from the available options. Next, you'll choose the specific React, Multi or Meta agents from the listed agents that you want to coordinate under the meta agent's supervision. </p> <p>The Meta Agent supports binding of the following worker agent templates:</p> <ul> <li>React Agent</li> <li>React Critic Agent</li> <li>Multi Agent</li> <li>Planner Executor Agent</li> <li>Planner Executor Critic Agent</li> </ul> <p>You'll then assign a descriptive name that clearly represents your meta agent's purpose and function. After naming your agent, you'll define its primary goal and objective, explaining what the coordinated agent system is designed to accomplish. </p> <p>Finally, you'll provide comprehensive workflow instructions that detail how the meta agent should handle requests, classify user intent, and delegate tasks to the appropriate worker agents. The system will use your selected model to automatically generate a system prompt based on your agent goal and workflow description.</p> Sample Agent Goal <p>To intelligently process user queries by dynamically identifying their intent whether for entertainment or academic research and delivering personalized movie recommendations using real-time Wikipedia data or generating structured academic insights through the analysis of recent scholarly literature. The workflow is designed to provide tailored, context-aware content, enhancing both leisure exploration and research discovery through a unified, tool driven pipeline.</p> Sample Workflow description <p>User Input and Intent Classification:</p> <p>The user initiates the interaction by entering a query, such as:</p> <ul> <li>\u201cTop action movies about war\u201d</li> <li>\u201cExplore the intersection of AI and climate modeling\u201d</li> </ul> <p>The system classifies the query into one of two categories:</p> <ul> <li>Entertainment Query (Movie-Related)</li> <li>Academic/Research Topic</li> </ul> <p>Movie Recommendation Path:</p> <p>Step 1: Movie Query Collection Prompt the user to provide a movie-related query, e.g., \u201cTop 10 adventure movies of all time.\u201d</p> <p>Step 2: Wikipedia Search Search Wikipedia for the query and retrieve the first relevant list-type article.</p> <p>Step 3: Extract Movie Links Extract individual movie article URLs from the retrieved Wikipedia page.</p> <p>Step 4: Scrape Movie Metadata Scrape metadata (e.g., title, genre, synopsis, director, release year, ratings) from the movie URLs.</p> <p>Step 5: Generate Recommendations Use the scraped metadata to generate personalized movie recommendations with summaries and rationale.</p> <p>Academic Research Path:</p> <p>Step 1: Research Topic Collection Prompt the user to input a research topic, e.g., \u201cRecent developments in quantum cryptography.\u201d</p> <p>Step 2: Literature Search Search Semantic Scholar for recent academic papers based on the query, retrieving details like title, abstract,     authors, and DOI.</p> <p>Step 3: Publication Analysis Analyze the retrieved papers to extract structured insights, including main findings, methodologies, and research gaps.</p> <p>Step 4 (Conditional): Cross-Disciplinary Synthesis If the query spans multiple disciplines (e.g., \u201cAI\u201d + \u201cHealthcare\u201d), generate a synthesis report highlighting sharedchallenges, innovation opportunities, and complementary methods.</p> <p>Step 5: Academic Report Generation Compile findings into a formal academic report with sections like Title, Abstract, Literature Review, and Conclusion.</p> <p>Select the model name from the dropdown - which is used to create <code>system prompt</code> based on provided Agent goal and Workflow description. </p> <p>System Prompt:</p> <p>Final guidelines for the agent - created by LLM based on provided Agent goal and Workflow description for the agent.</p> Sample Generated System Prompt <p>Agent Name Movie_and_scholar_Agent  </p> <p>Goal to Achieve for the Workflow The Movie_and_scholar_Agent is designed to intelligently process user queries by dynamically identifying their intent\u2014whether for entertainment (movie-related) or academic research\u2014and delivering tailored, high-quality outputs. The Meta Agent must: 1. Classify User Intent: Accurately determine whether the query is entertainment-focused or research-oriented. 2. Leverage Worker Agents: Efficiently delegate tasks to the appropriate worker agents (Movie Recommendation Agent or Research_Scholar_Agent) based on the classified intent. 3. Ensure Workflow Completion: Oversee the end-to-end execution of the workflow, ensuring the user receives personalized movie recommendations or structured academic insights. 4. Maintain Context Awareness: Adapt responses to the user\u2019s specific query, ensuring relevance, accuracy, and clarity in the final output.  </p> <p>Guidelines on Worker Agents Provided by the User </p> <ol> <li>Movie Recommendation Agent </li> <li>Key Features:  <ul> <li>Provides personalized movie recommendations based on genres, themes, and ratings.  </li> <li>Extracts and analyzes movie metadata from Wikipedia to generate recommendations.  </li> </ul> </li> <li>Role in Workflow:  <ul> <li>Handles all tasks related to entertainment queries, including Wikipedia searches, metadata extraction, and recommendation generation.  </li> </ul> </li> <li> <p>Limitations:  </p> <ul> <li>Relies on Wikipedia for movie data; may not cover all movies or provide exhaustive metadata.  </li> </ul> </li> <li> <p>Research_Scholar_Agent </p> </li> <li>Key Features:  <ul> <li>Conducts advanced academic searches using recent scholarly literature.  </li> <li>Analyzes publications to extract findings, methodologies, and research gaps.  </li> <li>Synthesizes insights across disciplines and generates formal academic reports.  </li> </ul> </li> <li>Role in Workflow:  <ul> <li>Manages all tasks related to academic research, including literature search, analysis, synthesis, and report generation.  </li> </ul> </li> <li>Limitations:  <ul> <li>Dependent on the availability of recent academic papers and cross-disciplinary synthesis triggers.  </li> </ul> </li> </ol> <p>Step-by-Step Task Execution </p> <p>Step 1: User Input Collection - Action: Prompt the user to input their query or topic. - Meta Agent Responsibility:   - Collect the query and ensure it is clear and well-structured.   - Example Queries:     - Entertainment: \u201cTop action movies about war\u201d     - Academic: \u201cExplore the intersection of AI and climate modeling\u201d  </p> <p>Step 2: Intent Classification - Action: Analyze the user query to classify it as either an Entertainment Query or an Academic/Research Topic. - Meta Agent Responsibility:   - Use keywords, structure, and context to determine intent.   - Decision Points:     - If the query is movie-related \u2192 Proceed to the Movie Recommendation Path.     - If the query is research-oriented \u2192 Proceed to the Academic Research Path.  </p> <p>Movie Recommendation Path (If Movie Intent Detected) </p> <p>Step 3: Movie Query Collection - Action: Confirm or refine the user\u2019s movie-related query for better specificity. - Meta Agent Responsibility:   - Ensure the query is actionable for the Movie Recommendation Agent.   - Example: \u201cTop 10 adventure movies of all time\u201d \u2192 Refine if needed.  </p> <p>Step 4: Wikipedia Search - Action: Trigger the <code>search_wikipedia_page</code> function. - Worker Agent Involved: Movie Recommendation Agent. - Input: User-provided query. - Output: First relevant Wikipedia article URL (e.g., a list of movies).  </p> <p>Step 5: Extract Movie Links - Action: Trigger the <code>extract_movie_links</code> function. - Worker Agent Involved: Movie Recommendation Agent. - Input: Wikipedia list article URL. - Output: List of individual movie Wikipedia article URLs.  </p> <p>Step 6: Scrape Movie Metadata - Action: Trigger the <code>scrape_movie_details</code> function. - Worker Agent Involved: Movie Recommendation Agent. - Input: Movie article URLs. - Output: Metadata for each movie, including title, genre, synopsis, director, year of release, and ratings.  </p> <p>Step 7: Generate Movie Recommendations - Action: Trigger the <code>get_movie_recommendations_from_wikipedia</code> function. - Worker Agent Involved: Movie Recommendation Agent. - Input: Original user query + scraped metadata. - Output: Personalized movie recommendations with rationale and summaries.  </p> <p>Step 8: Deliver Results - Action: Present the recommendations to the user in a clear and engaging format. - Meta Agent Responsibility:   - Ensure the output aligns with the user\u2019s preferences and query intent.  </p> <p>Academic Research Path (If Research Intent Detected) </p> <p>Step 3: Research Topic Collection - Action: Confirm or refine the user\u2019s research topic for clarity and specificity. - Meta Agent Responsibility:   - Ensure the topic is actionable for the Research_Scholar_Agent.   - Example: \u201cRecent developments in quantum cryptography\u201d \u2192 Refine if needed.  </p> <p>Step 4: Literature Search - Action: Trigger the <code>search_semantic_scholar</code> function. - Worker Agent Involved: Research_Scholar_Agent. - Input: User query or research topic (optional paper limit: default 5). - Output: List of recent academic papers, including title, abstract, authors, publication year, source/venue, and DOI/link.  </p> <p>Step 5: Publication Analysis - Action: Trigger the <code>analyze_publication</code> function for each retrieved paper. - Worker Agent Involved: Research_Scholar_Agent. - Input: Title and year of each paper. - Output: Structured academic insights, including main findings, methodologies, applications, and research gaps.  </p> <p>Step 6 (Conditional): Cross-Disciplinary Synthesis - Action: Trigger the <code>cross_disciplinary_synthesis</code> function if the query involves multiple disciplines. - Worker Agent Involved: Research_Scholar_Agent. - Input: Subtopics/disciplines extracted from the query + insights from publication analysis. - Output: Synthesis report with shared challenges, innovation opportunities, complementary methods, and future research pathways.  </p> <p>Step 7: Academic Report Generation - Action: Trigger the <code>generate_academic_report</code> function. - Worker Agent Involved: Research_Scholar_Agent. - Input: User\u2019s original research query + combined findings and synthesis. - Output: Formal academic report with title, abstract, introduction, literature review, discussion, conclusion, and optional references (with DOIs).  </p> <p>Step 8: Deliver Results - Action: Present the academic report to the user in a professional and structured format. - Meta Agent Responsibility:   - Ensure the report is comprehensive, accurate, and aligned with the user\u2019s research goals.  </p> <p>Additional Guidelines for the Meta Agent 1. Accuracy and Relevance: Ensure all outputs are accurate, contextually relevant, and tailored to the user\u2019s query. 2. Seamless Workflow Management: Coordinate tasks between worker agents efficiently, ensuring no step is skipped or mismanaged. 3. Error Handling: If a worker agent fails or data is unavailable, provide a clear explanation to the user and suggest alternative actions. 4. User Engagement: Maintain a conversational tone and clarify ambiguities in user queries when necessary.  </p> <p>This structured prompt ensures the <code>Movie_and_scholar_Agent</code> can effectively manage workflows, leverage worker agents, and deliver high-quality outputs tailored to user needs.</p>"},{"location":"agent_config/metaAgent/#agent-updation","title":"Agent Updation","text":"<p>After creating your Meta agent, you can modify its configuration as needed:</p> <p>1. Add Agents</p> <ul> <li>Navigate to agent configuration</li> <li>Select \"Add Agents\" and choose from available tools</li> <li>System automatically updates capabilities and regenerates system prompt</li> </ul> <p>2. Remove Agents: </p> <ul> <li>Access current agent configuration</li> <li>Select \"Remove Agents\" and choose tools to remove</li> <li>Confirm removal - system prompt updates automatically</li> </ul> <p>3. Update Workflow: </p> <ul> <li>Select \"Update Workflow\" </li> <li>Edit workflow description with new instructions</li> <li>System regenerates system prompt based on changes</li> </ul>"},{"location":"agent_config/metaAgent/#agent-deletion","title":"Agent Deletion","text":"<p>Agent Deletion is similar to React Agent Deletion</p> <p>Important</p> <p>Only the original creator of the agent has permission to update or delete it. Other users do not have access to modify or remove these resources.</p>"},{"location":"agent_config/multiAgent/","title":"Multi Agent Configuration","text":"<p>The Multi Agent operates on the <code>Planner-Executor-Critic</code> paradigm. It begins with a <code>Planner Agent</code> that generates a step-by-step plan based on the user query. The <code>Executor Agent</code> then executes each step of the plan. The <code>Critic</code> evaluates the outputs by scoring the results of each step.</p>"},{"location":"agent_config/multiAgent/#multi-agent-onboarding","title":"Multi Agent Onboarding","text":"<p>To onboard a Multi Agent, users begin by selecting the MULTI AGENT template and choosing their desired tools such as get_weather. Must provide an agent name like \"Weather Agent\" and define the agent goal, for example \"This agent provides personalized suggestions based on real-time weather data.\" </p> <p>A detailed workflow description with guidelines for the LLM is essential, outlining the step-by-step process the agent should follow. Users then select the model name from the dropdown menu, which generates system prompts based on the provided agent goal and workflow description.</p> <p>Sample Workflow description</p> <p>Understand the user intent and perform following steps:</p> <ol> <li>Retrieve Weather Data - Make an API call to fetch real-time weather data.</li> <li>Analyze Weather Conditions - Evaluate the weather data to determine current conditions.</li> <li>Generate Recommendations - Based on the analysis, provide personalized suggestions to the user.</li> </ol> <p>For example:</p> <p>If the weather is pleasant, suggest outdoor activities. If the weather is rainy or stormy, advise the user to carry an umbrella or avoid traveling. If extreme weather conditions are detected, recommend staying indoors and taking necessary precautions.</p> <p>Select the model name from the dropdown - which is used to create <code>system prompt</code> based on provided Agent goal and Workflow description. </p> <p>System Prompt:</p> <p>Using the provided agent goal and workflow description, LLM generates system prompts for the planner, executor, and critic agents within a multi-agent template.</p>"},{"location":"agent_config/multiAgent/#agent-updation","title":"Agent Updation","text":"<p>Agent Updation is similar to React Agent Updation</p>"},{"location":"agent_config/multiAgent/#agent-deletion","title":"Agent Deletion","text":"<p>Agent Deletion is similar to React Agent Deletion</p> <p>Important</p> <p>Only the original creator of the agent has permission to update or delete it. Other users do not have access to modify or remove these resources.</p>"},{"location":"agent_config/reactAgent/","title":"React Agent Configuration","text":"<p>The ReAct(Reasoning and Acting) agent combines reasoning traces with action execution. It uses a step by step thought process to determine what tool to use, executes it, observe the result, and continues until it can return a final answer.</p>"},{"location":"agent_config/reactAgent/#react-agent-onboarding","title":"React Agent Onboarding","text":"<p>To create a React agent, you'll begin by selecting the REACT AGENT template from the available options. </p> <p>Next, you'll choose the specific tools your agent needs from the provided list to effectively perform its intended tasks. You'll then assign a descriptive name that clearly represents your agent's purpose and function. </p> <p>After naming your agent, you'll define its primary goal and objective, explaining what the agent is designed to accomplish. </p> <p>Finally, you'll provide comprehensive workflow instructions that detail how the agent should handle requests and execute its tasks, giving it clear guidelines for processing and responding to user interactions.</p> <p>Sample Workflow description</p> <pre><code>Understand the user intent and perform following steps:\n\n1. Retrieve Weather Data - Make an API call to fetch real-time weather data.\n2. Analyze Weather Conditions - Evaluate the weather data to determine current conditions.\n3. Generate Recommendations - Based on the analysis, provide personalized suggestions to the user.\n\nFor example:\nIf the weather is pleasant, suggest outdoor activities.\nIf the weather is rainy or stormy, advise the user to carry an umbrella or avoid traveling.\nIf extreme weather conditions are detected, recommend staying indoors and taking necessary precautions.\n</code></pre> <p>Select the model name - which is used to create <code>system prompt</code> based on provided Agent goal and Workflow description.</p> <p>System Prompt:</p> <p>Final guidelines for the agent - created by LLM based on provided Agent goal and Workflow description for the agent.</p> <p>Sample Generated System Prompt</p> <p>Agent Name Weather Agent</p> <p>Goal to Achieve for the Workflow - The Weather Agent aims to provide personalized suggestions to users based on real-time weather data, enhancing their daily decision-making regarding activities and safety precautions.</p> <p>Guidelines on Tools Provided by the User - Tool Name: get_weather_new - Key Functionalities: This tool retrieves real-time weather data for a specified city using the OpenWeatherMap API. It provides details such as temperature, humidity, pressure, and a brief weather description. - Limitations: The tool requires a valid API key and city name to function. It returns an error message if the HTTP request fails, indicating potential issues with network connectivity or incorrect parameters.</p> <p>Step-by-Step Task Description 1. Retrieve Weather Data: - Use the <code>get_weather_new</code> tool to make an API call with the provided API key and city name. - Ensure the tool successfully fetches the weather data, including temperature, humidity, pressure, and weather description.</p> <ol> <li>Analyze Weather Conditions: </li> <li>Evaluate the retrieved weather data to determine current conditions.</li> <li> <p>Identify key weather attributes such as temperature range, humidity levels, and specific weather phenomena (e.g., rain, storm).</p> </li> <li> <p>Generate Recommendations: </p> </li> <li>Based on the analysis, provide personalized suggestions to the user:<ul> <li>If the weather is pleasant, suggest outdoor activities.</li> <li>If the weather is rainy or stormy, advise the user to carry an umbrella or avoid traveling.</li> <li>If extreme weather conditions are detected, recommend staying indoors and taking necessary precautions.</li> </ul> </li> </ol> <p>Additional Relevant Information - The agent should ensure the recommendations are timely and relevant to the user's location and current weather conditions. - In case of tool failure or unavailability, the agent should inform the user of the inability to provide weather-based suggestions and recommend checking weather updates through other means.</p>"},{"location":"agent_config/reactAgent/#agent-updation","title":"Agent Updation","text":"<p>After creating your React agent, you can modify its configuration as needed:</p> <p>1. Add Tools</p> <ul> <li>Navigate to agent configuration</li> <li>Select \"Add Tools\" and choose from available tools</li> <li>System automatically updates capabilities and regenerates system prompt</li> </ul> <p>2. Remove Tools</p> <ul> <li>Access current tool configuration</li> <li>Select \"Remove Tools\" and choose tools to remove</li> <li>Confirm removal - system prompt updates automatically</li> </ul> <p>3. Update Workflow Description</p> <ul> <li>Select \"Update Workflow\" </li> <li>Edit workflow description with new instructions</li> <li>System regenerates system prompt based on changes</li> </ul> <p>Update Permissions</p> <p>Only the original creator can update the agent configuration.</p>"},{"location":"agent_config/reactAgent/#agent-deletion","title":"Agent Deletion","text":"<p>To delete an agent:</p> <ol> <li>Go to agent dashboard</li> <li>Select the agent you want to delete</li> <li>Enter the creator's email address</li> <li>Click \"Delete Agent\"</li> </ol> <p>To delete an agent, you need to provide the creator's email address.</p> <p>Important</p> <p>Only the original creator can delete the agent.</p>"},{"location":"agent_config/reactCritic/","title":"React Critic Agent Configuration","text":"<p>The React Critic Agent builds on the React Agent by adding a dual system prompt for improved self-critique and output quality. All onboarding and configuration steps are the same as the React Agent, except for the system prompt.</p>"},{"location":"agent_config/reactCritic/#react-critic-agent-onboarding","title":"React Critic Agent Onboarding","text":"<p>Follow the onboarding steps in React Agent Onboarding. The only difference is that the React Critic Agent generates two system prompts during setup:</p> <ol> <li>Executor Agent System Prompt: Guides the agent's reasoning and actions (same as React Agent).</li> <li>Critic Agent System Prompt: Adds critical evaluation, reviewing and refining the agent's reasoning, actions, and outputs.</li> </ol>"},{"location":"agent_config/reactCritic/#agent-updation","title":"Agent Updation","text":"<p>To update the agent (e.g., add/remove tools, update workflow), follow the same steps as in React Agent Updation.</p>"},{"location":"agent_config/reactCritic/#agent-deletion","title":"Agent Deletion","text":"<p>See React Agent Deletion for deletion steps and permission notes.</p> <p>Important</p> <p>Only the original creator of the agent can update or delete it. Other users cannot modify or remove these resources.</p>"},{"location":"files_upload/files/","title":"Files","text":"<p>This section is used to upload different types of files that may be required by the tools to perform its operation. This may include files such as Database Files (.db), Excel Sheets (.xlsx), Word Documents (.docx), PDF Files (.pdf) and any other files required by your agent.</p> <p>Supported database files include SQLite (.db, .sqlite) and PostgreSQL dump files (.sql).</p>"},{"location":"files_upload/files/#uploading-files","title":"Uploading Files","text":"<p>The file upload functionality provides a convenient way to make your data and documents available to the system:</p> <ul> <li>Upload Methods: You can upload your files either by clicking the \"Browse\" button to select files from your computer or by using the drag and drop feature directly into the \"Upload\" window.</li> <li>File Storage: After uploading, all files will be automatically saved in the <code>user_uploads</code> directory within the system.</li> <li>File Access: To access any uploaded file through any tool or agent, use the standardized path format: <code>user_uploads/filename.extension</code>.</li> </ul> <p>Info</p> <p>Both single and multiple file uploads are supported.</p>"},{"location":"files_upload/files/#upload-guidelines","title":"Upload Guidelines","text":"<ul> <li>Ensure files are in supported formats for optimal compatibility</li> <li>File names should be descriptive and avoid special characters</li> <li>Large files may take longer to upload depending on your connection speed</li> <li>The system will validate file types before completing the upload</li> </ul>"},{"location":"files_upload/files/#viewing-downloading-and-deleting-files","title":"Viewing, Downloading and Deleting Files","text":"<p>The file management interface provides comprehensive control over your uploaded content:</p> <ul> <li>View Files: Click the \"View\" button to see a list of all uploaded files with their details including file name, size, and upload date</li> <li>Download Files: Use the \"Download\" button to retrieve a copy of any uploaded file to your local machine</li> <li>Delete Files: Remove unwanted files by clicking the \"Delete\" button - this action will permanently remove the file from the system</li> </ul>"},{"location":"tools_config/tools/","title":"Tools Configuration","text":""},{"location":"tools_config/tools/#what-are-tools","title":"What Are Tools?","text":"<p>Tools are external functions or actions that an AI agent can call to perform tasks. it can't do with language alone\u2014like searching the web, doing math, or querying a database.</p> <p>Tools give the agent real-world abilities\u2014they act like plugins or helpers that the agent can call when it needs to go beyond generating text.</p> <p>Scenario: The AI agent is asked: \"What\u2019s 1234 multiplied by 9876?\"</p> <ul> <li> <p><code>Without a tool:</code> The agent might try to calculate it just by generating the answer with text prediction. It could get it wrong, especially with large numbers, since LLMs aren\u2019t perfect at arithmetic.</p> </li> <li> <p><code>With a tool:</code> We give the agent access to a calculator tool (a Python function).</p> </li> </ul> <pre><code>def multiply_numbers(x: int, y: int) -&gt; int:\n    return x * y\n</code></pre> <p>Now, when asked \"What\u2019s 1234 multiplied by 9876?\"</p> <p>The agent thinks: This looks like a math problem. I should use the multiply_numbers tool.\" So it calls multiply_numbers(1234, 9876) and fetches the correct result .</p>"},{"location":"tools_config/tools/#tools-format","title":"Tools Format","text":"<p>To maintain consistency and reliability, each tool should follow a standard format While onboarding .</p> <ul> <li><code>Description:</code> A short explanation of what the tool does.</li> <li><code>Code Block:</code> The tool\u2019s logic, properly indented and syntactically correct.</li> <li><code>Created By:</code> Email of the tool creator, used to prevent unauthorized edits.</li> <li><code>Model:</code> Once the  model is selected, a doc string for the tool will be generated.</li> <li><code>Domain Tags:</code> Optional labels (e.g., manufacturing, logistics) indicating the domain the tool applies to.</li> </ul>"},{"location":"tools_config/tools/#onboarding-tool","title":"Onboarding Tool","text":"<p>Let's proceed by using the example of onboarding a <code>Weather Information Retrieval Tool</code>.</p> <p>Step 1: Provide a short description of the tool.</p> <ul> <li>Write a clear, concise description explaining what the tool does</li> <li>For example: \"This tool retrieves current weather information for a specified location\"</li> </ul> <p>Step 2: Add the python tool code into code snippet.</p> <ul> <li>Write the complete Python function with proper syntax</li> <li>Include all necessary imports and dependencies You can either paste the complete Python function directly into the code block or upload a <code>.py</code> file containing the tool code. The system will support both manual code entry and <code>.py</code> file uploads for onboarding tool logic.</li> </ul> <p>Step 3: Specify the required model and enter your email.</p> <ul> <li>Select the appropriate AI model from the dropdown menu</li> <li>Enter your email address as the tool creator</li> <li>This email will be used for authorization when updating or deleting the tool</li> </ul> <p>Step 4: Select the appropriate domain and click <code>Add Tool</code>.</p> <ul> <li>Choose relevant domain tags (e.g., Logistics, utilities, General)</li> <li>Review all entered information for accuracy</li> <li>Click the \"Add Tool\" button to complete the onboarding process</li> </ul>"},{"location":"tools_config/tools/#updating-tool","title":"Updating Tool","text":"<p>If we want to modify a tool that has already been onboarded, we must first ensure that the tool is not currently being referenced by any agent.  </p> <p>If it is, remove the dependency before starting the update process. Once the tool has been updated, you can re-establish the dependency.</p> <p>To update a tool, you must also provide the <code>creator email address</code></p> <p>Step 1: Select the Edit option from the tool management interface</p> <p>Step 2: Make your desired changes to the tool configuration</p> <p>Step 3: Enter the authorized creator's email ID </p> <p>Step 4: Click on <code>Update</code> to save your changes</p>"},{"location":"tools_config/tools/#deleting-a-tool","title":"Deleting a Tool","text":"<p>If you want to delete an existing tool, make sure to first remove any dependencies from agents that are using it.  </p> <p>Once all dependencies are cleared, you can proceed with deleting the tool.</p> <p>To delete a tool, you must also provide the <code>creator email address</code></p> <p>Step 1: Select the Delete option from the tool management interface</p> <p>Step 2: Enter the authorized creator's email ID </p> <p>Step 3: Click on <code>Delete</code> to permanently remove the tool</p> <p>Important</p> <p>Only the original creator of the tool has permission to update or delete it. Other users do not have access to modify or remove these resources.</p>"},{"location":"tools_config/tools/#tool-file-storage","title":"Tool File Storage","text":"<p>Tools are saved as files on the server during the onboarding and update process. This file-based storage approach ensures:</p> <ul> <li>Persistence: Tool code is stored as physical files on the server, aligned with database records</li> <li>Update Support: When a tool is updated, the corresponding file on the server is also updated</li> <li>Delete Support: When a tool is deleted, the file is removed from the server</li> <li>Restore Support: Deleted tools can be restored from the recycle bin, which also restores the associated tool file</li> </ul> <p>Info</p> <p>This file management logic is synchronized with the database operations, ensuring consistency between the tool records in the database and the actual tool files on the server.</p>"}]}