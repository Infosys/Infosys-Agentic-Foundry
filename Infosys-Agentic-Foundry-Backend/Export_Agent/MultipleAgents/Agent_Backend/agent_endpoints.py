import os
import shutil
import asyncio
from fastapi import FastAPI
from fastapi_ws_router import WSRouter
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse #FileResponse
from contextlib import asynccontextmanager
from pydantic import BaseModel
import asyncio
import sys
from typing import Dict, List, Union
from database_creation import initialize_tables
from database_manager import get_agents_details_for_chat,insert_into_feedback_table ,delete_chat_history_by_session_id #update_latest_query_response_with_tag
from fastapi import FastAPI, UploadFile, File, HTTPException 

DB_URI=os.getenv("POSTGRESQL_DATABASE_URL", "")
POSTGRESQL_HOST = os.getenv("POSTGRESQL_HOST", "")
POSTGRESQL_USER = os.getenv("POSTGRESQL_USER", "")
POSTGRESQL_PASSWORD = os.getenv("POSTGRESQL_PASSWORD", "")
DATABASE = os.getenv("DATABASE", "")
 
DB_URI = os.getenv("POSTGRESQL_DATABASE_URL", "")
 
if sys.platform.startswith("win"):
    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
 
@asynccontextmanager
async def lifespan(app: FastAPI):
    await initialize_tables()
    yield
 
app = FastAPI(lifespan=lifespan)
 
router = WSRouter()
 
 
origins = [
    "http://127.0.0.1:6001",
    "http://localhost:3001"
]
 
 
app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],  # Allows all methods
    allow_headers=["*"],  # Allows all headers
)

class AgentInferenceRequest(BaseModel):
    """
    Pydantic model representing the input request for agent inference.
    This model captures the necessary details for querying the agent,
    including the application ID, query, session ID, and model-related information.
    """
    agentic_application_id: str  # ID of the agentic application
    query: str  # The query to be processed by the agent
    session_id: str  # Unique session identifier
    model_name: str  # Name of the llm model
    reset_conversation: bool
    feedback: str = None  # Optional feedback for the agent
    interrupt_flag: bool = False

class ReplannerAgentInferenceRequest(BaseModel):
    """
    Pydantic model representing the input request for agent inference.
    This model captures the necessary details for querying the agent,
    including the application ID, query, session ID, and model-related information.
    """
    agentic_application_id: str  # ID of the agentic application
    query: str  # The query to be processed by the agent
    session_id: str  # Unique session identifier
    model_name: str  # Name of the llm model
    reset_conversation: bool = False# If true need to reset the conversation
    approval: str = None
    feedback: str = None
    prev_response: Dict = {}
    interrupt_flag: bool = False
    tool_feedback: str = None

class PrevSessionRequest(BaseModel):
    """
    Pydantic model representing the input request for retriving previous conversations.
    """
    session_id: str  # session id of the user
    agent_id: str  # agent id user is working on

@app.post("/get-query-response")
async def get_react_and_pec_agent_response(request: AgentInferenceRequest):
    """
    Gets the response for a query using agent inference.
 
    Parameters:
    ----------
    request : AgentInferenceRequest
        The request body containing the query details.
 
    Returns:
    -------
    dict
        A dictionary containing the response generated by the agent.
    """
    from agent_data import agent_data
    app_type=agent_data[request.agentic_application_id]["agentic_application_type"]
    if app_type=="react_agent":
        from react_inference import agent_inference
        response = await agent_inference(request)
    else:
        from multiagent_without_hitl import multiagent_inference
        response= await multiagent_inference(request)
    return response

@app.post("/planner-executor-critic-agent/get-query-response-hitl-replanner")
async def generate_response_replanner_executor_critic_agent_main(request: ReplannerAgentInferenceRequest):
    """
    Handles the inference request for the Planner-Executor-Critic-replanner agent.

    Args:
        request (ReplannerAgentInferenceRequest): The request object containing the query, session ID, and other parameters.

    Returns:
        JSONResponse: A JSON response containing the agent's response and state.

    Raises:
        HTTPException: If an error occurs during processing.
    """
    query = request.query
    response = {}
    try:
        from multiagent_with_hitl import human_in_the_loop_replanner_inference
        response =  await human_in_the_loop_replanner_inference(request)
        final_response = "\n".join(i for i in response["plan"])
        steps = ""
        old_response = "\n".join(i for i in request.prev_response["plan"])
        old_steps = ""
        await insert_into_feedback_table(
        agent_id=request.agentic_application_id.replace("-","_"),
        query=query,
        old_final_response= old_response,
        old_steps=old_steps,
        new_final_response=final_response,
        feedback=request.feedback, 
        new_steps=steps)
    except Exception as e:
        response["error"] = f"An error occurred while processing the response: {str(e)}"
    return response

@app.post("/meta-agent/get-query-response")
async def get_meta_agent_response(request: AgentInferenceRequest):
    """
    Gets the response for a query using agent inference.

    Parameters:
    ----------
    request : AgentInferenceRequest
        The request body containing the query details.

    Returns:
    -------
    dict
        A dictionary containing the response generated by the agent.
    """
    from meta_inference import meta_agent_inference
    response = await meta_agent_inference(request)
    return response

@app.post("/planner-meta-agent/get-query-response")
async def get_planner_meta_agent_response(request: AgentInferenceRequest):
    """
    Gets the response for a query using agent inference.

    Parameters:
    ----------
    request : AgentInferenceRequest
        The request body containing the query details.

    Returns:
    -------
    dict
        A dictionary containing the response generated by the agent.
    """
    from plannermeta_inference import meta_agent_with_planner_inference
    response = await meta_agent_with_planner_inference(request)
    # print(response)
    return response
 
@app.post("/react-agent/get-feedback-response/{feedback}") 
async def get_react_feedback_response(feedback: str, request: AgentInferenceRequest):
    """
    Gets the response for the feedback using agent inference.

    Parameters:
    ----------
    feedback: str
        The feedback type
    request : AgentInferenceRequest
        The request body containing the feedback details.

    Returns:
    -------
    dict
        A dictionary containing the response generated by the agent.
    """
    try:
        query = request.query
        if feedback == "like":
            # update_status = await update_latest_query_response_with_tag(agentic_application_id=request.agentic_application_id, session_id=request.session_id)
            update_status=True
            if update_status is True:
                return {"message": "Thanks for the like! We're glad you found the response helpful. If you have any more questions or need further assistance, feel free to ask!"}
            elif update_status is False:
                return {"message": "Your like has been removed. If you have any more questions or need further assistance, feel free to ask!"}
            else:
                return {"message": "Sorry, we couldn't update your request at the moment. Please try again later."}
        elif feedback == "regenerate":
            request.query = "[regenerate:][:regenerate]"
        elif feedback == "feedback":
            request.query = f"[feedback:]{request.feedback}[:feedback]"
        else:
            return {"error": "Invalid Path!"}

        request.reset_conversation = False
        from agent_data import agent_data
        app_type=agent_data[request.agentic_application_id]["agentic_application_type"]
        if app_type=="react_agent":
            from react_inference import agent_inference
            response = await agent_inference(request)
        else:
            from multiagent_without_hitl import multiagent_inference
            response = await multiagent_inference(request)
        try:
            final_response = response["response"]
            steps = response["executor_messages"][-1]["agent_steps"]
            old_response = request.prev_response["response"]
            old_steps = request.prev_response["executor_messages"][-1]["agent_steps"]
            await insert_into_feedback_table(
                agent_id=request.agentic_application_id.replace("-","_"),
                query=query,
                old_final_response= old_response,
                old_steps=old_steps,
                new_final_response=final_response,
                feedback=request.feedback, 
                new_steps=steps)
        except Exception as e:
            response["error"] = f"An error occurred while processing the response: {str(e)}"
        return response
    except Exception as e:
        return {"error": f"An error occurred while processing the request: {str(e)}"}
 


@app.post("/react-agent/get-chat-history")
@app.post("/get-chat-history")
async def get_history(request: PrevSessionRequest):
    """
    Retrieves the chat history of a previous session.

    Parameters:
    ----------
    request : PrevSessionRequest
        The request body containing the details of the previous session.

    Returns:
    -------
    dict
        A dictionary containing the previous conversation history.
    """
    from agent_data import agent_data
    app_type=agent_data[request.agent_id]["agentic_application_type"]
    if app_type=="react_agent":
        from react_inference import retrive_previous_conversation
    elif app_type =="multi_agent":
        from multiagent_without_hitl import retrive_previous_conversation
    elif app_type =="meta_agent":
        from meta_inference import retrive_previous_conversation
    elif app_type =="planner_meta_agent":
        from plannermeta_inference import retrive_previous_conversation
    return await retrive_previous_conversation(request=request)
@app.get("/get-agents-details-for-chat")
async def get_agents_details():
    """
    Retrieves detailed information about agents for chat purposes.
    Returns:
    -------
    list
        A list of dictionaries containing agent details.
        If no agents are found, raises an HTTPException with status code 404.
    """
    agent_details = await get_agents_details_for_chat()
    return agent_details

@app.get('/get-models')
def get_models():
    data = ["gpt4-8k","gpt-4o-mini","gpt-4o","gpt-35-turbo","gpt-4o-2","gpt-4o-3"]
    return JSONResponse(content={"models": data})

@app.get("/new_chat/{email}")
async def new_chat(email:str):
    import uuid
    id = str(uuid.uuid4()).replace("-","_")
    session_id = email + "_" + id
    return session_id


# Document uploading

# Base directory for uploads
BASE_DIR = "user_uploads"

# Function to save uploaded files
def save_uploaded_file(uploaded_file: UploadFile, save_path: str):
    file_location = os.path.join(save_path, uploaded_file.filename)
    with open(file_location, "wb") as f:
        shutil.copyfileobj(uploaded_file.file, f)
    return file_location

# Function to generate file structure
def generate_file_structure(directory="user_uploads"):
    file_struct = {}
    for root, dirs, files in os.walk(directory):
        path_parts = root.split(os.sep)
        current_level = file_struct
        for part in path_parts:
            if part not in current_level:
                current_level[part] = {}
            current_level = current_level[part]
        if files:
            current_level["__files__"] = files  
    return file_struct

@app.post("/files/user-uploads/upload-file/")
async def upload_file(file: UploadFile = File(...), subdirectory: str = ""):
    if subdirectory.startswith("/") or subdirectory.startswith("\\"):
        subdirectory = subdirectory[1:]

    save_path = os.path.join(BASE_DIR, subdirectory) if subdirectory else BASE_DIR
    if not os.path.exists(save_path):
        os.makedirs(save_path)
    file_location = save_uploaded_file(file, save_path)
    return {"info": f"File '{file.filename}' saved at '{file_location}'"}

@app.get("/files/user-uploads/get-file-structure/")
async def get_file_structure():
    file_structure = generate_file_structure(BASE_DIR)
    return JSONResponse(content=file_structure)

@app.delete("/files/user-uploads/delete-file/")
async def delete_file(file_path: str):
    full_path = os.path.join(BASE_DIR, file_path)
    if os.path.exists(full_path):
        if os.path.isfile(full_path):
            os.remove(full_path)
            return {"info": f"File '{file_path}' deleted successfully."}
        else:
            raise HTTPException(status_code=400, detail="The specified path is a directory. Only files can be deleted.")
    else:
        raise HTTPException(status_code=404, detail="No such file or directory.")

@app.get('/get-version')
def get_version():
    with open(os.path.join(os.path.dirname(__file__), 'VERSION')) as f:
        return f.read().strip()

@app.delete("/react-agent/clear-chat-history")
@app.delete("/clear-chat-history")
async def clear_chat_history(request: PrevSessionRequest):
    return await delete_chat_history_by_session_id(agentic_application_id=request.agent_id, session_id=request.session_id)
